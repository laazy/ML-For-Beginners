# Construir um modelo de regress√£o usando Scikit-learn: regress√£o de quatro formas

## Nota para iniciantes

A regress√£o linear √© usada quando queremos prever um **valor num√©rico** (por exemplo, pre√ßo de casa, temperatura ou vendas). Funciona encontrando uma linha reta que melhor representa a rela√ß√£o entre as caracter√≠sticas de entrada e a sa√≠da.

Nesta li√ß√£o, focamos em entender o conceito antes de explorar t√©cnicas de regress√£o mais avan√ßadas.
![Linear vs polynomial regression infographic](../../../../translated_images/pt-PT/linear-polynomial.5523c7cb6576ccab.webp)
> Infogr√°fico por [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ml/)

> ### [Esta li√ß√£o est√° dispon√≠vel em R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introdu√ß√£o

At√© agora exploraste o que √© regress√£o com dados de exemplo recolhidos do conjunto de dados de pre√ßos de ab√≥boras que iremos usar ao longo desta li√ß√£o. Tamb√©m os visualizaste usando Matplotlib.

Agora est√°s pronto para mergulhar mais fundo na regress√£o para ML. Enquanto a visualiza√ß√£o permite fazer sentido dos dados, o verdadeiro poder do Machine Learning vem do _treino de modelos_. Os modelos s√£o treinados com dados hist√≥ricos para capturar automaticamente depend√™ncias dos dados, e permitem prever resultados para novos dados, que o modelo ainda n√£o viu.

Nesta li√ß√£o, vais aprender mais sobre dois tipos de regress√£o: _regress√£o linear b√°sica_ e _regress√£o polinomial_, juntamente com alguma matem√°tica subjacente a estas t√©cnicas. Esses modelos permitir-nos-√£o prever pre√ßos de ab√≥boras dependendo de diferentes dados de entrada.

[![ML for beginners - Understanding Linear Regression](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML for beginners - Understanding Linear Regression")

> üé• Clica na imagem acima para um v√≠deo curto sobre regress√£o linear.

> Ao longo deste curr√≠culo, assumimos conhecimentos m√≠nimos de matem√°tica e procuramos torn√°-lo acess√≠vel a estudantes de outras √°reas, por isso observa as notas, üßÆ chamadas, diagramas e outras ferramentas de aprendizagem para ajudar na compreens√£o.

### Pr√©-requisitos

Deves estar familiarizado agora com a estrutura dos dados das ab√≥boras que estamos a examinar. Podes encontr√°-los pr√©-carregados e pr√©-limpos no ficheiro _notebook.ipynb_ desta li√ß√£o. No ficheiro, o pre√ßo da ab√≥bora √© apresentado por alqueire num novo dataframe. Certifica-te de que consegues executar estes notebooks em kernels no Visual Studio Code.

### Prepara√ß√£o

Como lembrete, est√°s a carregar estes dados para poder colocar-lhes quest√µes.

- Qual √© a melhor √©poca para comprar ab√≥boras?
- Que pre√ßo posso esperar por uma caixa de ab√≥boras miniatura?
- Devo compr√°-las em cestos de meio alqueire ou por caixas de 1 1/9 alqueires?
Vamos continuar a explorar estes dados.

Na li√ß√£o anterior, criaste um dataframe Pandas e populaste-o com uma parte do conjunto de dados original, padronizando os pre√ßos por alqueire. Ao fazer isso, no entanto, s√≥ conseguiste recolher cerca de 400 pontos de dados e apenas para os meses de outono.

D√° uma vista de olhos aos dados que pr√©-carreg√°mos no notebook acompanhante desta li√ß√£o. Os dados est√£o pr√©-carregados e um gr√°fico de dispers√£o inicial √© apresentado para mostrar dados mensais. Talvez possamos obter um pouco mais de detalhe sobre a natureza dos dados limpando-os melhor.

## Uma linha de regress√£o linear

Como aprendeste na Li√ß√£o 1, o objetivo de um exerc√≠cio de regress√£o linear √© ser capaz de tra√ßar uma linha para:

- **Mostrar rela√ß√µes entre vari√°veis**. Mostrar a rela√ß√£o entre vari√°veis.
- **Fazer previs√µes**. Fazer previs√µes precisas sobre onde um novo ponto de dados cairia em rela√ß√£o a essa linha.

√â t√≠pico da **Regress√£o dos M√≠nimos Quadrados** tra√ßar esse tipo de linha. O termo "M√≠nimos Quadrados" refere-se ao processo de minimizar o erro total no nosso modelo. Para cada ponto de dados, medimos a dist√¢ncia vertical (chamada residual) entre o ponto real e a nossa linha de regress√£o.

Elevamos ao quadrado essas dist√¢ncias por duas raz√µes principais:

1. **Magnitude em vez de Dire√ß√£o:** Queremos tratar um erro de -5 igual a um erro de +5. Elevando ao quadrado, todos os valores tornam-se positivos.

2. **Penaliza√ß√£o de Outliers:** Elevar ao quadrado d√° mais peso a erros maiores, for√ßando a linha a ficar mais pr√≥xima dos pontos mais afastados.

Depois somamos todos esses valores ao quadrado. O nosso objetivo √© encontrar a linha espec√≠fica onde essa soma final √© m√≠nima (o menor valor poss√≠vel) ‚Äî da√≠ o nome "M√≠nimos Quadrados".

> **üßÆ Mostra-me a matem√°tica**
> 
> Esta linha, chamada de _linha de melhor ajuste_ pode ser expressa por [uma equa√ß√£o](https://en.wikipedia.org/wiki/Simple_linear_regression):
> 
> ```
> Y = a + bX
> ```
>
> `X` √© a 'vari√°vel explicativa'. `Y` √© a 'vari√°vel dependente'. A inclina√ß√£o da linha √© `b` e `a` √© o intercepto no eixo y, que se refere ao valor de `Y` quando `X = 0`.
>
>![calculate the slope](../../../../translated_images/pt-PT/slope.f3c9d5910ddbfcf9.webp)
>
> Primeiro, calcula a inclina√ß√£o `b`. Infogr√°fico por [Jen Looper](https://twitter.com/jenlooper)
>
> Por outras palavras, e referindo-nos √† pergunta original dos nossos dados de ab√≥boras: "prever o pre√ßo de uma ab√≥bora por alqueire por m√™s", `X` referir-se-ia ao pre√ßo e `Y` ao m√™s de venda.
>
>![complete the equation](../../../../translated_images/pt-PT/calculation.a209813050a1ddb1.webp)
>
> Calcula o valor de Y. Se estiveres a pagar cerca de 4 d√≥lares, deve ser abril! Infogr√°fico por [Jen Looper](https://twitter.com/jenlooper)
>
> A matem√°tica que calcula a linha deve demonstrar a inclina√ß√£o da linha, que tamb√©m depende do intercepto, ou seja, onde `Y` se encontra quando `X = 0`.
>
> Podes observar o m√©todo de c√°lculo destes valores no site [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). Visita tamb√©m [este calculador de m√≠nimos quadrados](https://www.mathsisfun.com/data/least-squares-calculator.html) para ver como os valores dos n√∫meros afetam a linha.

## Correla√ß√£o

Mais um termo para entender √© o **Coeficiente de Correla√ß√£o** entre vari√°veis X e Y dadas. Usando um gr√°fico de dispers√£o, podes visualizar rapidamente esse coeficiente. Um gr√°fico com pontos dispersos em linha organizada tem alta correla√ß√£o, mas um gr√°fico com pontos dispersos por todo o lado entre X e Y tem baixa correla√ß√£o.

Um bom modelo de regress√£o linear ser√° aquele que tem um Coeficiente de Correla√ß√£o alto (mais perto de 1 do que de 0) usando o m√©todo de Regress√£o dos M√≠nimos Quadrados com uma linha de regress√£o.

‚úÖ Executa o notebook que acompanha esta li√ß√£o e olha para o gr√°fico de dispers√£o M√™s para Pre√ßo. Os dados que associam M√™s a Pre√ßo para vendas de ab√≥bora parecem ter correla√ß√£o alta ou baixa, segundo a tua interpreta√ß√£o visual do gr√°fico de dispers√£o? Isso muda se usares uma medida mais detalhada em vez de `M√™s`, p.ex. *dia do ano* (ou seja, n√∫mero de dias desde o in√≠cio do ano)?

No c√≥digo abaixo, assumiremos que limp√°mos os dados e obtivemos um dataframe chamado `new_pumpkins`, semelhante ao seguinte:

ID | M√™s | DiaDoAno | Variedade | Cidade | Embalagem | Pre√ßo Baixo | Pre√ßo Alto | Pre√ßo
---|-----|----------|-----------|--------|-----------|-------------|------------|--------
70 | 9 | 267 | TIPO PARA TORTA | BALTIMORE | caixas de 1 1/9 alqueire | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | TIPO PARA TORTA | BALTIMORE | caixas de 1 1/9 alqueire | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | TIPO PARA TORTA | BALTIMORE | caixas de 1 1/9 alqueire | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | TIPO PARA TORTA | BALTIMORE | caixas de 1 1/9 alqueire | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | TIPO PARA TORTA | BALTIMORE | caixas de 1 1/9 alqueire | 15.0 | 15.0 | 13.636364

> O c√≥digo para limpar os dados est√° dispon√≠vel em [`notebook.ipynb`](notebook.ipynb). Fizemos os mesmos passos de limpeza da li√ß√£o anterior, e calcul√°mos a coluna `DayOfYear` usando a seguinte express√£o: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Agora que tens uma compreens√£o da matem√°tica por detr√°s da regress√£o linear, vamos criar um modelo de Regress√£o para ver se conseguimos prever qual embalagem de ab√≥boras ter√° os melhores pre√ßos de ab√≥bora. Algu√©m a comprar ab√≥boras para uma decora√ß√£o de ab√≥boras de feriado pode querer essa informa√ß√£o para otimizar as suas compras de embalagens de ab√≥boras para a decora√ß√£o.

## Procurar Correla√ß√£o

[![ML for beginners - Looking for Correlation: The Key to Linear Regression](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML for beginners - Looking for Correlation: The Key to Linear Regression")

> üé• Clica na imagem acima para um v√≠deo curto sobre correla√ß√£o.

Na li√ß√£o anterior de certeza que viste que o pre√ßo m√©dio para diferentes meses parece assim:

<img alt="Average price by month" src="../../../../translated_images/pt-PT/barchart.a833ea9194346d76.webp" width="50%"/>

Isto sugere que deve haver alguma correla√ß√£o, e podemos tentar treinar um modelo de regress√£o linear para prever a rela√ß√£o entre `M√™s` e `Pre√ßo`, ou entre `DiaDoAno` e `Pre√ßo`. Aqui est√° o gr√°fico de dispers√£o que mostra a √∫ltima rela√ß√£o:

<img alt="Scatter plot of Price vs. Day of Year" src="../../../../translated_images/pt-PT/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" /> 

Vamos ver se existe correla√ß√£o usando a fun√ß√£o `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Parece que a correla√ß√£o √© bastante pequena, -0.15 pelo `M√™s` e -0.17 pelo `DiaDoAno`, mas pode haver outra rela√ß√£o importante. Parece que existem diferentes aglomerados de pre√ßos correspondendo a v√°rias variedades de ab√≥boras. Para confirmar esta hip√≥tese, vamos tra√ßar cada categoria de ab√≥bora usando uma cor diferente. Passando um par√¢metro `ax` para a fun√ß√£o de plotagem `scatter` podemos colocar todos os pontos no mesmo gr√°fico:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Scatter plot of Price vs. Day of Year" src="../../../../translated_images/pt-PT/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" /> 

A nossa investiga√ß√£o sugere que a variedade tem mais efeito sobre o pre√ßo global do que a data real de venda. Podemos ver isto com um gr√°fico de barras:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Bar graph of price vs variety" src="../../../../translated_images/pt-PT/price-by-variety.744a2f9925d9bcb4.webp" width="50%" /> 

Vamos focar por enquanto apenas numa variedade de ab√≥bora, o 'tipo para torta', e ver o efeito que a data tem no pre√ßo:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Scatter plot of Price vs. Day of Year" src="../../../../translated_images/pt-PT/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" /> 

Se agora calcularmos a correla√ß√£o entre `Pre√ßo` e `DiaDoAno` usando a fun√ß√£o `corr`, obteremos algo como `-0.27` - o que significa que treinar um modelo preditivo faz sentido.

> Antes de treinar um modelo de regress√£o linear, √© importante garantir que os nossos dados est√£o limpos. A regress√£o linear n√£o funciona bem com valores em falta, por isso faz sentido eliminar todas as c√©lulas vazias:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Outra abordagem seria preencher esses valores vazios com a m√©dia da coluna correspondente.

## Regress√£o Linear Simples

[![ML for beginners - Linear and Polynomial Regression using Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML for beginners - Linear and Polynomial Regression using Scikit-learn")

> üé• Clica na imagem acima para um v√≠deo curto sobre regress√£o linear e polinomial.

Para treinar o nosso modelo de Regress√£o Linear, vamos usar a biblioteca **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Come√ßamos por separar os valores de entrada (caracter√≠sticas) e a sa√≠da esperada (r√≥tulo) em arrays numpy separados:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Note que tivemos de fazer `reshape` na entrada de dados para que o pacote Linear Regression a entendesse corretamente. A Regress√£o Linear espera uma matriz 2D como entrada, onde cada linha da matriz corresponde a um vetor de caracter√≠sticas de entrada. No nosso caso, como temos apenas uma entrada - precisamos de um array com forma N&times;1, onde N √© o tamanho do conjunto de dados.

Depois, precisamos de dividir os dados em conjuntos de treino e teste, para que possamos validar o nosso modelo depois do treino:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Por fim, treinar o modelo real de Regress√£o Linear ocupa apenas duas linhas de c√≥digo. Definimos o objeto `LinearRegression` e ajustamo-lo aos nossos dados usando o m√©todo `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

O objeto `LinearRegression` ap√≥s ser `fit`-ado cont√©m todos os coeficientes da regress√£o, que podem ser acedidos usando a propriedade `.coef_`. No nosso caso, h√° apenas um coeficiente, que dever√° estar em torno de `-0.017`. Isso significa que os pre√ßos parecem diminuir um pouco com o tempo, mas n√£o muito, cerca de 2 c√™ntimos por dia. Podemos tamb√©m aceder ao ponto de interse√ß√£o da regress√£o com o eixo Y usando `lin_reg.intercept_` - ser√° cerca de `21` no nosso caso, indicando o pre√ßo no in√≠cio do ano.

Para ver o qu√£o preciso √© o nosso modelo, podemos prever pre√ßos num conjunto de dados de teste e ent√£o medir qu√£o pr√≥ximas as nossas previs√µes est√£o dos valores esperados. Isto pode ser feito usando a m√©trica de erro quadr√°tico m√©dio (MSE), que √© a m√©dia de todas as diferen√ßas ao quadrado entre o valor esperado e o previsto.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

O nosso erro parece estar em torno de 2 pontos, o que √© ~17%. N√£o √© muito bom. Outro indicador da qualidade do modelo √© o **coeficiente de determina√ß√£o**, que pode ser obtido assim:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```

Se o valor for 0, significa que o modelo n√£o considera os dados de entrada e atua como o *pior preditor linear*, que √© simplesmente a m√©dia dos resultados. O valor 1 significa que conseguimos predizer perfeitamente todas as sa√≠das esperadas. No nosso caso, o coeficiente est√° em torno de 0.06, que √© bastante baixo.

Podemos tamb√©m fazer um gr√°fico dos dados de teste juntamente com a linha de regress√£o para ver melhor como funciona a regress√£o no nosso caso:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linear regression" src="../../../../translated_images/pt-PT/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## Regress√£o Polinomial

Outro tipo de Regress√£o Linear √© a Regress√£o Polinomial. Embora por vezes exista uma rela√ß√£o linear entre vari√°veis - quanto maior a ab√≥bora em volume, maior o pre√ßo - √†s vezes essas rela√ß√µes n√£o podem ser representadas como um plano ou linha reta.

‚úÖ Aqui est√£o [alguns exemplos adicionais](https://online.stat.psu.edu/stat501/lesson/9/9.8) de dados que poderiam utilizar Regress√£o Polinomial.

Observe novamente a rela√ß√£o entre Data e Pre√ßo. Este gr√°fico de dispers√£o parece algo que deveria necessariamente ser analisado por uma linha reta? Os pre√ßos n√£o podem flutuar? Neste caso, pode tentar regress√£o polinomial.

‚úÖ Polin√≥mios s√£o express√µes matem√°ticas que podem consistir em uma ou mais vari√°veis e coeficientes.

A regress√£o polinomial cria uma linha curva para se ajustar melhor aos dados n√£o lineares. No nosso caso, se incluirmos uma vari√°vel `DayOfYear` ao quadrado nos dados de entrada, deveremos conseguir ajustar os nossos dados com uma curva parab√≥lica, que ter√° um m√≠nimo num certo ponto ao longo do ano.

O Scikit-learn inclui uma API √∫til [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) para combinar diferentes etapas do processamento de dados. Um **pipeline** √© uma cadeia de **estimadores**. No nosso caso, vamos criar um pipeline que primeiramente adiciona caracter√≠sticas polinomiais ao nosso modelo e depois treina a regress√£o:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Usar `PolynomialFeatures(2)` significa que iremos incluir todos os polin√≥mios de segundo grau a partir dos dados de entrada. No nosso caso, isso significar√° apenas `DayOfYear`<sup>2</sup>, mas dado dois vari√°veis de entrada X e Y, isto adicionar√° X<sup>2</sup>, XY e Y<sup>2</sup>. Podemos tamb√©m usar polin√≥mios de grau superior, se desejarmos.

Pipelines podem ser usados da mesma forma que o objeto original `LinearRegression`, ou seja, podemos `fit` o pipeline, e depois usar `predict` para obter os resultados das previs√µes. Aqui est√° o gr√°fico mostrando os dados de teste e a curva de aproxima√ß√£o:

<img alt="Polynomial regression" src="../../../../translated_images/pt-PT/poly-results.ee587348f0f1f60b.webp" width="50%" />

Usando a Regress√£o Polinomial, podemos obter um MSE ligeiramente mais baixo e um coeficiente de determina√ß√£o mais alto, mas n√£o significativamente. Precisamos de considerar outras caracter√≠sticas!

> Pode ver que os pre√ßos m√≠nimos das ab√≥boras ocorrem por volta do Halloween. Como pode explicar isso?

üéÉ Parab√©ns, acabou de criar um modelo que pode ajudar a prever o pre√ßo das ab√≥boras para torta. Provavelmente pode repetir o mesmo procedimento para todos os tipos de ab√≥boras, mas isso seria trabalhoso. Vamos agora aprender como considerar a variedade da ab√≥bora no nosso modelo!

## Caracter√≠sticas Categ√≥ricas

No mundo ideal, queremos ser capazes de prever pre√ßos para diferentes variedades de ab√≥bora usando o mesmo modelo. No entanto, a coluna `Variety` √© algo diferente de colunas como `Month`, porque cont√©m valores n√£o num√©ricos. Estas colunas s√£o chamadas **categ√≥ricas**.

[![ML para iniciantes - Previs√µes de Caracter√≠sticas Categ√≥ricas com Regress√£o Linear](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML para iniciantes - Previs√µes de Caracter√≠sticas Categ√≥ricas com Regress√£o Linear")

> üé• Clique na imagem acima para um v√≠deo curto sobre o uso de caracter√≠sticas categ√≥ricas.

Aqui pode ver como o pre√ßo m√©dio depende da variedade:

<img alt="Average price by variety" src="../../../../translated_images/pt-PT/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

Para considerar a variedade, primeiro precisamos convert√™-la para forma num√©rica, ou **codific√°-la**. Existem v√°rias formas de o fazer:

* Uma simples **codifica√ß√£o num√©rica** construiria uma tabela das diferentes variedades, e depois substituiria o nome da variedade pelo √≠ndice nessa tabela. Esta n√£o √© a melhor ideia para regress√£o linear, porque a regress√£o linear utiliza o valor num√©rico real do √≠ndice e adiciona-o ao resultado, multiplicando por algum coeficiente. No nosso caso, a rela√ß√£o entre o n√∫mero do √≠ndice e o pre√ßo √© claramente n√£o linear, mesmo se nos certificarmos que os √≠ndices s√£o ordenados de uma forma espec√≠fica.
* A **codifica√ß√£o one-hot** substitui a coluna `Variety` por 4 colunas diferentes, uma para cada variedade. Cada coluna conter√° `1` se a linha correspondente for da variedade dada, e `0` caso contr√°rio. Isto significa que haver√° quatro coeficientes na regress√£o linear, um para cada variedade de ab√≥bora, respons√°vel pelo "pre√ßo inicial" (ou melhor, "pre√ßo adicional") para essa variedade em particular.

O c√≥digo abaixo mostra como podemos codificar one-hot uma variedade:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Para treinar a regress√£o linear utilizando a variedade codificada one-hot como entrada, s√≥ precisamos inicializar corretamente os dados `X` e `y`:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

O resto do c√≥digo √© o mesmo que usamos acima para treinar a Regress√£o Linear. Se tentar, ver√° que o erro quadr√°tico m√©dio √© aproximadamente o mesmo, mas obtemos um coeficiente de determina√ß√£o muito maior (~77%). Para obter previs√µes ainda mais precisas, podemos considerar mais caracter√≠sticas categ√≥ricas, bem como vari√°veis num√©ricas, como `Month` ou `DayOfYear`. Para obter um √∫nico grande array de caracter√≠sticas, podemos usar `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Aqui tamb√©m consideramos `City` e o tipo de `Package`, o que nos d√° um MSE de 2.84 (10%) e determina√ß√£o 0.94!

## Juntando tudo

Para fazer o melhor modelo, podemos usar dados combinados (categ√≥ricos codificados one-hot + num√©ricos) do exemplo acima juntamente com a Regress√£o Polinomial. Aqui est√° o c√≥digo completo para sua conveni√™ncia:

```python
# preparar dados de treino
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# fazer divis√£o treino-teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# configurar e treinar o pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# prever resultados para dados de teste
pred = pipeline.predict(X_test)

# calcular MSE e determina√ß√£o
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Isto dever√° dar-nos o melhor coeficiente de determina√ß√£o de quase 97%, e MSE=2.23 (~8% de erro de previs√£o).

| Modelo | MSE | Determina√ß√£o |
|--------|-----|--------------|
| Linear com `DayOfYear` | 2.77 (17.2%) | 0.07 |
| Polinomial com `DayOfYear` | 2.73 (17.0%) | 0.08 |
| Linear com `Variety` | 5.24 (19.7%) | 0.77 |
| Linear com todas as caracter√≠sticas | 2.84 (10.5%) | 0.94 |
| Polinomial com todas as caracter√≠sticas | 2.23 (8.25%) | 0.97 |

üèÜ Muito bem! Criou quatro modelos de regress√£o numa s√≥ li√ß√£o, e melhorou a qualidade do modelo para 97%. Na sec√ß√£o final sobre Regress√£o, ir√° aprender sobre Regress√£o Log√≠stica para determinar categorias.

---
## üöÄDesafio

Teste v√°rias vari√°veis diferentes neste notebook para ver como a correla√ß√£o corresponde √† precis√£o do modelo.

## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ml/)

## Revis√£o & Estudo Aut√≥nomo

Nesta li√ß√£o aprendemos sobre Regress√£o Linear. Existem outros tipos importantes de Regress√£o. Leia sobre as t√©cnicas Stepwise, Ridge, Lasso e Elasticnet. Um bom curso para estudar e aprender mais √© o [curso de Aprendizagem Estat√≠stica de Stanford](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## Tarefa

[Construa um Modelo](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Aviso Legal**:
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, por favor, tenha em conta que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas resultantes do uso desta tradu√ß√£o.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->