# Scikit-learn kullanarak regresyon modeli oluÅŸturma: dÃ¶rt farklÄ± regresyon yÃ¶ntemi

## BaÅŸlangÄ±Ã§ Notu

Lineer regresyon, **sayÄ±sal bir deÄŸeri** tahmin etmek istediÄŸimizde kullanÄ±lÄ±r (Ã¶rneÄŸin, ev fiyatÄ±, sÄ±caklÄ±k veya satÄ±ÅŸlar).  
Girdi Ã¶zellikleri ile Ã§Ä±ktÄ± arasÄ±ndaki iliÅŸkiyi en iyi temsil eden doÄŸruyu bulmakla Ã§alÄ±ÅŸÄ±r.

Bu derste, daha ileri regresyon tekniklerini keÅŸfetmeden Ã¶nce kavramÄ± anlamaya odaklanÄ±yoruz.  
![Lineer ve polinom regresyon bilgigramÄ±](../../../../translated_images/tr/linear-polynomial.5523c7cb6576ccab.webp)  
> Bilgigram: [Dasani Madipalli](https://twitter.com/dasani_decoded)  
## [Ã–n ders sÄ±navÄ±](https://ff-quizzes.netlify.app/en/ml/)

> ### [Bu ders R dilinde de mevcut!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)  
### GiriÅŸ

Åimdiye kadar, balkabaÄŸÄ± fiyatlandÄ±rma veri setinden toplanan Ã¶rnek verilerle regresyonun ne olduÄŸunu keÅŸfettiniz. AyrÄ±ca Matplotlib kullanarak bu veriyi gÃ¶rselleÅŸtirdiniz.

ArtÄ±k makine Ã¶ÄŸrenimi iÃ§in regresyona daha derinlemesine dalmaya hazÄ±rsÄ±nÄ±z. GÃ¶rselleÅŸtirme veriyi anlamanÄ±zÄ± saÄŸlarken, makine Ã¶ÄŸreniminin gerÃ§ek gÃ¼cÃ¼ _modellerin eÄŸitilmesinden_ gelir. Modeller, geÃ§miÅŸ veriler Ã¼zerinde eÄŸitilerek veri baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± otomatik olarak yakalar ve modelin daha Ã¶nce gÃ¶rmediÄŸi yeni veriler iÃ§in sonuÃ§lar tahmin etmenizi saÄŸlar.

Bu derste, _temel lineer regresyon_ ve _polinom regresyon_ olmak Ã¼zere iki regresyon tÃ¼rÃ¼ ve bu tekniklerin altÄ±nda yatan matematik hakkÄ±nda daha fazla bilgi edineceksiniz. Bu modeller farklÄ± girdi verilerine baÄŸlÄ± olarak balkabaÄŸÄ± fiyatlarÄ±nÄ± tahmin etmemizi saÄŸlayacak.

[![ML for beginners - Understanding Linear Regression](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML for beginners - Understanding Linear Regression")

> ğŸ¥ Lineer regresyona kÄ±sa video genel bakÄ±ÅŸÄ± iÃ§in yukarÄ±daki gÃ¶rsele tÄ±klayÄ±n.

> Bu eÄŸitim programÄ± boyunca matematik bilgisi asgari dÃ¼zeyde varsayÄ±lmakta ve farklÄ± alanlardan gelen Ã¶ÄŸrenciler iÃ§in eriÅŸilebilir olmayÄ± hedeflemektedir. Bu yÃ¼zden notlar, ğŸ§® alÄ±ntÄ±lar, diyagramlar ve diÄŸer Ã¶ÄŸrenme araÃ§larÄ±na dikkat edin.

### Ã–n KoÅŸul

Åu ana kadar incelediÄŸimiz balkabaÄŸÄ± verisinin yapÄ±sÄ±na aÅŸina olmalÄ±sÄ±nÄ±z. Bu dersin _notebook.ipynb_ dosyasÄ±nda Ã¶nceden yÃ¼klenmiÅŸ ve temizlenmiÅŸ olarak bulabilirsiniz. Bu dosyada balkabaÄŸÄ± fiyatÄ± bushel baÅŸÄ±na yeni bir veri Ã§erÃ§evesinde gÃ¶sterilmiÅŸtir.  
Visual Studio Code'da kernel kullanarak bu not defterlerini Ã§alÄ±ÅŸtÄ±rabildiÄŸinizden emin olun.

### HazÄ±rlÄ±k

HatÄ±rlatma olarak, bu veriyi sorular sormak iÃ§in yÃ¼klÃ¼yorsunuz.

- BalkabaklarÄ±nÄ± satÄ±n almak iÃ§in en iyi zaman ne zaman?  
- MinyatÃ¼r balkabaklarÄ±nÄ±n bir kasasÄ± iÃ§in hangi fiyatÄ± bekleyebilirim?  
- OnlarÄ± yarÄ±m bushel sepetlerde mi yoksa 1 1/9 bushel kutuda mÄ± almalÄ±yÄ±m?  
Veri Ã¼zerinde kazÄ±ma iÅŸlemine devam edelim.

Ã–nceki derste, bir Pandas veri Ã§erÃ§evesi oluÅŸturdunuz ve orijinal veri setinin bir kÄ±smÄ±nÄ± bushel bazÄ±nda fiyatlandÄ±rmayÄ± standartlaÅŸtÄ±rarak doldurdunuz. Ancak bu ÅŸekilde sadece sonbahar aylarÄ± iÃ§in yaklaÅŸÄ±k 400 veri noktasÄ± toplamÄ±ÅŸ oldunuz.

Bu derse eÅŸlik eden not defterinde Ã¶nceden yÃ¼klenmiÅŸ verilere gÃ¶z atÄ±n. Veri Ã¶n yÃ¼klendi ve ay verisini gÃ¶stermek iÃ§in ilk daÄŸÄ±lÄ±m grafiÄŸi Ã§izildi. Veriyi daha fazla temizleyerek doÄŸasÄ± hakkÄ±nda biraz daha detay alabiliriz.

## Bir lineer regresyon doÄŸrusu

Ders 1'de Ã¶ÄŸrendiÄŸiniz gibi, lineer regresyon Ã§alÄ±ÅŸmasÄ±nÄ±n amacÄ± ÅŸu ÅŸekildedir:

- **DeÄŸiÅŸken iliÅŸkilerini gÃ¶stermek**. DeÄŸiÅŸkenler arasÄ±ndaki iliÅŸkiyi gÃ¶stermek  
- **Tahmin yapmak**. Yeni bir veri noktasÄ±nÄ±n bu doÄŸruya gÃ¶re nerede yer alacaÄŸÄ±nÄ± doÄŸru tahmin etmek

Bu tÃ¼r doÄŸrularÄ±n Ã§izimi iÃ§in tipik olan **En KÃ¼Ã§Ã¼k Kareler Regresyonu** yÃ¶ntemidir. "En KÃ¼Ã§Ã¼k Kareler" terimi, modelimizdeki toplam hatayÄ± minimize etme sÃ¼recine atÄ±fta bulunur. Her veri noktasÄ± iÃ§in, gerÃ§ek nokta ile regresyon doÄŸrumuz arasÄ±ndaki dikey mesafeyi (rezidÃ¼ olarak da adlandÄ±rÄ±lÄ±r) Ã¶lÃ§eriz.

Bu mesafeleri iki temel nedenle karesini alÄ±rÄ±z:

1. **BÃ¼yÃ¼klÃ¼k yÃ¶nÃ¼nden Ã¼stÃ¼nlÃ¼k:** -5 hata ile +5 hata aynÄ± ÅŸekilde ele alÄ±nmalÄ±. Karesini almak tÃ¼m deÄŸerleri pozitif yapar.

2. **AykÄ±rÄ± DeÄŸerlere Ceza:** Karesini almak daha bÃ¼yÃ¼k hatalara daha fazla aÄŸÄ±rlÄ±k verir ve doÄŸrunun uzak noktalarÄ±n yakÄ±nÄ±nda kalmasÄ±nÄ± zorunlu kÄ±lar.

Sonra bu karelenmiÅŸ deÄŸerlerin tÃ¼mÃ¼nÃ¼ toplarÄ±z. AmacÄ±mÄ±z bu toplamÄ±n en kÃ¼Ã§Ã¼k olduÄŸu doÄŸruyu bulmaktÄ±r; bu yÃ¼zden adÄ± "En KÃ¼Ã§Ã¼k Kareler"dir.

> **ğŸ§® MatematiÄŸi GÃ¶ster**  
>  
> Bu doÄŸru, _en uygun uyum doÄŸrusudur_ ve [bir denklemle](https://en.wikipedia.org/wiki/Simple_linear_regression) ifade edilebilir:  
>  
> ```
> Y = a + bX
> ```
>  
> `X` 'aÃ§Ä±klayÄ±cÄ± deÄŸiÅŸken'dir. `Y` ise 'baÄŸÄ±mlÄ± deÄŸiÅŸken'dir. DoÄŸrunun eÄŸimi `b` ve `a` y-kesiti olup, `X = 0` olduÄŸunda `Y` deÄŸerini ifade eder.  
>  
>![eÄŸimi hesapla](../../../../translated_images/tr/slope.f3c9d5910ddbfcf9.webp)  
> Ã–nce eÄŸimi `b` hesaplayÄ±n. Bilgigram: [Jen Looper](https://twitter.com/jenlooper)  
>  
> BaÅŸka bir deyiÅŸle ve balkabaÄŸÄ± verimizin asÄ±l sorusuna atÄ±fta bulunursak: "ay bazÄ±nda bushel baÅŸÄ±na balkabaÄŸÄ± fiyatÄ±nÄ± tahmin et", burada `X` fiyatÄ±, `Y` ise satÄ±ÅŸ ayÄ±nÄ± temsil eder.  
>  
>![denklemi tamamla](../../../../translated_images/tr/calculation.a209813050a1ddb1.webp)  
> Y'nin deÄŸerini hesaplayÄ±n. YaklaÅŸÄ±k $4 Ã¶dÃ¼yorsanÄ±z, bu kesinlikle Nisan ayÄ±dÄ±r! Bilgigram: [Jen Looper](https://twitter.com/jenlooper)  
>  
> DoÄŸruyu hesaplayan matematik, doÄŸrunun eÄŸimini gÃ¶stermeli, bu da kesit deÄŸerine, yani `Y`'nin `X = 0` olduÄŸundaki konumuna baÄŸlÄ±dÄ±r.  
>  
> Bu deÄŸerlerin hesaplanma yÃ¶ntemini [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) sitesinde gÃ¶rebilirsiniz. AyrÄ±ca sayÄ±larÄ±n Ã§izgi Ã¼zerindeki etkisini gÃ¶rmek iÃ§in [bu En-KÃ¼Ã§Ã¼k Kareler hesaplayÄ±cÄ±sÄ±nÄ±](https://www.mathsisfun.com/data/least-squares-calculator.html) ziyaret edin.

## Korelasyon

AnlamanÄ±z gereken bir diÄŸer terim, verilen X ve Y deÄŸiÅŸkenleri arasÄ±ndaki **Korelasyon KatsayÄ±sÄ±**dÄ±r. Bir daÄŸÄ±lÄ±m grafiÄŸi kullanarak bu katsayÄ±yÄ± hÄ±zlÄ±ca gÃ¶rselleÅŸtirebilirsiniz. NoktalarÄ±n dÃ¼zgÃ¼n bir doÄŸru Ã¼zerindeyse yÃ¼ksek korelasyon, her yere saÃ§Ä±lmÄ±ÅŸsa dÃ¼ÅŸÃ¼k korelasyon vardÄ±r.

Ä°yi bir lineer regresyon modeli, En KÃ¼Ã§Ã¼k Kareler Regresyon yÃ¶ntemi ve bir regresyon doÄŸrusu kullanarak yÃ¼ksek (0'a deÄŸil 1'e daha yakÄ±n) bir Korelasyon KatsayÄ±sÄ±na sahip olacaktÄ±r.

âœ… Bu derse eÅŸlik eden not defterini Ã§alÄ±ÅŸtÄ±rÄ±n ve Ay ile Fiyat arasÄ±ndaki daÄŸÄ±lÄ±m grafiÄŸine bakÄ±n. GÃ¶rsel deÄŸerlendirmenize gÃ¶re BalkabaÄŸÄ± satÄ±ÅŸlarÄ± iÃ§in Ay ile Fiyat arasÄ±ndaki veri yÃ¼ksek mi yoksa dÃ¼ÅŸÃ¼k korelasyon mu gÃ¶steriyor? Daha ayrÄ±ntÄ±lÄ± bir Ã¶lÃ§Ã¼m olarak `Ay` yerine *yÄ±lÄ±n gÃ¼nÃ¼* (Ã¶rneÄŸin, yÄ±l baÅŸÄ±ndan itibaren geÃ§en gÃ¼n sayÄ±sÄ±) kullanÄ±rsanÄ±z bu deÄŸiÅŸir mi?

AÅŸaÄŸÄ±daki kodda, verinin temizlendiÄŸini ve `new_pumpkins` adÄ±nda aÅŸaÄŸÄ±dakine benzer bir veri Ã§erÃ§evesi elde edildiÄŸini varsayÄ±yoruz:

ID | Ay | YilinGunu | Ã‡eÅŸit | Åehir | Paket | DÃ¼ÅŸÃ¼k Fiyat | YÃ¼ksek Fiyat | Fiyat  
---|-------|-----------|---------|------|---------|-----------|------------|-------  
70 | 9 | 267 | TURTA TÄ°PÄ° | BALTIMORE | 1 1/9 bushel karton | 15.0 | 15.0 | 13.636364  
71 | 9 | 267 | TURTA TÄ°PÄ° | BALTIMORE | 1 1/9 bushel karton | 18.0 | 18.0 | 16.363636  
72 | 10 | 274 | TURTA TÄ°PÄ° | BALTIMORE | 1 1/9 bushel karton | 18.0 | 18.0 | 16.363636  
73 | 10 | 274 | TURTA TÄ°PÄ° | BALTIMORE | 1 1/9 bushel karton | 17.0 | 17.0 | 15.454545  
74 | 10 | 281 | TURTA TÄ°PÄ° | BALTIMORE | 1 1/9 bushel karton | 15.0 | 15.0 | 13.636364  

> Veriyi temizleme kodu [`notebook.ipynb`](notebook.ipynb) dosyasÄ±nda mevcuttur. Ã–nceki derste yaptÄ±ÄŸÄ±mÄ±z aynÄ± temizlik adÄ±mlarÄ±nÄ± uyguladÄ±k ve `DayOfYear` sÃ¼tununu aÅŸaÄŸÄ±daki ifadeyi kullanarak hesapladÄ±k:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```
  
Lineer regresyonun matematiÄŸini anladÄ±ÄŸÄ±nÄ±za gÃ¶re, hangi balkabaÄŸÄ± paketinin en iyi fiyatlara sahip olacaÄŸÄ±nÄ± tahmin etmek iÃ§in bir Regresyon modeli oluÅŸturalÄ±m. Birisi tatil balkabaÄŸÄ± bahÃ§esinde balkabaÄŸÄ± satÄ±n alÄ±yorsa, bahÃ§e iÃ§in balkabaÄŸÄ± paketlerini optimize etmek amacÄ±yla bu bilgi faydalÄ± olabilir.

## Korelasyon ArayÄ±ÅŸÄ±

[![ML for beginners - Korelasyon ArayÄ±ÅŸÄ±: Lineer Regresyonun AnahtarÄ±](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML for beginners - Korelasyon ArayÄ±ÅŸÄ±: Lineer Regresyonun AnahtarÄ±")

> ğŸ¥ Korelasyon hakkÄ±nda kÄ±sa video genel bakÄ±ÅŸÄ± iÃ§in yukarÄ±daki gÃ¶rsele tÄ±klayÄ±n.

Ã–nceki dersten muhtemelen farklÄ± aylar iÃ§in ortalama fiyatlarÄ±n ÅŸu ÅŸekilde olduÄŸunu gÃ¶rmÃ¼ÅŸtÃ¼nÃ¼z:

<img alt="Aya gÃ¶re ortalama fiyat" src="../../../../translated_images/tr/barchart.a833ea9194346d76.webp" width="50%"/>

Bu, bir korelasyon olabileceÄŸini dÃ¼ÅŸÃ¼ndÃ¼rÃ¼r ve `Ay` ile `Fiyat` ya da `YilinGunu` ile `Fiyat` arasÄ±ndaki iliÅŸkiyi tahmin etmek iÃ§in lineer regresyon modeli eÄŸitebiliriz. Ä°ÅŸte sonuncuyu gÃ¶steren daÄŸÄ±lÄ±m grafiÄŸi:

<img alt="Fiyat vs. YÄ±lÄ±n GÃ¼nÃ¼ daÄŸÄ±lÄ±m grafiÄŸi" src="../../../../translated_images/tr/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" />  

Korelasyonu `corr` fonksiyonuyla gÃ¶relim:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```
  
GÃ¶rÃ¼nÃ¼ÅŸe gÃ¶re korelasyon oldukÃ§a dÃ¼ÅŸÃ¼k, `Ay` iÃ§in -0.15 ve `YilinGunu` iÃ§in -0.17, ancak baÅŸka Ã¶nemli bir iliÅŸki olabilir. FarklÄ± balkabaÄŸÄ± Ã§eÅŸitlerine karÅŸÄ±lÄ±k gelen farklÄ± fiyat kÃ¼meleri var gibi. Bu hipotezi doÄŸrulamak iÃ§in her balkabaÄŸÄ± kategorisini farklÄ± renklerle Ã§izelim. `scatter` Ã§izim fonksiyonuna bir `ax` parametresi geÃ§irilerek tÃ¼m noktalar aynÄ± grafikte Ã§izilebilir:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```
  
<img alt="Fiyat vs. YÄ±lÄ±n GÃ¼nÃ¼ (renkli) daÄŸÄ±lÄ±m grafiÄŸi" src="../../../../translated_images/tr/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" />  

AraÅŸtÄ±rmalarÄ±mÄ±z, Ã§eÅŸidin satÄ±ÅŸ tarihinden daha fazla fiyat Ã¼zerinde etkisi olduÄŸunu gÃ¶steriyor. Bunu bir Ã§ubuk grafikle gÃ¶rebiliriz:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```
  
<img alt="Ã‡eÅŸide gÃ¶re fiyat Ã§ubuk grafiÄŸi" src="../../../../translated_images/tr/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />  

Åimdilik yalnÄ±zca bir balkabaÄŸÄ± Ã§eÅŸidine, 'turta tipi'ne, odaklanalÄ±m ve tarihin fiyat Ã¼zerindeki etkisine bakalÄ±m:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Fiyat vs. YÄ±lÄ±n GÃ¼nÃ¼ daÄŸÄ±lÄ±m grafiÄŸi" src="../../../../translated_images/tr/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" />  

Åimdi `Price` ve `YilinGunu` arasÄ±nda `corr` fonksiyonunu kullanarak korelasyon hesaplarsak, yaklaÅŸÄ±k `-0.27` buluruz - ki bu da tahmin modeli eÄŸitmenin mantÄ±klÄ± olduÄŸunu gÃ¶sterir.

> Lineer regresyon modeli eÄŸitmeden Ã¶nce, verinin temiz olmasÄ± Ã¶nemlidir. Lineer regresyon eksik deÄŸerlerle iyi Ã§alÄ±ÅŸmaz, bu yÃ¼zden boÅŸ hÃ¼crelerden kurtulmak mantÄ±klÄ±dÄ±r:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```
  
DiÄŸer bir yaklaÅŸÄ±m, boÅŸ deÄŸerleri ilgili sÃ¼tunun ortalama deÄŸerleriyle doldurmaktÄ±r.

## Basit Lineer Regresyon

[![ML for beginners - Scikit-learn ile Lineer ve Polinom Regresyon](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML for beginners - Scikit-learn ile Lineer ve Polinom Regresyon")

> ğŸ¥ Lineer ve polinom regresyon hakkÄ±nda kÄ±sa video genel bakÄ±ÅŸÄ± iÃ§in yukarÄ±daki gÃ¶rsele tÄ±klayÄ±n.

Lineer Regresyon modelimizi eÄŸitmek iÃ§in **Scikit-learn** kÃ¼tÃ¼phanesini kullanacaÄŸÄ±z.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```
  
Girdi deÄŸerlerini (Ã¶zellikler) ve beklenen Ã§Ä±ktÄ±yÄ± (etiket) ayrÄ± numpy dizilerine ayÄ±rarak baÅŸlÄ±yoruz:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```
  
> Lineer Regresyon paketinin doÄŸru anlayabilmesi iÃ§in girdi verilerini `reshape` yapmamÄ±z gerektiÄŸine dikkat edin. Lineer Regresyon, her satÄ±rÄ± bir Ã¶zellik vektÃ¶rÃ¼nÃ¼ temsil eden 2D diziyi bekler. Bizim durumumuzda sadece bir girdi olduÄŸundan, NÃ—1 yapÄ±sÄ±nda bir dizi gerekir, burada N veri seti bÃ¼yÃ¼klÃ¼ÄŸÃ¼dÃ¼r.

Sonra, modeli eÄŸitip doÄŸrulayabilmek iÃ§in veriyi eÄŸitim ve test veri setlerine bÃ¶lmemiz gerekiyor:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```
  
Son olarak, gerÃ§ek Lineer Regresyon modelinin eÄŸitimi sadece iki kod satÄ±rÄ± alÄ±r. `LinearRegression` nesnesini tanÄ±mlarÄ±z ve `fit` metodu ile verimize uyarlarÄ±z:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```
  
`LinearRegression` nesnesi `fit` edildikten sonra regresyonun tÃ¼m katsayÄ±larÄ±nÄ± iÃ§erir ve bunlara `.coef_` Ã¶zelliÄŸi aracÄ±lÄ±ÄŸÄ±yla eriÅŸilebilir. Bizim durumumuzda, sadece bir katsayÄ± vardÄ±r ve bu yaklaÅŸÄ±k `-0.017` civarÄ±nda olmalÄ±dÄ±r. Bu, fiyatlarÄ±n zamanla biraz azaldÄ±ÄŸÄ±nÄ± ancak Ã§ok fazla olmadÄ±ÄŸÄ±nÄ±, yaklaÅŸÄ±k olarak gÃ¼nde 2 sent dÃ¼ÅŸtÃ¼ÄŸÃ¼nÃ¼ gÃ¶sterir. Regresyonun Y ekseniyle kesiÅŸtiÄŸi noktaya ise `lin_reg.intercept_` kullanÄ±larak eriÅŸilebilir - bizim durumumuzda bu yaklaÅŸÄ±k `21` olacak ve yÄ±lÄ±n baÅŸÄ±ndaki fiyatÄ± gÃ¶sterecektir.

Modelimizin ne kadar doÄŸru olduÄŸunu gÃ¶rmek iÃ§in test veri seti Ã¼zerinde fiyatlarÄ± tahmin edebilir ve ardÄ±ndan tahminlerimizin beklenen deÄŸerlere ne kadar yakÄ±n olduÄŸunu Ã¶lÃ§ebiliriz. Bu, beklenen ve tahmin edilen deÄŸer arasÄ±ndaki tÃ¼m kare farklarÄ±nÄ±n ortalamasÄ± olan ortalama kare hata (MSE) metriÄŸi ile yapÄ±labilir.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

Hata oranÄ±mÄ±z yaklaÅŸÄ±k 2 puan, yani ~%17 civarÄ±nda gÃ¶rÃ¼nÃ¼yor. Ã‡ok iyi deÄŸil. Model kalitesinin diÄŸer bir gÃ¶stergesi ise **belirleme katsayÄ±sÄ±**dÄ±r ve bu ÅŸÃ¶yle elde edilir:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
EÄŸer deÄŸer 0 ise, modelin giriÅŸ verilerini dikkate almadÄ±ÄŸÄ± ve *en kÃ¶tÃ¼ doÄŸrusal tahminci* gibi davrandÄ±ÄŸÄ± anlamÄ±na gelir, bu da sonuÃ§larÄ±n sadece ortalamasÄ± anlamÄ±na gelir. 1 deÄŸeri ise tÃ¼m beklenen Ã§Ä±ktÄ±larÄ± mÃ¼kemmel ÅŸekilde tahmin edebildiÄŸimizi gÃ¶sterir. Bizim durumumuzda, katsayÄ± yaklaÅŸÄ±k 0.06, bu da oldukÃ§a dÃ¼ÅŸÃ¼ktÃ¼r.

Regresyonun nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± daha iyi gÃ¶rmek iÃ§in test verilerini ve regresyon doÄŸrusunu birlikte Ã§izebiliriz:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linear regression" src="../../../../translated_images/tr/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## Polinom Regresyonu

Lineer Regresyonun bir baÅŸka tÃ¼rÃ¼ de Polinom Regresyonudur. Bazen deÄŸiÅŸkenler arasÄ±nda doÄŸrusal bir iliÅŸki olur - Ã¶rneÄŸin, balkabaÄŸÄ±nÄ±n hacmi ne kadar bÃ¼yÃ¼kse, fiyat da o kadar yÃ¼ksek olur - bazen bu iliÅŸkiler dÃ¼z bir dÃ¼zlem veya doÄŸru olarak Ã§izilemez.

âœ… Ä°ÅŸte Polinom Regresyonu iÃ§in kullanÄ±labilecek [baÅŸka Ã¶rnekler](https://online.stat.psu.edu/stat501/lesson/9/9.8)

Tarih ve Fiyat arasÄ±ndaki iliÅŸkiye yeniden bakÄ±n. Bu daÄŸÄ±lÄ±m grafiÄŸi gerÃ§ekten dÃ¼z bir doÄŸru ile analiz edilmeli mi? Fiyatlar dalgalanamaz mÄ±? Bu durumda polinom regresyonu deneyebilirsiniz.

âœ… Polinomlar, bir veya daha fazla deÄŸiÅŸken ve katsayÄ± iÃ§erebilen matematiksel ifadeleridir.

Polinom regresyon, doÄŸrusal olmayan verilere daha iyi uymasÄ± iÃ§in eÄŸri bir Ã§izgi oluÅŸturur. Bizim durumumuzda, giriÅŸ verisine kare `DayOfYear` deÄŸiÅŸkeni eklendiÄŸinde, verilerimizi yÄ±l iÃ§inde belirli bir noktada minimuma sahip bir parabolik eÄŸri ile uydurabilmeliyiz.

Scikit-learn, veri iÅŸleme adÄ±mlarÄ±nÄ± birleÅŸtirmek iÃ§in faydalÄ± bir [pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) iÃ§erir. Bir **pipeline** bir **tahminci** zinciridir. Bizim durumumuzda, Ã¶nce modele polinom Ã¶zellikler ekleyen ve ardÄ±ndan regresyonu eÄŸiten bir pipeline oluÅŸturacaÄŸÄ±z:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

`PolynomialFeatures(2)` kullanmak, giriÅŸ verisinden tÃ¼m ikinci dereceden polinomlarÄ± dahil edeceÄŸimiz anlamÄ±na gelir. Bizim Ã¶rneÄŸimizde bu yalnÄ±zca `DayOfYear`<sup>2</sup> demektir, ancak iki giriÅŸ deÄŸiÅŸkeni X ve Y verilirse, bu X<sup>2</sup>, XY ve Y<sup>2</sup> Ã¶ÄŸelerini ekler. Dilerseniz daha yÃ¼ksek dereceli polinomlar da kullanabilirsiniz.

Pipeline'lar, orijinal `LinearRegression` nesnesi gibi kullanÄ±labilir; yani pipeline'Ä± `fit` edebilir ve ardÄ±ndan tahmin sonuÃ§larÄ± almak iÃ§in `predict` kullanabilirsiniz. Ä°ÅŸte test verisi ve yaklaÅŸÄ±klaÅŸtÄ±rma eÄŸrisini gÃ¶steren grafik:

<img alt="Polynomial regression" src="../../../../translated_images/tr/poly-results.ee587348f0f1f60b.webp" width="50%" />

Polinom Regresyon kullanarak MSE biraz daha dÃ¼ÅŸÃ¼k ve belirleme katsayÄ±sÄ± biraz daha yÃ¼ksek olabilir, ama Ã§ok bÃ¼yÃ¼k bir fark olmaz. DiÄŸer Ã¶zellikleri de gÃ¶z Ã¶nÃ¼nde bulundurmamÄ±z gerekir!

> Minimum balkabaÄŸÄ± fiyatlarÄ±nÄ±n CadÄ±lar BayramÄ± civarÄ±nda gÃ¶zlemlendiÄŸini gÃ¶rebilirsiniz. Bunu nasÄ±l aÃ§Ä±klarsÄ±nÄ±z?

ğŸƒ Tebrikler, balkabaÄŸÄ± turta fiyatÄ±nÄ± tahmin eden bir model oluÅŸturdunuz. Muhtemelen tÃ¼m balkabaÄŸÄ± tÃ¼rleri iÃ§in aynÄ± iÅŸlemi tekrarlayabilirsiniz, ama bu zahmetli olur. Åimdi modelimizde balkabaÄŸÄ± Ã§eÅŸidini nasÄ±l dikkate alacaÄŸÄ±mÄ±zÄ± Ã¶ÄŸrenelim!

## Kategorik Ã–zellikler

Ä°deal dÃ¼nyada, farklÄ± balkabaÄŸÄ± Ã§eÅŸitleri iÃ§in fiyatlarÄ± aynÄ± modelle tahmin etmek isteriz. Ancak, `Variety` sÃ¼tunu `Month` gibi sÃ¼tunlardan farklÄ±dÄ±r Ã§Ã¼nkÃ¼ sayÄ±sal olmayan deÄŸerler iÃ§erir. Bu tÃ¼r sÃ¼tunlara **kategorik** denir.

[![ML for beginners - Categorical Feature Predictions with Linear Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML for beginners - Categorical Feature Predictions with Linear Regression")

> ğŸ¥ Kategorik Ã¶zelliklerin kullanÄ±mÄ±na dair kÄ±sa video Ã¶zetini izlemek iÃ§in yukarÄ±daki resme tÄ±klayÄ±n.

Burada ortalama fiyatÄ±n Ã§eÅŸitliliÄŸe nasÄ±l baÄŸlÄ± olduÄŸunu gÃ¶rebilirsiniz:

<img alt="Average price by variety" src="../../../../translated_images/tr/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

Ã‡eÅŸidi dikkate almak iÃ§in Ã¶ncelikle sayÄ±sal forma Ã§evirmemiz veya **kodlamamÄ±z** gerekir. Bunu yapmanÄ±n birkaÃ§ yolu vardÄ±r:

* Basit **sayÄ±sal kodlama**, farklÄ± Ã§eÅŸitlerin bir tablosunu oluÅŸturur ve ardÄ±ndan Ã§eÅŸit adÄ±nÄ± bu tablodaki bir indeksle deÄŸiÅŸtirir. Bu lineer regresyon iÃ§in en iyi yÃ¶ntem deÄŸildir, Ã§Ã¼nkÃ¼ lineer regresyon indeksin gerÃ§ek sayÄ±sal deÄŸerini alÄ±p bir katsayÄ± ile Ã§arpar ve sonuca ekler. Bizim durumumuzda, indeks numarasÄ± ile fiyat arasÄ±ndaki iliÅŸki aÃ§Ä±kÃ§a doÄŸrusal deÄŸildir; hatta indekslerin belirli bir ÅŸekilde sÄ±ralanmasÄ± garanti edilse bile.
* **One-hot encoding** ile `Variety` sÃ¼tunu, her bir Ã§eÅŸit iÃ§in 4 farklÄ± sÃ¼tuna bÃ¶lÃ¼nÃ¼r. Her sÃ¼tun, ilgili satÄ±r o Ã§eÅŸide aitse `1`, deÄŸilse `0` iÃ§erir. Bu, lineer regresyonda balkabaÄŸÄ± Ã§eÅŸidi baÅŸÄ±na biri "baÅŸlangÄ±Ã§ fiyatÄ±" (ya da "ek fiyat") iÃ§in olmak Ã¼zere dÃ¶rt katsayÄ± olacaÄŸÄ± anlamÄ±na gelir.

AÅŸaÄŸÄ±daki kod, bir Ã§eÅŸidin one-hot kodlamasÄ±nÄ± nasÄ±l yapabileceÄŸimizi gÃ¶steriyor:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

One-hot kodlanmÄ±ÅŸ Ã§eÅŸidi giriÅŸ olarak kullanarak lineer regresyonu eÄŸitmek iÃ§in sadece `X` ve `y` verilerini doÄŸru ÅŸekilde baÅŸlatmamÄ±z gerekir:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Geri kalan kod, yukarÄ±da lineer regresyonu eÄŸittiÄŸimiz kod ile aynÄ±dÄ±r. Denerseniz, ortalama kare hatasÄ±nÄ±n neredeyse aynÄ± olduÄŸunu ancak belirleme katsayÄ±sÄ±nÄ±n (~%77) Ã§ok daha yÃ¼ksek olduÄŸunu gÃ¶rÃ¼rsÃ¼nÃ¼z. Daha doÄŸru tahminler elde etmek iÃ§in daha fazla kategorik Ã¶zellik ile `Month` veya `DayOfYear` gibi sayÄ±sal Ã¶zellikleri de dikkate alabiliriz. TÃ¼m Ã¶zellikleri tek bir bÃ¼yÃ¼k diziye dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in `join` kullanabiliriz:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Burada ayrÄ±ca `City` ve `Package` tÃ¼rÃ¼ de dikkate alÄ±nmÄ±ÅŸtÄ±r, bu bize 2.84 (%%10) MSE ve 0.94 belirleme katsayÄ±sÄ± saÄŸlar!

## Hepsini Bir Araya Getirmek

En iyi modeli yapmak iÃ§in yukarÄ±daki Ã¶rnekten (one-hot kodlanmÄ±ÅŸ kategorik + sayÄ±sal veriler) ve Polinom Regresyonu birlikte kullanabiliriz. Ä°ÅŸte kolayÄ±nÄ±z iÃ§in tam kod:

```python
# eÄŸitim verilerini ayarla
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# eÄŸitim-test bÃ¶lÃ¼mÃ¼ yap
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# pipeline'Ä± kur ve eÄŸit
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# test verileri iÃ§in sonuÃ§larÄ± tahmin et
pred = pipeline.predict(X_test)

# MSE ve belirleme katsayÄ±sÄ±nÄ± hesapla
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Bu en iyi belirleme katsayÄ±sÄ± olan neredeyse %97 ve MSE=2.23 (yaklaÅŸÄ±k %8 tahmin hatasÄ±) verecektir.

| Model | MSE | Belirleme KatsayÄ±sÄ± |
|-------|-----|---------------------|
| `DayOfYear` Lineer | 2.77 (17.2%) | 0.07 |
| `DayOfYear` Polinom | 2.73 (17.0%) | 0.08 |
| `Variety` Lineer | 5.24 (19.7%) | 0.77 |
| TÃ¼m Ã¶zellikler Lineer | 2.84 (10.5%) | 0.94 |
| TÃ¼m Ã¶zellikler Polinom | 2.23 (8.25%) | 0.97 |

ğŸ† Ã‡ok iyi! Bu derste dÃ¶rt farklÄ± Regresyon modeli oluÅŸturup model kalitesini %97â€™ye geliÅŸtirdiniz. Son Regresyon bÃ¶lÃ¼mÃ¼nde, kategorileri belirlemek iÃ§in Lojistik Regresyonu Ã¶ÄŸreneceksiniz.

---
## ğŸš€Meydan Okuma

Bu not defterinde farklÄ± deÄŸiÅŸkenlerle deney yaparak korelasyonun model doÄŸruluÄŸuna nasÄ±l karÅŸÄ±lÄ±k geldiÄŸini test edin.

## [Ders sonrasÄ± quiz](https://ff-quizzes.netlify.app/en/ml/)

## GÃ¶zden GeÃ§irme & Kendi Kendine Ã‡alÄ±ÅŸma

Bu derste Lineer Regresyonu Ã¶ÄŸrendik. BaÅŸka Ã¶nemli Regresyon tÃ¼rleri de vardÄ±r. Stepwise, Ridge, Lasso ve Elasticnet teknikleri hakkÄ±nda okuyun. Daha fazla Ã¶ÄŸrenmek iÃ§in iyi bir kurs [Stanford Ä°statistiksel Ã–ÄŸrenme kursu](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## Ã–dev

[Bir Model OluÅŸtur](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Feragatname**:
Bu belge, AI Ã§eviri servisi [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stermemize raÄŸmen, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Orijinal belge, kendi dilinde yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ± sonucu ortaya Ã§Ä±kabilecek yanlÄ±ÅŸ anlaÅŸÄ±lmalardan veya yorum hatalarÄ±ndan sorumlu deÄŸiliz.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->