# Bygg en regressionsmodell med Scikit-learn: regression p√• fyra s√§tt

## Nyb√∂rjarmeddelande

Linj√§r regression anv√§nds n√§r vi vill f√∂ruts√§ga ett **numeriskt v√§rde** (till exempel huspris, temperatur eller f√∂rs√§ljning).  
Det fungerar genom att hitta en r√§t linje som b√§st representerar sambandet mellan indatafunktioner och utdata.

I denna lektion fokuserar vi p√• att f√∂rst√• konceptet innan vi utforskar mer avancerade regressionstekniker.  
![Linear vs polynomial regression infographic](../../../../translated_images/sv/linear-polynomial.5523c7cb6576ccab.webp)  
> Informationsgrafik av [Dasani Madipalli](https://twitter.com/dasani_decoded)  
## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

> ### [Den h√§r lektionen finns tillg√§nglig i R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)  
### Introduktion

Hittills har du utforskat vad regression √§r med hj√§lp av exempeldata fr√•n pumpaprisdatasetet som vi kommer anv√§nda i hela denna lektion. Du har ocks√• visualiserat det med Matplotlib.

Nu √§r du redo att f√∂rdjupa dig i regression f√∂r ML. Medan visualisering hj√§lper dig att f√∂rst√• data, kommer den verkliga styrkan i maskininl√§rning fr√•n _tr√§nade modeller_. Modeller tr√§nas p√• historisk data f√∂r att automatiskt f√•nga databeroenden, och de l√•ter dig f√∂ruts√§ga utfall f√∂r nya data som modellen inte har sett tidigare.

I denna lektion kommer du att l√§ra dig mer om tv√• typer av regression: _grundl√§ggande linj√§r regression_ och _polynomregression_, tillsammans med en del av matematiken bakom dessa tekniker. Dessa modeller kommer att l√•ta oss f√∂ruts√§ga pumpapris beroende p√• olika indata.

[![ML for beginners - Understanding Linear Regression](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML for beginners - Understanding Linear Regression")

> üé• Klicka p√• bilden ovan f√∂r en kort video√∂versikt av linj√§r regression.

> Genom hela detta utbildningsprogram antar vi minimala kunskaper i matematik och str√§var efter att g√∂ra det tillg√§ngligt f√∂r studenter fr√•n andra omr√•den, s√• h√•ll utkik efter anteckningar, üßÆ markeringar, diagram och andra inl√§rningsverktyg f√∂r att underl√§tta f√∂rst√•elsen.

### F√∂rkunskaper

Du b√∂r nu vara bekant med strukturen i pumpadata som vi unders√∂ker. Den finns f√∂rladdad och f√∂rberedd rengjord i denna lektions _notebook.ipynb_-fil.  
I filen visas pumpapriset per bushel i en ny data frame. Se till att du kan k√∂ra dessa notebooks i k√§rnor i Visual Studio Code.

### F√∂rberedelse

Som en p√•minnelse laddar du in denna data f√∂r att kunna st√§lla fr√•gor om den.

- N√§r √§r b√§sta tidpunkten att k√∂pa pumpor?  
- Vilket pris kan jag f√∂rv√§nta mig f√∂r ett fall med miniatyrpumpor?  
- B√∂r jag k√∂pa dem i halva bushel-korgar eller i 1 1/9 bushel-l√•dor?  
L√•t oss forts√§tta att gr√§va i denna data.

I f√∂reg√•ende lektion skapade du en Pandas data frame och fyllde den med en del av den ursprungliga datasetet, standardiserande priser per bushel. Genom att g√∂ra detta kunde du dock bara samla in ungef√§r 400 datapunkter och endast f√∂r h√∂stm√•naderna.

Ta en titt p√• datan som vi f√∂rladdat i denna lektions tillh√∂rande notebook. Data √§r f√∂rladdad och en initial spridningsdiagram √§r ritad f√∂r att visa m√•nadsdata. Kanske kan vi f√• lite mer insikt om datan genom att reng√∂ra den mer.

## En linj√§r regressionslinje

Som du l√§rde dig i Lektion 1 √§r m√•let med ett linj√§rt regressions√∂vning att kunna rita en linje f√∂r att:

- **Visa variabelrelationer**. Visa relationen mellan variabler  
- **G√∂ra f√∂ruts√§gelser**. G√∂ra noggranna f√∂ruts√§gelser om var en ny datapunkt skulle hamna i relation till den linjen.

Det √§r typiskt f√∂r **Minsta Kvadrat-regression** att rita denna typ av linje. Begreppet "Minsta Kvadrat" syftar p√• processen att minimera den totala felet i v√•r modell. F√∂r varje datapunkt m√§ter vi det vertikala avst√•ndet (kallat residual) mellan den faktiska punkten och v√•r regressionslinje.

Vi kvadrerar dessa avst√•nd av tv√• huvudsakliga sk√§l:

1. **Storlek √∂ver riktning:** Vi vill behandla ett fel p√• -5 lika som ett fel p√• +5. Kvadrering g√∂r alla v√§rden positiva.

2. **Straffa avvikare:** Kvadrering ger st√∂rre vikt √•t st√∂rre fel och tvingar linjen att stanna n√§rmare punkter som ligger l√•ngt ifr√•n.

Vi summerar sedan alla dessa kvadrerade v√§rden. V√•rt m√•l √§r att hitta den specifika linje d√§r denna slutliga summa √§r som minst (det minsta m√∂jliga v√§rdet) ‚Äì d√§rav namnet "Minsta Kvadrat".

> **üßÆ Visa mig matematiken**  
>  
> Denna linje, kallad _b√§sta anpassningslinje_ kan uttryckas med [en ekvation](https://en.wikipedia.org/wiki/Simple_linear_regression):  
>  
> ```
> Y = a + bX
> ```
>  
> `X` √§r den 'f√∂rklarande variabeln'. `Y` √§r den 'beroende variabeln'. Linjens lutning √§r `b` och `a` √§r y-axelns intercept, vilket avser v√§rdet p√• `Y` n√§r `X = 0`.  
>  
>![calculate the slope](../../../../translated_images/sv/slope.f3c9d5910ddbfcf9.webp)  
>  
> F√∂rst ber√§kna lutningen `b`. Informationsgrafik av [Jen Looper](https://twitter.com/jenlooper)  
>  
> Med andra ord, och med h√§nvisning till v√•r pumpadata ursprungliga fr√•ga: "f√∂ruts√§g priset p√• en pumpa per bushel efter m√•nad", skulle `X` avse priset och `Y` avse f√∂rs√§ljningsm√•naden.  
>  
>![complete the equation](../../../../translated_images/sv/calculation.a209813050a1ddb1.webp)  
>  
> Ber√§kna v√§rdet p√• Y. Om du betalar runt 4 dollar m√•ste det vara april! Informationsgrafik av [Jen Looper](https://twitter.com/jenlooper)  
>  
> Den matematik som ber√§knar linjen m√•ste visa lutningen p√• linjen, som ocks√• beror p√• interceptet, eller var `Y` befinner sig n√§r `X = 0`.  
>  
> Du kan observera ber√§kningsmetoden f√∂r dessa v√§rden p√• [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) webbplats. Bes√∂k ocks√• [denna Minsta Kvadrat-kalkylator](https://www.mathsisfun.com/data/least-squares-calculator.html) f√∂r att se hur v√§rdenas storlek p√•verkar linjen.

## Korrelationskoefficient

Ett till begrepp att f√∂rst√• √§r **korrelationskoefficienten** mellan givna X- och Y-variabler. Med hj√§lp av ett spridningsdiagram kan du snabbt visualisera denna koefficient. Ett diagram med datapunkter utspridda i en prydlig linje har h√∂g korrelation medan ett diagram med datapunkter utspridda √∂verallt mellan X och Y har l√•g korrelation.

En bra linj√§r regressionsmodell √§r en som har h√∂g (n√§rmare 1 √§n 0) korrelationskoefficient med metoden Minsta Kvadrat Regression med regressionslinjen.

‚úÖ K√∂r notebooken som h√∂r till denna lektion och titta p√• spridningsdiagrammet M√•nad till Pris. Verkar datan som kopplar M√•nad till Pris f√∂r pumpaf√∂rs√§ljning ha h√∂g eller l√•g korrelation enligt din visuella tolkning av spridningsdiagrammet? √Ñndras det om du anv√§nder en mer detaljerad m√§tning ist√§llet f√∂r `Month`, t.ex. *dag p√• √•ret* (d.v.s. antal dagar sedan √•rets b√∂rjan)?

I koden nedan antar vi att vi har rengjort data och f√•tt en dataframe kallad `new_pumpkins`, liknande f√∂ljande:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price  
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364  
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636  
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636  
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545  
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364  

> Koden f√∂r att reng√∂ra data finns tillg√§nglig i [`notebook.ipynb`](notebook.ipynb). Vi har utf√∂rt samma reng√∂ringssteg som i f√∂reg√•ende lektion och ber√§knat kolumnen `DayOfYear` med f√∂ljande uttryck:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```
  
Nu n√§r du har en f√∂rst√•else f√∂r matematiken bakom linj√§r regression, l√•t oss skapa en regressionsmodell f√∂r att se om vi kan f√∂ruts√§ga vilken pumpkinf√∂rpackning som kommer att ha de b√§sta pumpapriserna. N√•gon som k√∂per pumpor f√∂r en h√∂gtidlig pumpaplats kanske vill ha denna information f√∂r att kunna optimera sina ink√∂p av pumpkinpaket f√∂r sin plats.

## Letar efter korrelation

[![ML for beginners - Looking for Correlation: The Key to Linear Regression](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML for beginners - Looking for Correlation: The Key to Linear Regression")

> üé• Klicka p√• bilden ovan f√∂r en kort video√∂versikt √∂ver korrelation.

Fr√•n f√∂reg√•ende lektion har du f√∂rmodligen sett att genomsnittspriset f√∂r olika m√•nader ser ut s√• h√§r:

<img alt="Average price by month" src="../../../../translated_images/sv/barchart.a833ea9194346d76.webp" width="50%"/>

Detta antyder att det borde finnas n√•gon korrelation, och vi kan f√∂rs√∂ka tr√§na en linj√§r regressionsmodell f√∂r att f√∂ruts√§ga sambandet mellan `Month` och `Price`, eller mellan `DayOfYear` och `Price`. H√§r √§r spridningsdiagrammet som visar det senare f√∂rh√•llandet:

<img alt="Scatter plot of Price vs. Day of Year" src="../../../../translated_images/sv/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" /> 

L√•t oss se om det finns en korrelation med funktionen `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```
  
Det verkar som att korrelationen √§r ganska liten, -0,15 f√∂r `Month` och -0,17 f√∂r `DayOfYear`, men det kan finnas ett annat viktigt samband. Det ser ut som att det finns olika kluster av priser som motsvarar olika pumparter. F√∂r att bekr√§fta denna hypotes, l√•t oss plotta varje pumpkategori med en annan f√§rg. Genom att skicka en `ax`-parameter till `scatter`-plotfunktionen kan vi rita alla punkter i samma diagram:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```
  
<img alt="Scatter plot of Price vs. Day of Year" src="../../../../translated_images/sv/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" /> 

V√•r unders√∂kning tyder p√• att sorten har st√∂rre effekt p√• priset √§n sj√§lva f√∂rs√§ljningsdatumet. Vi kan se detta med ett stapeldiagram:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```
  
<img alt="Bar graph of price vs variety" src="../../../../translated_images/sv/price-by-variety.744a2f9925d9bcb4.webp" width="50%" /> 

L√•t oss f√∂r tillf√§llet fokusera endast p√• en pumpart, 'pie type', och se vilken effekt datumet har p√• priset:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Scatter plot of Price vs. Day of Year" src="../../../../translated_images/sv/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" /> 

Om vi nu ber√§knar korrelationen mellan `Price` och `DayOfYear` med funktionen `corr`, f√•r vi n√•got runt `-0.27` ‚Äì vilket betyder att det √§r vettigt att tr√§na en prediktiv modell.

> Innan du tr√§nar en linj√§r regressionsmodell √§r det viktigt att se till att v√•r data √§r ren. Linj√§r regression fungerar inte bra med saknade v√§rden, s√• det √§r vettigt att bli av med alla tomma celler:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```
  
Ett annat tillv√§gag√•ngss√§tt √§r att fylla de tomma v√§rdena med medelv√§rden fr√•n motsvarande kolumn.

## Enkel linj√§r regression

[![ML for beginners - Linear and Polynomial Regression using Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML for beginners - Linear and Polynomial Regression using Scikit-learn")

> üé• Klicka p√• bilden ovan f√∂r en kort video√∂versikt √∂ver linj√§r och polynomisk regression.

F√∂r att tr√§na v√•r linj√§ra regressionsmodell kommer vi att anv√§nda **Scikit-learn**-biblioteket.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```
  
Vi b√∂rjar med att separera indata (features) och f√∂rv√§ntad utdata (label) i separata numpy-arrays:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```
  
> Observera att vi var tvungna att g√∂ra en `reshape` p√• indatan f√∂r att Linear Regression-paketet ska f√∂rst√• den korrekt. Linj√§r regression f√∂rv√§ntar sig en 2D-array som indata, d√§r varje rad i arrayen motsvarar en vektor av indatafunktioner. I v√•rt fall, eftersom vi bara har en indata, beh√∂ver vi en array med formen N√ó1, d√§r N √§r datasetets storlek.

Sedan m√•ste vi dela datan i tr√§nings- och testdataupps√§ttningar s√• att vi kan validera v√•r modell efter tr√§ning:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```
  
Slutligen tar det bara tv√• kodrader att tr√§na den faktiska linj√§ra regressionsmodellen. Vi definierar `LinearRegression`-objektet och anpassar det till v√•r data med metoden `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

`LinearRegression`-objektet efter att ha anpassats inneh√•ller alla regressionskoefficienter, vilka kan n√•s via `.coef_`-egenskapen. I v√•rt fall finns det bara en koefficient, som b√∂r vara runt `-0.017`. Det betyder att priserna verkar sjunka lite med tiden, men inte s√• mycket, ungef√§r 2 cent per dag. Vi kan ocks√• n√• sk√§rningspunkten f√∂r regressionen med Y-axeln genom `lin_reg.intercept_` - den kommer i v√•rt fall vara runt `21`, vilket indikerar priset i b√∂rjan av √•ret.

F√∂r att se hur korrekt v√•r modell √§r kan vi f√∂ruts√§ga priser p√• en testdatam√§ngd och sedan m√§ta hur n√§ra v√•ra f√∂ruts√§gelser √§r de f√∂rv√§ntade v√§rdena. Detta kan g√∂ras med hj√§lp av medelkvadratfel (MSE), vilket √§r medelv√§rdet av alla kvadrerade skillnader mellan f√∂rv√§ntat och f√∂rutsagt v√§rde.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

V√•rt fel verkar vara runt 2 po√§ng, vilket √§r ~17%. Inte s√• bra. En annan indikator p√• modellens kvalitet √§r **best√§mningskoefficienten**, som kan erh√•llas s√• h√§r:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Om v√§rdet √§r 0 betyder det att modellen inte tar inputdata i beaktande, och agerar som den *s√§msta linj√§ra prediktorn*, vilket helt enkelt √§r medelv√§rdet av resultatet. V√§rdet 1 inneb√§r att vi perfekt kan f√∂ruts√§ga alla f√∂rv√§ntade utg√•ngar. I v√•rt fall √§r koefficienten runt 0,06, vilket √§r ganska l√•gt.

Vi kan ocks√• plotta testdata tillsammans med regressionslinjen f√∂r att b√§ttre se hur regressionen fungerar i v√•rt fall:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linear regression" src="../../../../translated_images/sv/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## Polynomregression

En annan typ av linj√§r regression √§r polynomregression. Medan det ibland finns ett linj√§rt samband mellan variabler ‚Äì ju st√∂rre pumpan i volym, desto h√∂gre pris ‚Äì kan dessa samband ibland inte plottas som ett plan eller rakt linje.

‚úÖ H√§r √§r [fler exempel](https://online.stat.psu.edu/stat501/lesson/9/9.8) p√• data som kan anv√§nda polynomregression

Titta igen p√• sambandet mellan Datum och Pris. Verkar detta spridningsdiagram som n√•got som n√∂dv√§ndigtvis b√∂r analyseras med en r√§t linje? Kan inte priserna fluktuera? I det h√§r fallet kan du prova polynomregression.

‚úÖ Polynom √§r matematiska uttryck som kan best√• av en eller flera variabler och koefficienter

Polynomregression skapar en kurvad linje f√∂r att b√§ttre passa icke-linj√§ra data. I v√•rt fall, om vi inkluderar en kvadratisk `DayOfYear`-variabel i indata, b√∂r vi kunna anpassa v√•ra data med en parabolisk kurva, som kommer att ha ett minimum vid en viss punkt under √•ret.

Scikit-learn inneh√•ller ett anv√§ndbart [pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) f√∂r att kombinera olika steg i databehandlingen. En **pipeline** √§r en kedja av **estimators**. I v√•rt fall skapar vi en pipeline som f√∂rst l√§gger till polynomfunktioner till v√•r modell och sedan tr√§nar regressionen:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Att anv√§nda `PolynomialFeatures(2)` inneb√§r att vi kommer att inkludera alla andragradspolynom fr√•n inputdata. I v√•rt fall betyder det bara `DayOfYear`<sup>2</sup>, men med tv√• inputvariabler X och Y skulle detta l√§gga till X<sup>2</sup>, XY och Y<sup>2</sup>. Vi kan ocks√• anv√§nda h√∂gre gradens polynom om vi vill.

Pipelines kan anv√§ndas p√• samma s√§tt som det ursprungliga `LinearRegression`-objektet, dvs vi kan `fit` pipelinen, och sedan anv√§nda `predict` f√∂r att f√• f√∂ruts√§gelser. H√§r √§r grafen som visar testdata och approximationskurvan:

<img alt="Polynomial regression" src="../../../../translated_images/sv/poly-results.ee587348f0f1f60b.webp" width="50%" />

Med polynomregression kan vi f√• n√•got l√§gre MSE och h√∂gre best√§mning, men inte signifikant. Vi beh√∂ver ta h√§nsyn till andra egenskaper!

> Du kan se att de l√§gsta pumpapriserna observeras runt Halloween. Hur kan du f√∂rklara detta?

üéÉ Grattis, du skapade precis en modell som kan hj√§lpa till att f√∂ruts√§ga priset p√• pajpumpor. Du kan f√∂rmodligen upprepa samma procedur f√∂r alla pumpatyper, men det vore tr√•kigt. L√•t oss nu l√§ra oss hur vi tar pumpasort i beaktning i v√•r modell!

## Kategoriska egenskaper

I en perfekt v√§rld vill vi kunna f√∂ruts√§ga priser f√∂r olika pumpasorter med samma modell. Dock skiljer sig kolumnen `Variety` n√•got fr√•n kolumner som `Month`, eftersom den inneh√•ller icke-numeriska v√§rden. S√•dana kolumner kallas **kategoriska**.

[![ML for beginners - Categorical Feature Predictions with Linear Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML for beginners - Categorical Feature Predictions with Linear Regression")

> üé• Klicka p√• bilden ovan f√∂r en kort video√∂versikt om anv√§ndning av kategoriska egenskaper.

H√§r kan du se hur medelpriset beror p√• sort:

<img alt="Average price by variety" src="../../../../translated_images/sv/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

F√∂r att ta sort i beaktning m√•ste vi f√∂rst konvertera den till ett numeriskt format, eller **koda** den. Det finns flera s√§tt att g√∂ra det:

* Enkel **numerisk kodning** skapar en tabell med olika sorter och ers√§tter sedan sortnamnet med ett index i den tabellen. Detta √§r inte b√§sta id√© f√∂r linj√§r regression, eftersom linj√§r regression tar det faktiska numeriska v√§rdet p√• indexet och adderar det till resultatet, multiplicerat med n√•gon koefficient. I v√•rt fall √§r sambandet mellan indexnummer och pris uppenbart icke-linj√§rt, √§ven om vi ser till att indexen ordnas p√• n√•got specifikt s√§tt.
* **One-hot-encoding** ers√§tter `Variety`-kolumnen med 4 olika kolumner, en f√∂r varje sort. Varje kolumn inneh√•ller `1` om motsvarande rad √§r av en viss sort, och `0` annars. Det inneb√§r att det kommer att finnas fyra koefficienter i linj√§r regression, en f√∂r varje pumpsort, ansvarig f√∂r "startpris" (eller snarare "till√§ggspris") f√∂r just den sorten.

Koden nedan visar hur vi kan one-hot-koda en sort:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

F√∂r att tr√§na linj√§r regression med one-hot-kodad sort som input beh√∂ver vi bara initiera `X` och `y` data korrekt:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Resten av koden √§r densamma som vi anv√§nde ovan f√∂r att tr√§na linj√§r regression. Om du provar detta kommer du se att medelkvadratfelet blir ungef√§r detsamma, men vi f√•r en mycket h√∂gre best√§mningskoefficient (~77%). F√∂r att f√• √§nnu mer exakta f√∂ruts√§gelser kan vi ta h√§nsyn till fler kategoriska egenskaper, samt numeriska egenskaper som `Month` eller `DayOfYear`. F√∂r att f√• en stor array med egenskaper kan vi anv√§nda `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

H√§r tar vi ocks√• h√§nsyn till `City` och `Package`-typ, vilket ger oss MSE 2.84 (10%) och best√§mning 0.94!

## Att s√§tta ihop allt

F√∂r att g√∂ra den b√§sta modellen kan vi anv√§nda kombinerade (one-hot-kodade kategoriska + numeriska) data fr√•n f√∂reg√•ende exempel tillsammans med polynomregression. H√§r √§r den kompletta koden f√∂r din bekv√§mlighet:

```python
# skapa tr√§ningsdata
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# g√∂r f√∂rdelning av tr√§nings- och testdata
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# konfigurera och tr√§na pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# f√∂ruts√§g resultat f√∂r testdata
pred = pipeline.predict(X_test)

# ber√§kna MSE och f√∂rklaringsgrad
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Detta b√∂r ge oss den b√§sta best√§mningskoefficienten p√• n√§stan 97% och MSE=2.23 (~8% f√∂ruts√§gelsefel).

| Modell | MSE | Best√§mning |
|-------|-----|------------|
| `DayOfYear` Linj√§r | 2.77 (17.2%) | 0.07 |
| `DayOfYear` Polynom | 2.73 (17.0%) | 0.08 |
| `Variety` Linj√§r | 5.24 (19.7%) | 0.77 |
| Alla egenskaper Linj√§r | 2.84 (10.5%) | 0.94 |
| Alla egenskaper Polynom | 2.23 (8.25%) | 0.97 |

üèÜ Bra jobbat! Du skapade fyra regressionsmodeller p√• en lektion och f√∂rb√§ttrade modellens kvalitet till 97%. I det sista avsnittet om regression f√•r du l√§ra dig om logistisk regression f√∂r att best√§mma kategorier.

---
## üöÄUtmaning

Testa flera olika variabler i den h√§r anteckningsboken f√∂r att se hur korrelationen motsvarar modellens noggrannhet.

## [Quiz efter f√∂rel√§sning](https://ff-quizzes.netlify.app/en/ml/)

## Genomg√•ng & Sj√§lvstudier

I den h√§r lektionen l√§rde vi oss om linj√§r regression. Det finns andra viktiga typer av regression. L√§s om Stepwise, Ridge, Lasso och Elasticnet-tekniker. En bra kurs att studera f√∂r att l√§ra sig mer √§r [Stanford Statistical Learning course](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## Uppgift

[Bygg en modell](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfriskrivning**:
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). Trots att vi str√§var efter noggrannhet, b√∂r du vara medveten om att automatiska √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess modersm√•l ska betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r n√•gra missf√∂rst√•nd eller feltolkningar som uppst√•r fr√•n anv√§ndningen av denna √∂vers√§ttning.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->