# K√∂kklassificerare 2

I denna andra klassificeringslektion kommer du att utforska fler s√§tt att klassificera numeriska data. Du kommer ocks√• att l√§ra dig om konsekvenserna av att v√§lja en klassificerare √∂ver en annan.

## [F√∂rf√∂rel√§sningsquiz](https://ff-quizzes.netlify.app/en/ml/)

### F√∂rkunskaper

Vi antar att du har slutf√∂rt de tidigare lektionerna och har en rensad dataset i din `data`-mapp som heter _cleaned_cuisines.csv_ i roten av denna 4-lektionsmapp.

### F√∂rberedelse

Vi har laddat din _notebook.ipynb_-fil med den rensade datasetet och har delat upp den i X- och y-dataframes, redo f√∂r modellbyggnadsprocessen.

## En klassificeringskarta

Tidigare l√§rde du dig om de olika alternativen du har n√§r du klassificerar data med hj√§lp av Microsofts fusklapp. Scikit-learn erbjuder en liknande, men mer detaljerad fusklapp som kan hj√§lpa dig att ytterligare begr√§nsa dina estimatorer (ett annat ord f√∂r klassificerare):

![ML Map from Scikit-learn](../../../../translated_images/sv/map.e963a6a51349425a.webp)
> Tips: [bes√∂k denna karta online](https://scikit-learn.org/stable/tutorial/machine_learning_map/) och klicka l√§ngs v√§gen f√∂r att l√§sa dokumentation.

### Planen

Denna karta √§r mycket hj√§lpsam n√§r du har en klar f√∂rst√•else f√∂r dina data, d√• du kan 'g√•' l√§ngs dess v√§gar till ett beslut:

- Vi har >50 prover
- Vi vill f√∂ruts√§ga en kategori
- Vi har m√§rkta data
- Vi har f√§rre √§n 100K prover
- ‚ú® Vi kan v√§lja en Linear SVC
- Om det inte fungerar, eftersom vi har numeriska data
    - Kan vi prova en ‚ú® KNeighbors Classifier 
      - Om det inte fungerar, prova ‚ú® SVC och ‚ú® Ensemble Classifiers

Detta √§r en mycket hj√§lpsam v√§g att f√∂lja.

## √ñvning - dela upp datan

F√∂ljande v√§g b√∂r vi b√∂rja med att importera n√•gra bibliotek att anv√§nda.

1. Importera de n√∂dv√§ndiga biblioteken:

    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    import numpy as np
    ```

1. Dela upp din tr√§nings- och testdata:

    ```python
    X_train, X_test, y_train, y_test = train_test_split(cuisines_features_df, cuisines_label_df, test_size=0.3)
    ```

## Linear SVC-klassificerare

Support-Vector clustering (SVC) √§r ett barn till Support-Vector machines-familjen av ML-tekniker (l√§s mer om dessa nedan). I denna metod kan du v√§lja en 'kernel' f√∂r att best√§mma hur etiketterna ska grupperas. Parametern 'C' avser 'regulj√§risering' som reglerar parametrarnas inflytande. Kerneln kan vara en av [flera](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC); h√§r s√§tter vi den till 'linear' f√∂r att s√§kerst√§lla att vi utnyttjar linear SVC. Probability √§r standard satt till 'false'; h√§r s√§tter vi den till 'true' f√∂r att samla sannolikhetsuppskattningar. Vi s√§tter random state till '0' f√∂r att slumpa om datan f√∂r att f√• sannolikheter.

### √ñvning - applicera en linear SVC

B√∂rja med att skapa en array med klassificerare. Du kommer att l√§gga till successivt till denna array n√§r vi testar.

1. B√∂rja med en Linear SVC:

    ```python
    C = 10
    # Skapa olika klassificerare.
    classifiers = {
        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
    }
    ```

2. Tr√§na din modell med Linear SVC och skriv ut en rapport:

    ```python
    n_classifiers = len(classifiers)
    
    for index, (name, classifier) in enumerate(classifiers.items()):
        classifier.fit(X_train, np.ravel(y_train))
    
        y_pred = classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
        print(classification_report(y_test,y_pred))
    ```

    Resultatet √§r ganska bra:

    ```output
    Accuracy (train) for Linear SVC: 78.6% 
                  precision    recall  f1-score   support
    
         chinese       0.71      0.67      0.69       242
          indian       0.88      0.86      0.87       234
        japanese       0.79      0.74      0.76       254
          korean       0.85      0.81      0.83       242
            thai       0.71      0.86      0.78       227
    
        accuracy                           0.79      1199
       macro avg       0.79      0.79      0.79      1199
    weighted avg       0.79      0.79      0.79      1199
    ```

## K-Neighbors-klassificerare

K-Neighbors √§r en del av "neighbors"-familjen av ML-metoder, som kan anv√§ndas f√∂r b√•de √∂vervakad och o√∂vervakad inl√§rning. I denna metod skapas ett f√∂rdefinierat antal punkter och data samlas runt dessa punkter s√• att generaliserade etiketter kan f√∂ruts√§gas f√∂r datan.

### √ñvning - applicera K-Neighbors-klassificeraren

Den tidigare klassificeraren var bra och fungerade v√§l med datan, men kanske kan vi f√• b√§ttre noggrannhet. Prova en K-Neighbors-klassificerare.

1. L√§gg till en rad i din klassificerar-array (l√§gg till ett kommatecken efter Linear SVC-elementet):

    ```python
    'KNN classifier': KNeighborsClassifier(C),
    ```

    Resultatet √§r lite s√§mre:

    ```output
    Accuracy (train) for KNN classifier: 73.8% 
                  precision    recall  f1-score   support
    
         chinese       0.64      0.67      0.66       242
          indian       0.86      0.78      0.82       234
        japanese       0.66      0.83      0.74       254
          korean       0.94      0.58      0.72       242
            thai       0.71      0.82      0.76       227
    
        accuracy                           0.74      1199
       macro avg       0.76      0.74      0.74      1199
    weighted avg       0.76      0.74      0.74      1199
    ```

    ‚úÖ L√§s om [K-Neighbors](https://scikit-learn.org/stable/modules/neighbors.html#neighbors)

## Support Vector Classifier

Support-Vector-klassificerare √§r en del av [Support-Vector Machine](https://wikipedia.org/wiki/Support-vector_machine)-familjen av ML-metoder som anv√§nds f√∂r klassificerings- och regressionsuppgifter. SVM "kartl√§gger tr√§ningsdata till punkter i rymden" f√∂r att maximera avst√•ndet mellan tv√• kategorier. Efterf√∂ljande data kartl√§ggs in i denna rymd s√• att deras kategori kan f√∂ruts√§gas.

### √ñvning - applicera en Support Vector Classifier

L√•t oss f√∂rs√∂ka f√• lite b√§ttre noggrannhet med en Support Vector Classifier.

1. L√§gg till ett kommatecken efter K-Neighbors-elementet, och sedan l√§gg till denna rad:

    ```python
    'SVC': SVC(),
    ```

    Resultatet √§r ganska bra!

    ```output
    Accuracy (train) for SVC: 83.2% 
                  precision    recall  f1-score   support
    
         chinese       0.79      0.74      0.76       242
          indian       0.88      0.90      0.89       234
        japanese       0.87      0.81      0.84       254
          korean       0.91      0.82      0.86       242
            thai       0.74      0.90      0.81       227
    
        accuracy                           0.83      1199
       macro avg       0.84      0.83      0.83      1199
    weighted avg       0.84      0.83      0.83      1199
    ```

    ‚úÖ L√§s om [Support-Vectors](https://scikit-learn.org/stable/modules/svm.html#svm)

## Ensemble-klassificerare

L√•t oss f√∂lja v√§gen hela v√§gen till slutet, √§ven om det tidigare testet var ganska bra. L√•t oss prova n√•gra 'Ensemble Classifiers', specifikt Random Forest och AdaBoost:

```python
  'RFST': RandomForestClassifier(n_estimators=100),
  'ADA': AdaBoostClassifier(n_estimators=100)
```

Resultatet √§r mycket bra, s√§rskilt f√∂r Random Forest:

```output
Accuracy (train) for RFST: 84.5% 
              precision    recall  f1-score   support

     chinese       0.80      0.77      0.78       242
      indian       0.89      0.92      0.90       234
    japanese       0.86      0.84      0.85       254
      korean       0.88      0.83      0.85       242
        thai       0.80      0.87      0.83       227

    accuracy                           0.84      1199
   macro avg       0.85      0.85      0.84      1199
weighted avg       0.85      0.84      0.84      1199

Accuracy (train) for ADA: 72.4% 
              precision    recall  f1-score   support

     chinese       0.64      0.49      0.56       242
      indian       0.91      0.83      0.87       234
    japanese       0.68      0.69      0.69       254
      korean       0.73      0.79      0.76       242
        thai       0.67      0.83      0.74       227

    accuracy                           0.72      1199
   macro avg       0.73      0.73      0.72      1199
weighted avg       0.73      0.72      0.72      1199
```

‚úÖ L√§s om [Ensemble Classifiers](https://scikit-learn.org/stable/modules/ensemble.html)

Denna metod inom Maskininl√§rning "kombinerar f√∂ruts√§gelser fr√•n flera basestimatorer" f√∂r att f√∂rb√§ttra modellens kvalitet. I v√•rt exempel anv√§nde vi Random Trees och AdaBoost. 

- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest), en genomsnittlig metod, bygger en 'skog' av 'besluts-tr√§d' infunderade med slumpm√§ssighet f√∂r att undvika √∂veranpassning. Parametern n_estimators √§r satt till antalet tr√§d.

- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) anpassar en klassificerare till en dataset och anpassar sedan kopior av den klassificeraren till samma dataset. Den fokuserar p√• vikterna av felklassificerade objekt och justerar anpassningen f√∂r n√§sta klassificerare f√∂r att korrigera.

---

## üöÄUtmaning

Var och en av dessa tekniker har ett stort antal parametrar som du kan justera. Unders√∂k standardparametrarna f√∂r var och en och fundera p√• vad justering av dessa parametrar skulle inneb√§ra f√∂r modellens kvalitet.

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

## Granskning & Sj√§lvstudier

Det finns mycket fackspr√•k i dessa lektioner, s√• ta en minut att g√• igenom [denna lista](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott) med anv√§ndbar terminologi!

## Uppgift 

[Parameterlek](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfriskrivning**:
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, v√§nligen var medveten om att automatiska √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess modersm√•l b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r n√•gra missf√∂rst√•nd eller feltolkningar som uppst√•r fr√•n anv√§ndningen av denna √∂vers√§ttning.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->