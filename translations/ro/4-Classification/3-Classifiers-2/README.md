# Clasificatoare de bucÄƒtÄƒrii 2

Ãn aceastÄƒ a doua lecÈ›ie de clasificare, vei explora mai multe modalitÄƒÈ›i de a clasifica date numerice. De asemenea, vei afla despre consecinÈ›ele alegerii unui clasificator Ã®n defavoarea altuia.

## [Chestionar pre-lecturÄƒ](https://ff-quizzes.netlify.app/en/ml/)

### PrecondiÈ›ii

Presupunem cÄƒ ai finalizat lecÈ›iile anterioare È™i ai un set de date curÄƒÈ›at Ã®n folderul tÄƒu `data` denumit _cleaned_cuisines.csv_ Ã®n rÄƒdÄƒcina acestui folder cu 4 lecÈ›ii.

### PregÄƒtire

Am Ã®ncÄƒrcat fiÈ™ierul tÄƒu _notebook.ipynb_ cu setul de date curÄƒÈ›at È™i l-am Ã®mpÄƒrÈ›it Ã®n dataframuri X È™i y, gata pentru procesul de construire a modelului.

## O hartÄƒ a clasificÄƒrii

Anterior, ai Ã®nvÄƒÈ›at despre opÈ›iunile pe care le ai atunci cÃ¢nd clasifici date folosind foaia de trucuri Microsoft. Scikit-learn oferÄƒ o foaie de trucuri similarÄƒ, dar mai granularÄƒ, care te poate ajuta sÄƒ restrÃ¢ngi mai mult estimatorii tÄƒi (un alt termen pentru clasificatori):

![ML Map from Scikit-learn](../../../../translated_images/ro/map.e963a6a51349425a.webp)
> Sfat: [viziteazÄƒ aceastÄƒ hartÄƒ online](https://scikit-learn.org/stable/tutorial/machine_learning_map/) È™i fÄƒ clic de-a lungul traseului pentru a citi documentaÈ›ia.

### Planul

AceastÄƒ hartÄƒ este foarte utilÄƒ odatÄƒ ce ai o Ã®nÈ›elegere clarÄƒ a datelor tale, deoarece poÈ›i â€parcurgeâ€ traseele sale cÄƒtre o decizie:

- Avem >50 de eÈ™antioane
- Vrem sÄƒ prezicem o categorie
- Avem date etichetate
- Avem mai puÈ›in de 100.000 de eÈ™antioane
- âœ¨ Putem alege un Linear SVC
- DacÄƒ nu funcÈ›ioneazÄƒ, deoarece avem date numerice
    - Putem Ã®ncerca un âœ¨ KNeighbors Classifier
      - DacÄƒ nu funcÈ›ioneazÄƒ, Ã®ncearcÄƒ âœ¨ SVC È™i âœ¨ Ensemble Classifiers

Acesta este un traseu foarte util de urmat.

## ExerciÈ›iu - Ã®mparte datele

UrmÃ¢nd acest traseu, ar trebui sÄƒ Ã®ncepem prin a importa cÃ¢teva biblioteci pe care sÄƒ le folosim.

1. ImportÄƒ bibliotecile necesare:

    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    import numpy as np
    ```

1. Ãmparte-È›i datele de antrenament È™i testare:

    ```python
    X_train, X_test, y_train, y_test = train_test_split(cuisines_features_df, cuisines_label_df, test_size=0.3)
    ```

## Clasificator Linear SVC

Support-Vector clustering (SVC) este parte din familia de metode ML Support-Vector machines (afiÈ™eazÄƒ mai multe detalii Ã®n continuare). Ãn aceastÄƒ metodÄƒ, poÈ›i alege un â€kernelâ€ pentru a decide cum sÄƒ grupezi etichetele. Parametrul â€Câ€ se referÄƒ la â€regularizareâ€ care regleazÄƒ influenÈ›a parametrilor. Kernel-ul poate fi unul din [mai multe](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC); aici Ã®l setÄƒm pe 'linear' pentru a asigura folosirea Linear SVC. Probabilitatea este implicit 'false'; aici o setÄƒm pe 'true' pentru a obÈ›ine estimÄƒri ale probabilitÄƒÈ›ii. SetÄƒm random state la '0' pentru a amesteca datele È™i a obÈ›ine probabilitÄƒÈ›i.

### ExerciÈ›iu - aplicÄƒ un Linear SVC

Ãncepe prin a crea un array de clasificatori. Vei adÄƒuga progresiv la acest array pe mÄƒsurÄƒ ce testÄƒm.

1. Ãncepe cu un Linear SVC:

    ```python
    C = 10
    # CreeazÄƒ clasificatoare diferite.
    classifiers = {
        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
    }
    ```

2. AntreneazÄƒ-È›i modelul folosind Linear SVC È™i afiÈ™eazÄƒ un raport:

    ```python
    n_classifiers = len(classifiers)
    
    for index, (name, classifier) in enumerate(classifiers.items()):
        classifier.fit(X_train, np.ravel(y_train))
    
        y_pred = classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
        print(classification_report(y_test,y_pred))
    ```

    Rezultatul este destul de bun:

    ```output
    Accuracy (train) for Linear SVC: 78.6% 
                  precision    recall  f1-score   support
    
         chinese       0.71      0.67      0.69       242
          indian       0.88      0.86      0.87       234
        japanese       0.79      0.74      0.76       254
          korean       0.85      0.81      0.83       242
            thai       0.71      0.86      0.78       227
    
        accuracy                           0.79      1199
       macro avg       0.79      0.79      0.79      1199
    weighted avg       0.79      0.79      0.79      1199
    ```

## Clasificator K-Neighbors

K-Neighbors face parte din familia metodelor ML â€veciniâ€, care pot fi folosite atÃ¢t pentru Ã®nvÄƒÈ›are supervizatÄƒ cÃ¢t È™i nesupervizatÄƒ. Ãn aceastÄƒ metodÄƒ se creeazÄƒ un numÄƒr predefinit de puncte È™i datele sunt adunate Ã®n jurul acestor puncte astfel Ã®ncÃ¢t se pot prezice etichete generalizate pentru date.

### ExerciÈ›iu - aplicÄƒ clasificatorul K-Neighbors

Clasificatorul anterior a fost bun È™i a funcÈ›ionat bine cu datele, dar poate putem obÈ›ine o acurateÈ›e mai bunÄƒ. ÃncearcÄƒ un clasificator K-Neighbors.

1. AdaugÄƒ o linie Ã®n array-ul tÄƒu de clasificatori (adaugÄƒ o virgulÄƒ dupÄƒ elementul Linear SVC):

    ```python
    'KNN classifier': KNeighborsClassifier(C),
    ```

    Rezultatul este puÈ›in mai slab:

    ```output
    Accuracy (train) for KNN classifier: 73.8% 
                  precision    recall  f1-score   support
    
         chinese       0.64      0.67      0.66       242
          indian       0.86      0.78      0.82       234
        japanese       0.66      0.83      0.74       254
          korean       0.94      0.58      0.72       242
            thai       0.71      0.82      0.76       227
    
        accuracy                           0.74      1199
       macro avg       0.76      0.74      0.74      1199
    weighted avg       0.76      0.74      0.74      1199
    ```

    âœ… AflÄƒ despre [K-Neighbors](https://scikit-learn.org/stable/modules/neighbors.html#neighbors)

## Clasificator Support Vector

Clasificatoarele Support-Vector fac parte din familia de metode ML [Support-Vector Machine](https://wikipedia.org/wiki/Support-vector_machine) folositÄƒ pentru sarcini de clasificare È™i regresie. SVM-urile â€mapazÄƒ exemplele de antrenament cÄƒtre puncte Ã®n spaÈ›iuâ€ pentru a maximiza distanÈ›a Ã®ntre douÄƒ categorii. Datele ulterioare sunt mapate Ã®n acest spaÈ›iu pentru a prezice categoria lor.

### ExerciÈ›iu - aplicÄƒ un Support Vector Classifier

SÄƒ Ã®ncercÄƒm o acurateÈ›e puÈ›in mai bunÄƒ cu un Support Vector Classifier.

1. AdaugÄƒ o virgulÄƒ dupÄƒ itemul K-Neighbors, apoi adaugÄƒ aceastÄƒ linie:

    ```python
    'SVC': SVC(),
    ```

    Rezultatul este destul de bun!

    ```output
    Accuracy (train) for SVC: 83.2% 
                  precision    recall  f1-score   support
    
         chinese       0.79      0.74      0.76       242
          indian       0.88      0.90      0.89       234
        japanese       0.87      0.81      0.84       254
          korean       0.91      0.82      0.86       242
            thai       0.74      0.90      0.81       227
    
        accuracy                           0.83      1199
       macro avg       0.84      0.83      0.83      1199
    weighted avg       0.84      0.83      0.83      1199
    ```

    âœ… AflÄƒ despre [Support-Vectors](https://scikit-learn.org/stable/modules/svm.html#svm)

## Clasificatoare Ensemble

SÄƒ urmÄƒm traseul pÃ¢nÄƒ la capÄƒt, chiar dacÄƒ testul anterior a fost destul de bun. SÄƒ Ã®ncercÄƒm niÈ™te 'Classificatoare Ensemble', Ã®n mod specific Random Forest È™i AdaBoost:

```python
  'RFST': RandomForestClassifier(n_estimators=100),
  'ADA': AdaBoostClassifier(n_estimators=100)
```

Rezultatul este foarte bun, Ã®n special pentru Random Forest:

```output
Accuracy (train) for RFST: 84.5% 
              precision    recall  f1-score   support

     chinese       0.80      0.77      0.78       242
      indian       0.89      0.92      0.90       234
    japanese       0.86      0.84      0.85       254
      korean       0.88      0.83      0.85       242
        thai       0.80      0.87      0.83       227

    accuracy                           0.84      1199
   macro avg       0.85      0.85      0.84      1199
weighted avg       0.85      0.84      0.84      1199

Accuracy (train) for ADA: 72.4% 
              precision    recall  f1-score   support

     chinese       0.64      0.49      0.56       242
      indian       0.91      0.83      0.87       234
    japanese       0.68      0.69      0.69       254
      korean       0.73      0.79      0.76       242
        thai       0.67      0.83      0.74       227

    accuracy                           0.72      1199
   macro avg       0.73      0.73      0.72      1199
weighted avg       0.73      0.72      0.72      1199
```

âœ… AflÄƒ despre [Clasificatoare Ensemble](https://scikit-learn.org/stable/modules/ensemble.html)

AceastÄƒ metodÄƒ de ÃnvÄƒÈ›are AutomatÄƒ â€combinÄƒ predicÈ›iile mai multor estimatori de bazÄƒâ€ pentru a Ã®mbunÄƒtÄƒÈ›i calitatea modelului. Ãn exemplul nostru, am folosit Random Trees È™i AdaBoost.

- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest), o metodÄƒ de mediere, construieÈ™te o â€pÄƒdureâ€ de â€arbori de decizieâ€ infuzaÈ›i cu aleatorietate pentru a evita suprapotrivirea. Parametrul n_estimators este setat la numÄƒrul de arbori.

- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) potriveÈ™te un clasificator pe un set de date È™i apoi potriveÈ™te copii ale acelui clasificator pe acelaÈ™i set de date. Se concentreazÄƒ pe greutÄƒÈ›ile elementelor clasificate incorect È™i ajusteazÄƒ potrivirea pentru urmÄƒtorul clasificator, pentru a corecta.

---

## ğŸš€Provocare

Fiecare dintre aceste tehnici are un numÄƒr mare de parametri pe care Ã®i poÈ›i modifica. CerceteazÄƒ parametrii lor impliciÈ›i È™i gÃ¢ndeÈ™te-te ce ar Ã®nsemna sÄƒ modifici aceÈ™ti parametri pentru calitatea modelului.

## [Chestionar post-lecturÄƒ](https://ff-quizzes.netlify.app/en/ml/)

## Recapitulare & Studiu individual

ExistÄƒ mult jargon Ã®n aceste lecÈ›ii, aÈ™a cÄƒ ia-È›i un minut sÄƒ revizuieÈ™ti [aceastÄƒ listÄƒ](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott) de terminologie utilÄƒ!

## TemÄƒ

[Jocul cu parametrii](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Declinare de responsabilitate**:
Acest document a fost tradus folosind serviciul de traducere automatÄƒ AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim pentru acurateÈ›e, vÄƒ rugÄƒm sÄƒ È›ineÈ›i cont cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa nativÄƒ trebuie considerat sursa autorizatÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un traducÄƒtor uman. Nu ne asumÄƒm responsabilitatea pentru eventuale neÃ®nÈ›elegeri sau interpretÄƒri eronate rezultate din utilizarea acestei traduceri.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->