# ConstruieÈ™te un model de regresie folosind Scikit-learn: patru moduri de regresie

## NotÄƒ pentru Ã®ncepÄƒtori

Regresia liniarÄƒ este folositÄƒ atunci cÃ¢nd dorim sÄƒ prezicem o **valoare numericÄƒ** (de exemplu, preÈ›ul unei case, temperatura sau vÃ¢nzÄƒrile).
FuncÈ›ioneazÄƒ prin gÄƒsirea unei linii drepte care reprezintÄƒ cel mai bine relaÈ›ia dintre caracteristicile de intrare È™i ieÈ™ire.

Ãn aceastÄƒ lecÈ›ie, ne concentrÄƒm pe Ã®nÈ›elegerea conceptului Ã®nainte de a explora tehnici mai avansate de regresie.
![Infografic regresie liniarÄƒ vs polinomialÄƒ](../../../../translated_images/ro/linear-polynomial.5523c7cb6576ccab.webp)
> Infografic realizat de [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Test pre-lecturÄƒ](https://ff-quizzes.netlify.app/en/ml/)

> ### [AceastÄƒ lecÈ›ie este disponibilÄƒ È™i Ã®n R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introducere

PÃ¢nÄƒ acum ai explorat ce este regresia cu date de probÄƒ culese din setul de date despre preÈ›urile dovlecilor pe care Ã®l vom folosi pe parcursul acestei lecÈ›ii. De asemenea, ai vizualizat datele folosind Matplotlib.

Acum eÈ™ti pregÄƒtit sÄƒ aprofundezi regresia pentru ÃnvÄƒÈ›are AutomatÄƒ. Ãn timp ce vizualizarea Ã®È›i permite sÄƒ Ã®nÈ›elegi datele, adevÄƒrata putere a ÃnvÄƒÈ›Äƒrii Automate provine din _antrenarea modelelor_. Modelele sunt antrenate pe date istorice pentru a captura automat dependenÈ›ele din date È™i Ã®È›i permit sÄƒ prevezi rezultate pentru date noi, pe care modelul nu le-a vÄƒzut Ã®nainte.

Ãn aceastÄƒ lecÈ›ie, vei Ã®nvÄƒÈ›a mai multe despre douÄƒ tipuri de regresie: _regresia liniarÄƒ de bazÄƒ_ È™i _regresia polinomialÄƒ_, alÄƒturi de unele aspecte matematice care stau la baza acestor tehnici. Aceste modele ne vor permite sÄƒ prezicem preÈ›urile dovlecilor Ã®n funcÈ›ie de diferite date de intrare.

[![ML pentru Ã®ncepÄƒtori - ÃnÈ›elegerea regresiei liniare](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML pentru Ã®ncepÄƒtori - ÃnÈ›elegerea regresiei liniare")

> ğŸ¥ DÄƒ click pe imaginea de mai sus pentru un scurt videoclip despre regresia liniarÄƒ.

> Pe tot parcursul acestui curriculum, presupunem un minim de cunoÈ™tinÈ›e matematice È™i Ã®ncercÄƒm sÄƒ Ã®l facem accesibil pentru studenÈ›i din alte domenii, aÈ™a cÄƒ fii atent la note, ğŸ§® explicaÈ›ii, diagrame È™i alte instrumente de Ã®nvÄƒÈ›are care te vor ajuta sÄƒ Ã®nÈ›elegi.

### PregÄƒtire prealabilÄƒ

Ar trebui sÄƒ fii familiarizat pÃ¢nÄƒ acum cu structura datelor despre dovleci pe care le examinÄƒm. Le poÈ›i gÄƒsi preÃ®ncÄƒrcate È™i prelucrate Ã®n fiÈ™ierul _notebook.ipynb_ al acestei lecÈ›ii. Ãn fiÈ™ier, preÈ›ul dovlecilor este afiÈ™at pe bushel Ã®ntr-un nou tabel de date. AsigurÄƒ-te cÄƒ poÈ›i rula aceste jurnale Ã®n kernel-uri din Visual Studio Code.

### PregÄƒtire

Ca o reamintire, Ã®ncarci aceste date pentru a putea pune Ã®ntrebÄƒri despre ele.

- Care este cel mai bun moment pentru a cumpÄƒra dovleci?
- Ce preÈ› pot sÄƒ mÄƒ aÈ™tept pentru o cutie de dovleci miniaturÄƒ?
- Ar trebui sÄƒ Ã®i cumpÄƒr Ã®n coÈ™uri de jumÄƒtate de bushel sau Ã®n cutii de 1 1/9 busheli?
Hai sÄƒ continuÄƒm sÄƒ explorÄƒm aceste date.

Ãn lecÈ›ia anterioarÄƒ, ai creat un tabel Pandas È™i l-ai populat cu o parte din setul original de date, standardizÃ¢nd preÈ›ul pe bushel. Prin aceastÄƒ metodÄƒ, Ã®nsÄƒ, ai reuÈ™it sÄƒ aduni doar aproximativ 400 de puncte de date È™i doar pentru lunile de toamnÄƒ.

AruncÄƒ o privire la datele preÃ®ncÄƒrcate Ã®n jurnalul Ã®nsoÈ›itor al acestei lecÈ›ii. Datele sunt preÃ®ncÄƒrcate È™i este reprezentat un grafic dispersie (scatterplot) iniÈ›ial pentru a arÄƒta datele pe lunÄƒ. Poate putem sÄƒ aflÄƒm mai multe detalii despre natura datelor curÄƒÈ›Ã¢ndu-le mai mult.

## O linie de regresie liniarÄƒ

DupÄƒ cum ai Ã®nvÄƒÈ›at Ã®n LecÈ›ia 1, scopul exerciÈ›iului de regresie liniarÄƒ este sÄƒ putem trasa o linie pentru a:

- **ArÄƒta relaÈ›iile dintre variabile**. AratÄƒ relaÈ›ia dintre variabile
- **Face predicÈ›ii**. Face predicÈ›ii precise privind unde ar cÄƒdea un nou punct de date Ã®n raport cu acea linie.

Este tipic pentru **regresia celor mai mici pÄƒtrate** sÄƒ traseze acest tip de linie. Termenul â€Cei mai mici pÄƒtraÈ›iâ€ se referÄƒ la procesul de minimizare a erorii totale din modelul nostru. Pentru fiecare punct de date, mÄƒsurÄƒm distanÈ›a verticalÄƒ (numitÄƒ rezidualÄƒ) Ã®ntre punctul real È™i linia de regresie.

Aceste distanÈ›e le ridicÄƒm la pÄƒtrat din douÄƒ motive principale:

1. **Magnitudine, nu DirecÈ›ie:** Dorim sÄƒ tratÄƒm o eroare de -5 la fel ca o eroare de +5. Ridicarea la pÄƒtrat transformÄƒ toate valorile Ã®n pozitive.

2. **Penalizarea valorilor extreme:** Ridicarea la pÄƒtrat atribuie o greutate mai mare erorilor mari, forÈ›Ã¢nd linia sÄƒ fie mai aproape de punctele care sunt mai Ã®ndepÄƒrtate.

Apoi adunÄƒm toate aceste valori ridicate la pÄƒtrat. Scopul nostru este sÄƒ gÄƒsim linia specificÄƒ pentru care aceastÄƒ sumÄƒ finalÄƒ este cea mai micÄƒ (valoarea posibilÄƒ cea mai micÄƒ) â€“ de aici È™i numele â€CeilalÈ›i mai mici pÄƒtraÈ›iâ€.

> **ğŸ§® AratÄƒ-mi matematica** 
> 
> AceastÄƒ linie, numitÄƒ _linia de cea mai bunÄƒ potrivire_, poate fi exprimatÄƒ prin [o ecuaÈ›ie](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` este â€variabila explicativÄƒâ€. `Y` este â€variabila dependentÄƒâ€. Panta liniei este `b` iar `a` este interceptul pe axa y, care se referÄƒ la valoarea lui `Y` cÃ¢nd `X = 0`.
>
>![calculeazÄƒ panta](../../../../translated_images/ro/slope.f3c9d5910ddbfcf9.webp)
>
> Mai Ã®ntÃ¢i, calculeazÄƒ panta `b`. Infografic realizat de [Jen Looper](https://twitter.com/jenlooper)
>
> Cu alte cuvinte, referindu-ne la Ã®ntrebarea originalÄƒ din datele noastre despre dovleci: â€prezice preÈ›ul unui dovleac pe bushel Ã®n funcÈ›ie de lunÄƒâ€, `X` se referÄƒ la preÈ›, iar `Y` la luna vÃ¢nzÄƒrii.
>
>![completeazÄƒ ecuaÈ›ia](../../../../translated_images/ro/calculation.a209813050a1ddb1.webp)
>
> CalculeazÄƒ valoarea lui Y. DacÄƒ plÄƒteÈ™ti Ã®n jur de 4 dolari, trebuie sÄƒ fie aprilie! Infografic realizat de [Jen Looper](https://twitter.com/jenlooper)
>
> Matematica care calculeazÄƒ linia trebuie sÄƒ demonstreze panta liniei, care depinde È™i de intercept, adicÄƒ locul unde se aflÄƒ `Y` cÃ¢nd `X = 0`.
>
> PoÈ›i observa metoda de calcul pentru aceste valori pe site-ul [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). De asemenea, viziteazÄƒ [acest calculator de cei mai mici pÄƒtraÈ›i](https://www.mathsisfun.com/data/least-squares-calculator.html) pentru a vedea cum valorile numerelor influenÈ›eazÄƒ linia.

## CorelaÈ›ia

Un termen Ã®n plus de Ã®nÈ›eles este **Coeficientul de corelaÈ›ie** Ã®ntre variabilele date X È™i Y. Folosind un grafic dispersie, poÈ›i vizualiza rapid acest coeficient. Un grafic cu puncte dispersate Ã®ntr-o linie ordonatÄƒ are corelaÈ›ie mare, iar un grafic cu puncte dispersate peste tot Ã®ntre X È™i Y are corelaÈ›ie scÄƒzutÄƒ.

Un model bun de regresie liniarÄƒ va fi unul care are un coeficient de corelaÈ›ie ridicat (mai aproape de 1 decÃ¢t de 0) folosind metoda regresiei celor mai mici pÄƒtrate cu o linie de regresie.

âœ… RuleazÄƒ jurnalul Ã®nsoÈ›itor acestei lecÈ›ii È™i uitÄƒ-te la graficul dispersie LunÄƒ vs. PreÈ›. Datele care leagÄƒ Luna de PreÈ› pentru vÃ¢nzÄƒrile de dovleci par sÄƒ aibÄƒ corelaÈ›ie mare sau micÄƒ, conform interpretÄƒrii tale vizuale a graficului? Se schimbÄƒ asta dacÄƒ foloseÈ™ti o mÄƒsurÄƒtoare mai detaliatÄƒ Ã®n loc de `Month`, de ex. *ziua anului* (adicÄƒ numÄƒrul de zile de la Ã®nceputul anului)?

Ãn codul de mai jos, vom presupune cÄƒ am curÄƒÈ›at datele È™i am obÈ›inut un tabel de date numit `new_pumpkins`, similar cu urmÄƒtorul:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> Codul pentru curÄƒÈ›area datelor este disponibil Ã®n [`notebook.ipynb`](notebook.ipynb). Am efectuat aceiaÈ™i paÈ™i de curÄƒÈ›are ca Ã®n lecÈ›ia precedentÄƒ È™i am calculat coloana `DayOfYear` folosind expresia urmÄƒtoare:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Acum cÄƒ ai o Ã®nÈ›elegere a matematicii din spatele regresiei liniare, sÄƒ creÄƒm un model de regresie pentru a vedea dacÄƒ putem prezice care pachet de dovleci va avea cele mai bune preÈ›uri. Cineva care cumpÄƒrÄƒ dovleci pentru o patch de dovleci de sÄƒrbÄƒtori ar putea dori aceastÄƒ informaÈ›ie pentru a-È™i optimiza achiziÈ›iile de pachete de dovleci pentru patch.

## CÄƒutÃ¢nd corelaÈ›ie

[![ML pentru Ã®ncepÄƒtori - CÄƒutÃ¢nd CorelaÈ›ia: Cheia regresiei liniare](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML pentru Ã®ncepÄƒtori - CÄƒutÃ¢nd CorelaÈ›ia: Cheia regresiei liniare")

> ğŸ¥ DÄƒ click pe imaginea de mai sus pentru un scurt videoclip despre corelaÈ›ie.

Din lecÈ›ia precedentÄƒ probabil ai observat cÄƒ preÈ›ul mediu pentru diferite luni aratÄƒ astfel:

<img alt="PreÈ›ul mediu pe lunÄƒ" src="../../../../translated_images/ro/barchart.a833ea9194346d76.webp" width="50%"/>

Acest lucru sugereazÄƒ cÄƒ ar trebui sÄƒ existe o corelaÈ›ie, È™i putem Ã®ncerca sÄƒ antrenÄƒm un model de regresie liniarÄƒ pentru a prezice relaÈ›ia dintre `Month` È™i `Price`, sau dintre `DayOfYear` È™i `Price`. IatÄƒ graficul dispersie care aratÄƒ aceastÄƒ a doua relaÈ›ie:

<img alt="Grafic dispersie al PreÈ›ului vs Ziua anului" src="../../../../translated_images/ro/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" />

SÄƒ vedem dacÄƒ existÄƒ corelaÈ›ie folosind funcÈ›ia `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Se pare cÄƒ corelaÈ›ia este destul de micÄƒ, -0.15 pentru `Month` È™i -0.17 pentru `DayOfMonth`, dar ar putea exista o altÄƒ relaÈ›ie importantÄƒ. Se pare cÄƒ existÄƒ clustere diferite de preÈ›uri corespunzÄƒtoare diferitelor soiuri de dovleci. Pentru a confirma aceastÄƒ ipotezÄƒ, sÄƒ desenÄƒm fiecare categorie de dovleci cu o culoare diferitÄƒ. TransmitÃ¢nd parametrul `ax` cÄƒtre funcÈ›ia de plotare `scatter` putem plota toate punctele pe acelaÈ™i grafic:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Grafic dispersie al PreÈ›ului vs Ziua anului" src="../../../../translated_images/ro/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" />

InvestigaÈ›ia noastrÄƒ sugereazÄƒ cÄƒ soiul are un efect mai mare asupra preÈ›ului total decÃ¢t data efectivÄƒ a vÃ¢nzÄƒrii. Putem vedea asta cu un grafic cu bare:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Grafic cu bare al preÈ›ului vs soiului" src="../../../../translated_images/ro/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

SÄƒ ne concentrÄƒm momentan doar pe un soi de dovleci, cel â€pie typeâ€, È™i sÄƒ vedem ce efect are data asupra preÈ›ului:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Grafic dispersie al PreÈ›ului vs Ziua anului" src="../../../../translated_images/ro/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" />

DacÄƒ acum calculÄƒm corelaÈ›ia dintre `Price` È™i `DayOfYear` folosind funcÈ›ia `corr`, vom obÈ›ine ceva de genul `-0.27` - ceea ce Ã®nseamnÄƒ cÄƒ antrenarea unui model predictiv are sens.

> Ãnainte de a antrena un model de regresie liniarÄƒ, este important sÄƒ ne asigurÄƒm cÄƒ datele sunt curate. Regresia liniarÄƒ nu funcÈ›ioneazÄƒ bine cu valori lipsÄƒ, deci este recomandat sÄƒ eliminÄƒm toate celulele goale:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

O altÄƒ abordare ar fi sÄƒ completÄƒm acele valori lipsÄƒ cu valorile medii din coloana corespunzÄƒtoare.

## Regresie liniarÄƒ simplÄƒ

[![ML pentru Ã®ncepÄƒtori - Regresie liniarÄƒ È™i polinomialÄƒ folosind Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML pentru Ã®ncepÄƒtori - Regresie liniarÄƒ È™i polinomialÄƒ folosind Scikit-learn")

> ğŸ¥ DÄƒ click pe imaginea de mai sus pentru un scurt videoclip despre regresia liniarÄƒ È™i polinomialÄƒ.

Pentru a antrena modelul nostru de regresie liniarÄƒ vom folosi biblioteca **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Ãncepem prin separarea valorilor de intrare (caracteristici) È™i a ieÈ™irii aÈ™teptate (etichetei) Ã®n array-uri numpy separate:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> ObservÄƒ cÄƒ a fost necesarÄƒ folosirea metodei `reshape` pe datele de intrare pentru ca pachetul de regresie liniarÄƒ sÄƒ le Ã®nÈ›eleagÄƒ corect. Regresia liniarÄƒ aÈ™teaptÄƒ un array 2D ca intrare, unde fiecare rÃ¢nd al array-ului corespunde unui vector de caracteristici de intrare. Ãn cazul nostru, deoarece avem o singurÄƒ intrare, avem nevoie de un array cu forma N&times;1, unde N este dimensiunea setului de date.

Apoi, trebuie sÄƒ Ã®mpÄƒrÈ›im datele Ã®n seturi de antrenament È™i test pentru a valida modelul dupÄƒ antrenare:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Ãn cele din urmÄƒ, antrenarea modelelor propriu-zise de regresie liniarÄƒ se face Ã®n doar douÄƒ linii de cod. Definim obiectul `LinearRegression`, È™i Ã®l potrivim pe date folosind metoda `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Obiectul `LinearRegression` dupÄƒ `fit`-are conÈ›ine toÈ›i coeficienÈ›ii regresiei, care pot fi accesaÈ›i folosind proprietatea `.coef_`. Ãn cazul nostru, existÄƒ doar un coeficient, care ar trebui sÄƒ fie Ã®n jur de `-0.017`. Aceasta Ã®nseamnÄƒ cÄƒ preÈ›urile par sÄƒ scadÄƒ puÈ›in Ã®n timp, dar nu prea mult, Ã®n jur de 2 cenÈ›i pe zi. De asemenea, putem accesa punctul de intersecÈ›ie al regresiei cu axa Y folosind `lin_reg.intercept_` - acesta va fi Ã®n jur de `21` Ã®n cazul nostru, indicÃ¢nd preÈ›ul la Ã®nceputul anului.

Pentru a vedea cÃ¢t de precis este modelul nostru, putem prezice preÈ›urile pe un set de date de test, apoi putem mÄƒsura cÃ¢t de aproape sunt predicÈ›iile noastre de valorile aÈ™teptate. Acest lucru se poate face folosind metrica eroarea pÄƒtraticÄƒ medie (MSE), care este media tuturor diferenÈ›elor pÄƒtrate dintre valori aÈ™teptate È™i valori prezise.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

Eroarea noastrÄƒ pare sÄƒ fie Ã®n jur de 2 puncte, ceea ce este ~17%. Nu prea bine. Un alt indicator al calitÄƒÈ›ii modelului este **coeficientul de determinare**, care poate fi obÈ›inut astfel:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
DacÄƒ valoarea este 0, Ã®nseamnÄƒ cÄƒ modelul nu ia Ã®n considerare datele de intrare È™i acÈ›ioneazÄƒ ca *cel mai slab predictor liniar*, care este pur È™i simplu o valoare medie a rezultatului. Valoarea 1 Ã®nseamnÄƒ cÄƒ putem prezice perfect toate rezultatele aÈ™teptate. Ãn cazul nostru, coeficientul este Ã®n jur de 0.06, ceea ce este destul de scÄƒzut.

Putem, de asemenea, sÄƒ reprezentÄƒm grafic datele de test Ã®mpreunÄƒ cu linia de regresie pentru a vedea mai bine cum funcÈ›ioneazÄƒ regresia Ã®n cazul nostru:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linear regression" src="../../../../translated_images/ro/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## Regresie PolinomialÄƒ

Un alt tip de Regressie LiniarÄƒ este Regresia PolinomialÄƒ. DeÈ™i uneori existÄƒ o relaÈ›ie liniarÄƒ Ã®ntre variabile - cu cÃ¢t dovleacul este mai mare ca volum, cu atÃ¢t este mai mare preÈ›ul - uneori aceste relaÈ›ii nu pot fi reprezentate ca un plan sau o linie dreaptÄƒ.

âœ… IatÄƒ [cÃ¢teva exemple suplimentare](https://online.stat.psu.edu/stat501/lesson/9/9.8) de date care ar putea necesita Regresie PolinomialÄƒ

Privim din nou relaÈ›ia dintre Data È™i PreÈ›. Pare acest scatterplot sÄƒ trebuiascÄƒ neapÄƒrat analizat printr-o linie dreaptÄƒ? Nu pot preÈ›urile fluctua? Ãn acest caz, poÈ›i Ã®ncerca regresia polinomialÄƒ.

âœ… Polinoamele sunt expresii matematice care pot consta din unul sau mai mulÈ›i termeni È™i coeficienÈ›i

Regresia polinomialÄƒ creeazÄƒ o curbÄƒ pentru a se potrivi mai bine datelor neliniare. Ãn cazul nostru, dacÄƒ includem o variabilÄƒ pÄƒtraticÄƒ `DayOfYear` Ã®n datele de intrare, ar trebui sÄƒ putem ajusta datele noastre cu o curbÄƒ parabolicÄƒ, care va avea un minim Ã®ntr-un anumit punct din an.

Scikit-learn include un util [API pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) pentru a combina diferite etape de procesare a datelor Ã®mpreunÄƒ. Un **pipeline** este un lanÈ› de **estimatori**. Ãn cazul nostru, vom crea un pipeline care mai Ã®ntÃ¢i adaugÄƒ caracteristici polinomiale modelului, apoi antreneazÄƒ regresia:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Folosind `PolynomialFeatures(2)` Ã®nseamnÄƒ cÄƒ vom include toÈ›i polinomii de gradul doi din datele de intrare. Ãn cazul nostru, aceasta Ã®nseamnÄƒ doar `DayOfYear`<sup>2</sup>, dar avÃ¢nd douÄƒ variabile de intrare X È™i Y, aceasta va adÄƒuga X<sup>2</sup>, XY È™i Y<sup>2</sup>. Putem de asemenea sÄƒ folosim polinoame de grad mai Ã®nalt dacÄƒ dorim.

Pipeline-urile pot fi folosite Ã®n acelaÈ™i mod ca obiectul original `LinearRegression`, adicÄƒ putem face `fit` pe pipeline, apoi folosi `predict` pentru a obÈ›ine rezultatele predicÈ›iei. IatÄƒ graficul care aratÄƒ datele de test È™i curba de aproximare:

<img alt="Polynomial regression" src="../../../../translated_images/ro/poly-results.ee587348f0f1f60b.webp" width="50%" />

Folosind Regresia PolinomialÄƒ, putem obÈ›ine o MSE puÈ›in mai micÄƒ È™i un coeficient de determinare mai mare, dar fÄƒrÄƒ o Ã®mbunÄƒtÄƒÈ›ire semnificativÄƒ. Trebuie sÄƒ luÄƒm Ã®n considerare È™i alte caracteristici!

> Se poate observa cÄƒ preÈ›urile minime ale dovlecilor sunt observate Ã®n jur de Halloween. Cum ai putea explica acest lucru?

ğŸƒ FelicitÄƒri, tocmai ai creat un model care poate ajuta la prezicerea preÈ›ului dovlecilor pentru plÄƒcintÄƒ. Probabil poÈ›i repeta aceeaÈ™i procedurÄƒ pentru toate tipurile de dovleci, dar asta ar fi obositor. Acum sÄƒ Ã®nvÄƒÈ›Äƒm cum sÄƒ luÄƒm Ã®n calcul soiul de dovleac Ã®n modelul nostru!

## Caracteristici categorice

Ãn lumea idealÄƒ, vrem sÄƒ putem prezice preÈ›urile pentru diferite soiuri de dovleci folosind acelaÈ™i model. TotuÈ™i, coloana `Variety` este oarecum diferitÄƒ de coloane precum `Month`, pentru cÄƒ conÈ›ine valori nenumerice. Astfel de coloane se numesc **categorice**.

[![ML pentru Ã®ncepÄƒtori - PredicÈ›ii cu caracteristici categorice Ã®n regresia liniarÄƒ](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML pentru Ã®ncepÄƒtori - PredicÈ›ii cu caracteristici categorice Ã®n regresia liniarÄƒ")

> ğŸ¥ FÄƒ click pe imaginea de mai sus pentru un scurt videoclip despre utilizarea caracteristicilor categorice.

Aici poÈ›i vedea cum preÈ›ul mediu depinde de soi:

<img alt="Average price by variety" src="../../../../translated_images/ro/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

Pentru a lua Ã®n calcul soiul, mai Ã®ntÃ¢i trebuie sÄƒ-l convertim Ã®n formÄƒ numericÄƒ, sau sÄƒ-l **encodÄƒm**. ExistÄƒ mai multe moduri de a face asta:

* O simplÄƒ **codificare numericÄƒ** va crea un tabel cu soiurile diferite È™i apoi va Ã®nlocui numele soiului cu un index din acel tabel. Aceasta nu este o idee bunÄƒ pentru regresia liniarÄƒ, deoarece regresia liniarÄƒ ia valoarea numericÄƒ efectivÄƒ a indexului È™i o adaugÄƒ Ã®n rezultat, Ã®nmulÈ›ind cu un coeficient. Ãn cazul nostru, relaÈ›ia dintre numÄƒrul indexului È™i preÈ› este clar neliniarÄƒ, chiar dacÄƒ ne asigurÄƒm cÄƒ indicii sunt ordonaÈ›i Ã®ntr-un anumit mod.
* **Codificarea one-hot** va Ã®nlocui coloana `Variety` cu 4 coloane diferite, cÃ¢te una pentru fiecare soi. Fiecare coloanÄƒ va conÈ›ine `1` dacÄƒ rÃ¢ndul respectiv este dintr-un anumit soi È™i `0` Ã®n caz contrar. Aceasta Ã®nseamnÄƒ cÄƒ vor exista patru coeficienÈ›i Ã®n regresia liniarÄƒ, cÃ¢te unul pentru fiecare soi de dovleac, responsabili pentru â€preÈ›ul de bazÄƒâ€ (sau mai degrabÄƒ â€preÈ›ul suplimentarâ€) pentru acel soi Ã®n particular.

Codul de mai jos aratÄƒ cum putem aplica one-hot encoding pentru soi:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Pentru a antrena regresia liniarÄƒ folosind soiul one-hot encoded drept intrare, trebuie doar sÄƒ iniÈ›ializÄƒm corect datele X È™i y:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Restul codului este la fel ca cel folosit mai sus pentru a antrena regresia liniarÄƒ. DacÄƒ Ã®ncerci, vei vedea cÄƒ eroarea pÄƒtraticÄƒ medie este cam aceeaÈ™i, dar coeficientul de determinare creÈ™te mult (~77%). Pentru predicÈ›ii È™i mai precise, putem lua Ã®n calcul mai multe caracteristici categorice, precum È™i caracteristici numerice, cum ar fi `Month` sau `DayOfYear`. Pentru a obÈ›ine un singur set mare de caracteristici, putem folosi `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Aici luÄƒm Ã®n considerare È™i `City` È™i tipul `Package`, ceea ce ne oferÄƒ MSE 2.84 (10%) È™i coeficient de determinare 0.94!

## CombinÃ¢nd totul

Pentru a face cel mai bun model, putem folosi date combinate (categorice one-hot encoded + numerice) din exemplul de mai sus Ã®mpreunÄƒ cu Regresia PolinomialÄƒ. IatÄƒ codul complet pentru convenienÈ›a ta:

```python
# configurare date de antrenament
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# realizare divizare antrenament-test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# configurare È™i antrenare pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# prezice rezultate pentru datele de test
pred = pipeline.predict(X_test)

# calculare MSE È™i coeficient de determinare
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Aceasta ar trebui sÄƒ ne ofere cel mai bun coeficient de determinare de aproape 97% È™i MSE=2.23 (~8% eroare de predicÈ›ie).

| Model | MSE | Determinare |
|-------|-----|-------------|
| Linear `DayOfYear` | 2.77 (17.2%) | 0.07 |
| Polinomial `DayOfYear` | 2.73 (17.0%) | 0.08 |
| Linear `Variety` | 5.24 (19.7%) | 0.77 |
| Linear toate caracteristicile | 2.84 (10.5%) | 0.94 |
| Polinomial toate caracteristicile | 2.23 (8.25%) | 0.97 |

ğŸ† Foarte bine! Ai creat patru modele de Regresie Ã®ntr-o singurÄƒ lecÈ›ie È™i ai Ã®mbunÄƒtÄƒÈ›it calitatea modelului la 97%. Ãn secÈ›iunea finalÄƒ despre Regresie vei Ã®nvÄƒÈ›a despre Regresia LogisticÄƒ pentru determinarea categoriilor.

---
## ğŸš€Provocare

TesteazÄƒ mai multe variabile diferite Ã®n acest notebook pentru a vedea cum se coreleazÄƒ acestea cu acurateÈ›ea modelului.

## [Test post-lector](https://ff-quizzes.netlify.app/en/ml/)

## Recapitulare & Auto-studiu

Ãn aceastÄƒ lecÈ›ie am Ã®nvÄƒÈ›at despre Regresia LiniarÄƒ. ExistÄƒ È™i alte tipuri importante de Regresie. CiteÈ™te despre tehnicile Stepwise, Ridge, Lasso È™i Elasticnet. Un curs bun pentru aprofundare este [Stanford Statistical Learning course](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## TemÄƒ

[Build a Model](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Declinarea responsabilitÄƒÈ›ii**:  
Acest document a fost tradus folosind serviciul de traducere automatÄƒ AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim pentru acurateÈ›e, vÄƒ rugÄƒm sÄƒ reÈ›ineÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa de origine trebuie considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm nicio responsabilitate pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri eronate care pot rezulta din utilizarea acestei traduceri.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->