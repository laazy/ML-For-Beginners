# RazvrÅ¡Äevalci kuhinj 2

V tej drugi lekciji o razvrÅ¡Äanju boste raziskali veÄ naÄinov razvrÅ¡Äanja Å¡tevilskih podatkov. NauÄili se boste tudi posledic izbire enega razvrÅ¡Äevalca namesto drugega.

## [Kviz pred predavanjem](https://ff-quizzes.netlify.app/en/ml/)

### Predpogoj

Predvidevamo, da ste opravili prejÅ¡nje lekcije in imate v vaÅ¡i mapi `data` oÄiÅ¡Äeno podatkovno zbirko imenovano _cleaned_cuisines.csv_ v korenu te 4-lekcijske mape.

### Priprava

NaloÅ¾ili smo vaÅ¡o datoteko _notebook.ipynb_ z oÄiÅ¡Äeno podatkovno zbirko in jo razdelili v podatkovni okvir X in y, pripravljena za proces gradnje modela.

## Zemljevid razvrÅ¡Äanja

Prej ste spoznali razliÄne moÅ¾nosti, ki jih imate pri razvrÅ¡Äanju podatkov z uporabo Microsoftovega prevarantskega lista. Scikit-learn ponuja podoben, a bolj podroben prevarantski list, ki lahko Å¡e dodatno pomaga zoÅ¾iti vaÅ¡e ocenovalce (drug izraz za razvrÅ¡Äevalce):

![ML Map from Scikit-learn](../../../../translated_images/sl/map.e963a6a51349425a.webp)
> Namig: [obiÅ¡Äite ta zemljevid na spletu](https://scikit-learn.org/stable/tutorial/machine_learning_map/) in klikajte po poti, da preberete dokumentacijo.

### NaÄrt

Ta zemljevid je zelo koristen, ko imate jasen vpogled v svoje podatke, saj lahko â€˜hoditeâ€™ po njegovih poteh do odloÄitve:

- Imamo >50 vzorcev
- Å½elimo napovedati kategorijo
- Imamo oznaÄene podatke
- Imamo manj kot 100.000 vzorcev
- âœ¨ Lahko izberemo Linear SVC
- ÄŒe to ne deluje, ker imamo Å¡tevilske podatke
    - Lahko poskusimo s âœ¨ KNeighbors Classifier
      - ÄŒe tudi to ne deluje, poskusimo âœ¨ SVC in âœ¨ Ensemble Classifiers

To je zelo uporabna slediti.

## Vaja - razdelite podatke

Sledi tej poti, zaÄnemo z uvozom nekaterih knjiÅ¾nic, ki jih bomo uporabili.

1. Uvoz potrebnih knjiÅ¾nic:

    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    import numpy as np
    ```

1. Razdelite svoje trening in testne podatke:

    ```python
    X_train, X_test, y_train, y_test = train_test_split(cuisines_features_df, cuisines_label_df, test_size=0.3)
    ```

## Linearni SVC razvrÅ¡Äevalec

Support-Vector clustering (SVC) je del druÅ¾ine metod strojnega uÄenja Support-Vector machines (SVM) (o njih veÄ spodaj). Pri tej metodi lahko izberete 'jedro' (kernel), da doloÄite, kako zgrupirate oznake. Parameter 'C' se nanaÅ¡a na 'regularizacijo', ki uravnava vpliv parametrov. Jedro je lahko eno izmed [veÄ](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC); tukaj ga nastavimo na 'linear', da zagotovimo uporabo linearnega SVC. Privzeto je Probability nastavljeno na 'false'; tukaj smo ga nastavili na 'true', da zberemo ocene verjetnosti. Za nakljuÄno stanje smo nastavili '0', da premeÅ¡amo podatke za verjetnosti.

### Vaja - uporabite linearen SVC

ZaÄnite z ustvarjanjem tabele razvrÅ¡Äevalcev. Postopoma boste dodajali elemente v to tabelo, ko boste testirali.

1. ZaÄnite z Linear SVC:

    ```python
    C = 10
    # Ustvari razliÄne klasifikatorje.
    classifiers = {
        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
    }
    ```

2. NauÄite svoj model z uporabo Linear SVC in izpiÅ¡ite poroÄilo:

    ```python
    n_classifiers = len(classifiers)
    
    for index, (name, classifier) in enumerate(classifiers.items()):
        classifier.fit(X_train, np.ravel(y_train))
    
        y_pred = classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
        print(classification_report(y_test,y_pred))
    ```

    Rezultat je precej dober:

    ```output
    Accuracy (train) for Linear SVC: 78.6% 
                  precision    recall  f1-score   support
    
         chinese       0.71      0.67      0.69       242
          indian       0.88      0.86      0.87       234
        japanese       0.79      0.74      0.76       254
          korean       0.85      0.81      0.83       242
            thai       0.71      0.86      0.78       227
    
        accuracy                           0.79      1199
       macro avg       0.79      0.79      0.79      1199
    weighted avg       0.79      0.79      0.79      1199
    ```

## K-najbliÅ¾jih sosedov razvrÅ¡Äevalec

K-najbliÅ¾jih sosedov spada v druÅ¾ino metod strojnega uÄenja "neighbors", ki se lahko uporabljajo tako za nadzorovano kot nenadzorovano uÄenje. Pri tej metodi se ustvari preddefinirano Å¡tevilo toÄk in podatki se zbirajo okoli teh toÄk, tako da je mogoÄe napovedati posploÅ¡ene oznake za podatke.

### Vaja - uporabite K-najbliÅ¾jih sosedov

PrejÅ¡nji razvrÅ¡Äevalec je bil dober in je deloval dobro s podatki, vendar morda lahko doseÅ¾emo boljÅ¡o natanÄnost. Poskusite s K-najbliÅ¾jih sosedov.

1. Dodajte vrstico v svojo tabelo razvrÅ¡Äevalcev (dodajte vejico za Linear SVC elementom):

    ```python
    'KNN classifier': KNeighborsClassifier(C),
    ```

    Rezultat je malo slabÅ¡i:

    ```output
    Accuracy (train) for KNN classifier: 73.8% 
                  precision    recall  f1-score   support
    
         chinese       0.64      0.67      0.66       242
          indian       0.86      0.78      0.82       234
        japanese       0.66      0.83      0.74       254
          korean       0.94      0.58      0.72       242
            thai       0.71      0.82      0.76       227
    
        accuracy                           0.74      1199
       macro avg       0.76      0.74      0.74      1199
    weighted avg       0.76      0.74      0.74      1199
    ```

    âœ… Spoznajte [K-najbliÅ¾jih sosedov](https://scikit-learn.org/stable/modules/neighbors.html#neighbors)

## Support Vector razvrÅ¡Äevalec

Support-Vector razvrÅ¡Äevalci so del druÅ¾ine [Support-Vector Machines](https://wikipedia.org/wiki/Support-vector_machine) metod strojnega uÄenja, ki se uporabljajo za razvrÅ¡Äanje in regresijo. SVM â€œpreslika trening primere v toÄke v prostoruâ€, da maksimira razdaljo med dvema kategorijama. KasnejÅ¡i podatki so preslikani v ta prostor, da je mogoÄe napovedati njihovo kategorijo.

### Vaja - uporabite Support Vector razvrÅ¡Äevalec

Poskusimo doseÄi malo boljÅ¡o natanÄnost s Support Vector razvrÅ¡Äevalcem.

1. Dodajte vejico za K-najbliÅ¾jih sosedov elementom in nato dodajte to vrstico:

    ```python
    'SVC': SVC(),
    ```

    Rezultat je precej dober!

    ```output
    Accuracy (train) for SVC: 83.2% 
                  precision    recall  f1-score   support
    
         chinese       0.79      0.74      0.76       242
          indian       0.88      0.90      0.89       234
        japanese       0.87      0.81      0.84       254
          korean       0.91      0.82      0.86       242
            thai       0.74      0.90      0.81       227
    
        accuracy                           0.83      1199
       macro avg       0.84      0.83      0.83      1199
    weighted avg       0.84      0.83      0.83      1199
    ```

    âœ… Spoznajte [Support-Vectors](https://scikit-learn.org/stable/modules/svm.html#svm)

## Ensemble razvrÅ¡Äevalci

Pojdimo do konca poti, Äeprav je bil prejÅ¡nji test precej dober. Poskusimo nekaj 'Ensemble razvrÅ¡Äevalcev', natanÄneje Random Forest in AdaBoost:

```python
  'RFST': RandomForestClassifier(n_estimators=100),
  'ADA': AdaBoostClassifier(n_estimators=100)
```

Rezultat je zelo dober, Å¡e posebej za Random Forest:

```output
Accuracy (train) for RFST: 84.5% 
              precision    recall  f1-score   support

     chinese       0.80      0.77      0.78       242
      indian       0.89      0.92      0.90       234
    japanese       0.86      0.84      0.85       254
      korean       0.88      0.83      0.85       242
        thai       0.80      0.87      0.83       227

    accuracy                           0.84      1199
   macro avg       0.85      0.85      0.84      1199
weighted avg       0.85      0.84      0.84      1199

Accuracy (train) for ADA: 72.4% 
              precision    recall  f1-score   support

     chinese       0.64      0.49      0.56       242
      indian       0.91      0.83      0.87       234
    japanese       0.68      0.69      0.69       254
      korean       0.73      0.79      0.76       242
        thai       0.67      0.83      0.74       227

    accuracy                           0.72      1199
   macro avg       0.73      0.73      0.72      1199
weighted avg       0.73      0.72      0.72      1199
```

âœ… Spoznajte [Ensemble razvrÅ¡Äevalce](https://scikit-learn.org/stable/modules/ensemble.html)

Ta metoda strojnega uÄenja "zdruÅ¾uje napovedi veÄ osnovnih ocenjevalcev", da izboljÅ¡a kakovost modela. V naÅ¡em primeru smo uporabili Random Trees in AdaBoost.

- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest), metoda povpreÄenja, gradi 'gosto' drevo 'odloÄilnih dreves' vpeto z nakljuÄnostjo, da prepreÄi prekomerno prileganje (overfitting). Parameter n_estimators je nastavljen na Å¡tevilo dreves.

- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) prilagodi razvrÅ¡Äevalec na podatkovno mnoÅ¾ico in nato prilagodi kopije tega razvrÅ¡Äevalca na isti podatkovni mnoÅ¾ici. OsredotoÄa se na uteÅ¾i nepravilno razvrÅ¡Äenih elementov in prilagaja prileganje za naslednjega razvrÅ¡Äevalca, da to popravi.

---

## ğŸš€Izziv

Vsaka od teh tehnik ima veliko parametrov, ki jih lahko spreminjate. Raziskujte privzete nastavitve vsakega in razmislite, kaj bi pomenilo prilagajanje teh parametrov za kakovost modela.

## [Kviz po predavanju](https://ff-quizzes.netlify.app/en/ml/)

## Pregled in samostojno uÄenje

V teh lekcijah je veliko strokovnega besediÅ¡Äa, zato si vzemite trenutek za pregled [tega seznama](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott) uporabne terminologije!

## DomaÄa naloga

[Igra s parametri](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Zavrnitev odgovornosti**:
Ta dokument je bil preveden z uporabo AI prevajalske storitve [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da lahko avtomatski prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem izvirnem jeziku velja za avtoritativni vir. Za pomembne informacije priporoÄamo strokovni prevod s strani Äloveka. Za morebitna nesporazume ali napaÄne razlage, ki izhajajo iz uporabe tega prevoda, ne prevzemamo odgovornosti.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->