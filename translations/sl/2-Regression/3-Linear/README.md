# Ustvarjanje regresijskega modela z uporabo Scikit-learn: regresija na Å¡tiri naÄine

## Opomba za zaÄetnike

Linearna regresija se uporablja, kadar Å¾elimo napovedati **Å¡tevilsko vrednost** (na primer cena hiÅ¡e, temperatura ali prodaja).
Deluje tako, da najde ravno Ärto, ki najbolje predstavlja razmerje med vhodnimi znaÄilnostmi in izhodom.

V tej lekciji se osredotoÄamo na razumevanje koncepta, preden raziskujemo bolj napredne regresijske tehnike.
![Linearna proti polinomska regresija infografika](../../../../translated_images/sl/linear-polynomial.5523c7cb6576ccab.webp)
> Infografika avtorja [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Predpredavalni kviz](https://ff-quizzes.netlify.app/en/ml/)

> ### [Ta lekcija je na voljo v R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Uvod

Do zdaj ste raziskali, kaj je regresija na podlagi vzorÄnih podatkov iz nabora podatkov o cenah buÄ, ki jih bomo uporabljali skozi to lekcijo. Prav tako ste jih vizualizirali z uporabo Matplotlib.

Zdaj ste pripravljeni, da se poglobite v regresijo za ML. Medtem ko vizualizacija omogoÄa razumevanje podatkov, prava moÄ strojnega uÄenja izvira iz _usposabljanja modelov_. Modele usposabljamo na zgodovinskih podatkih, da samodejno zajamejo odvisnosti podatkov, in omogoÄajo napovedovanje izidov za nove podatke, ki jih model Å¡e ni videl.

V tej lekciji boste izvedeli veÄ o dveh vrstah regresije: _osnovni linearni regresiji_ in _polinomski regresiji_, skupaj z nekaj matematike, ki je osnova teh tehnik. Ti modeli nam bodo omogoÄili napovedovanje cen buÄ glede na razliÄne vhodne podatke.

[![ML za zaÄetnike - Razumevanje linearne regresije](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML za zaÄetnike - Razumevanje linearne regresije")

> ğŸ¥ Kliknite na sliko zgoraj za kratek video pregled linearne regresije.

> V celotnem uÄnem naÄrtu predvidevamo minimalno znanje matematike in Å¾elimo, da je dostopen Å¡tudentom iz drugih podroÄij, zato bodite pozorni na opombe, ğŸ§® klice, diagrame in druge uÄne pripomoÄke, ki pomagajo pri razumevanju.

### Predpogoj

Ugotoviti bi morali, da se seznanjate s strukturo podatkov o buÄah, ki jih preuÄujemo. Najdete jih naloÅ¾ene in oÄiÅ¡Äene v datoteki _notebook.ipynb_ te lekcije. V tej datoteki je cena buÄe prikazana na mero bushela v novi podatkovni tabeli. PrepriÄajte se, da lahko te zvezke zaÅ¾enete v okolju Visual Studio Code.

### Priprava

Za opomnik, ta podatke nalagate zato, da bi lahko zastavljali vpraÅ¡anja o njih.

- Kdaj je najboljÅ¡i Äas za nakup buÄ?
- KakÅ¡no ceno lahko priÄakujem za Å¡katlo mini buÄ?
- Naj jih kupim v koÅ¡arah za pol bushela ali v Å¡katlah za 1 1/9 bushel?
Poglejmo torej podrobneje v te podatke.

V prejÅ¡nji lekciji ste ustvarili Pandasovo podatkovno tabelo in jo napolnili z delom izvirnega nabora podatkov ter standardizirali cene glede na mero bushela. S tem pa ste pridobili le pribliÅ¾no 400 podatkovnih toÄk in samo za jesenske mesece.

Poglejte si podatke, ki smo jih prednaloÅ¾ili v spremljajoÄem zvezku za to lekcijo. Podatki so prednaloÅ¾eni, zaÄetni razprÅ¡eni graf pa prikazuje meseÄne podatke. Morda lahko z dodatnim ÄiÅ¡Äenjem pridobimo veÄ podrobnosti o naravi podatkov.

## Linearna regresijska Ärta

Kot ste se nauÄili v Lekciji 1, je cilj linearne regresije narisati Ärto, ki:

- **Prikazuje odnose med spremenljivkami**. PrikaÅ¾e povezavo med spremenljivkami
- **Naredi napovedi**. Accurate napove, kje bo nova podatkovna toÄka glede na to Ärto.

ObiÄajno za **najmanjÅ¡e kvadrate regresije** nariÅ¡emo tovrstno Ärto. Izraz "NajmanjÅ¡i kvadrati" se nanaÅ¡a na proces minimizacije skupne napake v naÅ¡em modelu. Za vsako podatkovno toÄko izmerimo vertikalno razdaljo (katerakoli ostanek) med dejansko toÄko in regresijsko Ärto.

Te razdalje kvadriramo iz dveh glavnih razlogov:

1. **Velikost nad smerjo:** Å½elimo, da je napaka -5 enaka napaki +5. Kvadriranje pretvori vse vrednosti v pozitivne.

2. **Kaznovanje odstopanj:** Kvadriranje daje veÄjo teÅ¾o veÄjim napakam, zaradi Äesar mora Ärta ostati bliÅ¾je toÄkam, ki so daleÄ stran.

Nato seÅ¡tejemo vse te kvadrirane vrednosti. NaÅ¡ cilj je najti toÄno tisto linijo, kjer je ta konÄni vsota najmanjÅ¡a (najmanjÅ¡a moÅ¾na vrednost)â€”od tod ime "najmanjÅ¡i kvadrati".

> **ğŸ§® PokaÅ¾i mi matematiko**
> 
> Ta Ärta, imenovana _Ärta najboljÅ¡ega prileganja_, je izraÅ¾ena z [enaÄbo](https://en.wikipedia.org/wiki/Simple_linear_regression):
>
> ```
> Y = a + bX
> ```
>
> `X` je 'razlagaÅ¡ka spremenljivka'. `Y` je 'odvisna spremenljivka'. Naklon Ärte je `b`, `a` pa je y-presek, ki oznaÄuje vrednost `Y`, ko je `X = 0`.
>
>![izraÄun naklona](../../../../translated_images/sl/slope.f3c9d5910ddbfcf9.webp)
>
> Najprej izraÄunaj naklon `b`. Infografika avtorja [Jen Looper](https://twitter.com/jenlooper)
>
> Z drugimi besedami, in se sklicujoÄ na prvotno vpraÅ¡anje glede naÅ¡ih podatkov o buÄah: "napovedati ceno buÄe na bushel glede na mesec", bi `X` predstavljal ceno, `Y` pa mesec prodaje.
>
>![dopolni enaÄbo](../../../../translated_images/sl/calculation.a209813050a1ddb1.webp)
>
> IzraÄunaj vrednost Y. ÄŒe plaÄujeÅ¡ okoli 4 $, mora biti april! Infografika avtorja [Jen Looper](https://twitter.com/jenlooper)
>
> Matematika, ki izraÄuna Ärto, mora upoÅ¡tevati naklon Ärte, ki je odvisen tudi od preseka, torej kje je `Y`, ko je `X = 0`.
>
> Metode za izraÄun teh vrednosti si lahko ogledate na spletni strani [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). Prav tako obiÅ¡Äite [ta kalkulator najmanjÅ¡ih kvadratov](https://www.mathsisfun.com/data/least-squares-calculator.html), kjer lahko vidite, kako vrednosti Å¡tevil vplivajo na Ärto.

## Korelacija

Å e en pojem, ki ga je treba razumeti, je **koeficient korelacije** med danima spremenljivkama X in Y. Z uporabo razprÅ¡enega grafa lahko hitro vizualizirate ta koeficient. Graf s podatkovnimi toÄkami, razporejenimi v lepo Ärto, ima visoko korelacijo, medtem ko graf s podatkovnimi toÄkami, raztresenimi povsod med X in Y, ima nizko korelacijo.

Dober linearni regresijski model je tisti, ki ima visok (bliÅ¾ji 1 kot 0) koeficient korelacije, izraÄunan z metodo najmanjÅ¡ih kvadratov s Ärto regresije.

âœ… ZaÅ¾enite zvezek, ki spremlja to lekcijo, in si oglejte razprÅ¡eni graf Mesecev proti Ceni. Ali se zdi, da ima podatek, ki povezuje mesec s ceno pri prodaji buÄ, visoko ali nizko korelacijo glede na vaÅ¡o vizualno interpretacijo razprÅ¡enega grafa? Ali se kaj spremeni, Äe uporabite bolj natanÄno meritev namesto `Month`, npr. *dan v letu* (to je Å¡tevilo dni od zaÄetka leta)?

V spodnji kodi bomo predpostavili, da smo podatke oÄistili in pridobili podatkovno tabelo z imenom `new_pumpkins`, podobno spodnjemu:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> Koda za ÄiÅ¡Äenje podatkov je na voljo v [`notebook.ipynb`](notebook.ipynb). Izvedli smo enake korake ÄiÅ¡Äenja kot v prejÅ¡nji lekciji in izraÄunali stolpec `DayOfYear` z naslednjim izrazom:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Zdaj, ko imate razumevanje matematike za linearno regresijo, ustvarimo regresijski model, da vidimo, ali lahko napovemo, katera embalaÅ¾a buÄ bo imela najboljÅ¡e cene. Nekdo, ki kupuje buÄe za prazniÄni buÄni vrt, bi Å¾elel te informacije, da bi lahko optimiziral svoj nakup.

## Iskanje korelacije

[![ML za zaÄetnike - Iskanje korelacije: kljuÄ linearne regresije](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML za zaÄetnike - Iskanje korelacije: kljuÄ linearne regresije")

> ğŸ¥ Kliknite na sliko zgoraj za kratek video pregled korelacije.

Iz prejÅ¡nje lekcije ste verjetno videli, da povpreÄna cena v razliÄnih mesecih izgleda tako:

<img alt="PovpreÄna cena po mesecih" src="../../../../translated_images/sl/barchart.a833ea9194346d76.webp" width="50%"/>

To kaÅ¾e, da mora obstajati neka korelacija in lahko poskusimo usposobiti linearni regresijski model za napovedovanje odnosa med `Month` in `Price` ali med `DayOfYear` in `Price`. Tukaj je razprÅ¡eni graf, ki prikazuje slednji odnos:

<img alt="RazprÅ¡eni graf Price proti Day of Year" src="../../../../translated_images/sl/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" /> 

Poglejmo, ali obstaja korelacija z uporabo funkcije `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Zdi se, da je korelacija precej majhna, -0,15 glede na `Month` in -0,17 glede na `DayOfMonth`, lahko pa obstaja druga pomembna povezava. Zdi se, da so razliÄni grozdi cen, ki ustrezajo razliÄnim sortam buÄ. Da potrdimo to hipotezo, nariÅ¡imo vsako kategorijo buÄ z drugo barvo. Tako da podamo parameter `ax` funkciji `scatter` lahko vse toÄke nariÅ¡emo v isti graf:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="RazprÅ¡eni graf Price proti Day of Year s barvami" src="../../../../translated_images/sl/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" /> 

NaÅ¡e raziskave nakazujejo, da ima sorta veÄji vpliv na skupno ceno kot dejanski datum prodaje. To lahko vidimo z stolpiÄnim grafom:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="StolpiÄni graf cen glede na sorto buÄe" src="../../../../translated_images/sl/price-by-variety.744a2f9925d9bcb4.webp" width="50%" /> 

Za zdaj se osredotoÄimo samo na eno sorto buÄ, 'pie type', in poglejmo, kakÅ¡en vpliv ima datum na ceno:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="RazprÅ¡eni graf Price proti Day of Year za sorto pie type" src="../../../../translated_images/sl/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" /> 

ÄŒe zdaj izraÄunamo korelacijo med `Price` in `DayOfYear` z uporabo funkcije `corr`, bomo dobili nekaj takega kot `-0.27` â€“ kar pomeni, da smiselno trenirati napovedni model.

> Preden usposobimo linearen regresijski model, je pomembno zagotoviti, da so naÅ¡i podatki Äisti. Linearna regresija ne deluje dobro z manjkajoÄimi vrednostmi, zato je smiselno odstraniti vse prazne celice:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Druga moÅ¾nost je, da te prazne vrednosti zapolnimo s povpreÄnimi vrednostmi iz ustreznega stolpca.

## Enostavna linearna regresija

[![ML za zaÄetnike - Linearna in polinomska regresija z uporabo Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML za zaÄetnike - Linearna in polinomska regresija z uporabo Scikit-learn")

> ğŸ¥ Kliknite na sliko zgoraj za kratek video pregled linearne in polinomske regresije.

Za usposabljanje naÅ¡ega modela linearne regresije bomo uporabili knjiÅ¾nico **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

ZaÄnemo tako, da loÄimo vhodne vrednosti (znaÄilnosti) in priÄakovani izhod (oznako) v loÄene numpy polja:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> UpoÅ¡tevajte, da smo morali na vhodne podatke uporabiti `reshape`, da jih je paket LinearRegression pravilno razumel. Linearna regresija namreÄ priÄakuje vhod v obliki 2D polja, kjer vsak vrstiÄni vektor predstavlja vektor vhodnih znaÄilnosti. V naÅ¡em primeru, ker imamo samo en vhod, potrebujemo polje oblike N&times;1, kjer je N velikost nabora podatkov.

Nato moramo podatke razdeliti na uÄni in testni sklop, da lahko model preverimo po usposabljanju:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Na koncu usposabljanje modela linearne regresije traja le dve vrstici kode. Definiramo objekt `LinearRegression` in ga prilagodimo naÅ¡im podatkom z metodo `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Objekt `LinearRegression` po prilagoditvi (`fit`) vsebuje vse koeficiente regresije, do katerih lahko dostopamo z lastnostjo `.coef_`. V naÅ¡em primeru je samo en koeficient, ki bi moral biti okoli `-0.017`. To pomeni, da se cene zdi, da nekoliko padajo s Äasom, vendar ne preveÄ, pribliÅ¾no 2 centa na dan. Do preseÄiÅ¡Äa regresije z Y-osjo lahko dostopamo tudi z `lin_reg.intercept_` - v naÅ¡em primeru bo okoli `21`, kar daje informacijo o ceni na zaÄetku leta.

Za preverjanje toÄnosti naÅ¡ega modela lahko napovemo cene na testnem naboru podatkov in nato merimo, kako blizu so naÅ¡e napovedi priÄakovanim vrednostim. To lahko storimo z merjenjem srednje kvadratne napake (MSE), kar je srednja vrednost vseh kvadratov razlik med priÄakovano in napovedano vrednostjo.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

NaÅ¡a napaka je okoli 2 toÄki, kar je ~17 %. Ni ravno dobro. Drug indikator kakovosti modela je **koeficient determinacije**, ki ga lahko pridobimo na naslednji naÄin:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
ÄŒe je vrednost 0, pomeni, da model ne upoÅ¡teva vhodnih podatkov in deluje kot *najslabÅ¡i linearni napovedovalec*, ki je preprosto povpreÄna vrednost rezultata. Vrednost 1 pomeni, da lahko popolnoma napovemo vse priÄakovane izide. V naÅ¡em primeru je koeficient okoli 0.06, kar je precej nizko.

Testne podatke lahko tudi nariÅ¡emo skupaj z regresijsko premico, da bolje vidimo, kako regresija deluje v naÅ¡em primeru:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linear regression" src="../../../../translated_images/sl/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## Polinomska regresija

Druga vrsta linearne regresije je polinomska regresija. ÄŒeprav obstaja vÄasih linearna povezava med spremenljivkami - veÄja kot je buÄa po prostornini, viÅ¡ja je cena - vÄasih teh povezav ni mogoÄe prikazati kot ravnino ali ravno Ärto.

âœ… Tu je [Å¡e nekaj primerov](https://online.stat.psu.edu/stat501/lesson/9/9.8) podatkov, kjer bi lahko uporabili polinomsko regresijo.

Oglejmo si Å¡e enkrat odnos med Datumom in Ceno. Ali ta diagram razprÅ¡itve nujno kaÅ¾e, da se mora analizirati z ravno Ärto? Ali cene ne morejo nihati? V tem primeru lahko poskusimo polinomsko regresijo.

âœ… Polinomi so matematiÄni izrazi, ki so lahko sestavljeni iz ene ali veÄ spremenljivk in koeficientov.

Polinomska regresija ustvari ukrivljeno Ärto, da bolje ustreza nelinearnim podatkom. V naÅ¡em primeru, Äe vkljuÄimo kvadratno variablo `DayOfYear`, bi morali podatke prilagoditi paraboli, ki bo imela minimum v doloÄenem trenutku v letu.

Scikit-learn vkljuÄuje uporabno [pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) za zdruÅ¾evanje razliÄnih korakov obdelave podatkov skupaj. **Pipeline** je veriga **ocenjevalcev**. V naÅ¡em primeru bomo ustvarili pipeline, ki najprej doda polinomske znaÄilnosti modelu, nato pa izvede uÄenje regresije:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Uporaba `PolynomialFeatures(2)` pomeni, da bomo vkljuÄili vse polinome drugega reda iz vhodnih podatkov. V naÅ¡em primeru to pomeni samo `DayOfYear`<sup>2</sup>, vendar Äe imamo dve vhodni spremenljivki X in Y, bo to dodalo X<sup>2</sup>, XY in Y<sup>2</sup>. Uporabimo lahko tudi poljuben viÅ¡ji red polinomov, Äe Å¾elimo.

Pipepline lahko uporabljamo na enak naÄin kot izvirni objekt `LinearRegression`, torej lahko na pipeline uporabimo `fit`, nato pa uporabimo `predict` za pridobitev rezultatov napovedi. Tu je graf, ki prikazuje testne podatke in pribliÅ¾no krivuljo:

<img alt="Polynomial regression" src="../../../../translated_images/sl/poly-results.ee587348f0f1f60b.webp" width="50%" />

Z uporabo polinomske regresije lahko doseÅ¾emo nekoliko niÅ¾jo vrednost MSE in viÅ¡ji koeficient determinacije, vendar ne bistveno. Potrebno je upoÅ¡tevati tudi druge znaÄilnosti!

> Vidite, da so minimalne cene buÄ opazne nekje okoli noÄi Äarovnic. Kako bi to razloÅ¾ili? 

ğŸƒ ÄŒestitamo, ustvarili ste model, ki lahko napove ceno buÄ za pito. Verjetno lahko isto proceduro ponovite za vse vrste buÄ, ampak to bi bilo zamudno. NauÄimo se zdaj, kako upoÅ¡tevati vrsto buÄe v naÅ¡em modelu!

## Kategorikalne znaÄilnosti

V idealnem svetu Å¾elimo napovedovati cene za razliÄne vrste buÄ z uporabo istega modela. Vendar pa je stolpec `Variety` nekoliko drugaÄen od stolpcev, kot je `Month`, ker vsebuje nenumeriÄne vrednosti. Takim stolpcem pravimo **kategorikalni**.

[![ML za zaÄetnike - Napovedi kategorialnih znaÄilnosti z linearno regresijo](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML za zaÄetnike - Napovedi kategorialnih znaÄilnosti z linearno regresijo")

> ğŸ¥ Kliknite sliko zgoraj za kratek video pregled uporabe kategorikalnih znaÄilnosti.

Tukaj si lahko ogledate, kako povpreÄna cena zavisi od vrste:

<img alt="Average price by variety" src="../../../../translated_images/sl/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

Za upoÅ¡tevanje vrste moramo najprej pretvoriti podatke v Å¡tevilÄno obliko, oziroma jih **kodirati**. Obstaja veÄ naÄinov za to:

* Preprosta **numeriÄna koda** bo sestavila tabelo razliÄnih vrst, nato pa zamenjala ime vrstice z indeksom v tej tabeli. To ni najboljÅ¡a ideja za linearno regresijo, ker linearna regresija vzame dejansko numeriÄno vrednost indeksa in jo doda rezultatu, pomnoÅ¾eno z nekim koeficientom. V naÅ¡em primeru je odnos med Å¡tevilom indeksa in ceno oÄitno nelinearen, tudi Äe zagotovimo, da so indeksi urejeni na doloÄen naÄin.
* **One-hot kodiranje** bo stolpec `Variety` nadomestilo s 4 razliÄnimi stolpci, po enim za vsako vrsto. Vsak stolpec bo vseboval `1`, Äe je ustrezni vrstici doloÄena vrsta, in `0` sicer. To pomeni, da bo v linearni regresiji Å¡tiri koeficiente, po enega za vsako vrsto buÄe, ki bo odgovoren za "zaÄetno ceno" (ali bolj "dodatno ceno") za doloÄeno vrsto.

Spodnja koda prikazuje, kako lahko one-hot kodiramo vrsto:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Za uÄenje linearne regresije z one-hot kodirano vrsto kot vhodom moramo samo pravilno pripraviti podatka `X` in `y`:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Preostanek kode je enak tisti, ki smo jo uporabili prej za uÄenje linearne regresije. ÄŒe to poskusite, boste videli, da je srednja kvadratna napaka pribliÅ¾no enaka, vendar bomo dobili precej viÅ¡ji koeficient determinacije (~77 %). Za Å¡e natanÄnejÅ¡e napovedi lahko upoÅ¡tevamo veÄ kategorikalnih znaÄilnosti ter tudi numeriÄne, kot sta `Month` ali `DayOfYear`. Za zdruÅ¾itev vseh znaÄilnosti v eno veliko tabelo lahko uporabimo `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Tukaj upoÅ¡tevamo tudi `City` in tip `Package`, kar nam da MSE 2.84 (10 %) in koeficient determinacije 0.94!

## Vse skupaj

Za najboljÅ¡i model lahko uporabimo zdruÅ¾ene (one-hot kodirane kategorikalne in numeriÄne) podatke iz zgornjega primera skupaj s polinomsko regresijo. Tukaj je celotna koda za vaÅ¡o udobje:

```python
# nastavi uÄne podatke
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# naredi razdelitev na uÄno in testno mnoÅ¾ico
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# nastavi in izuÄi cevovod
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# napovej rezultate za testne podatke
pred = pipeline.predict(X_test)

# izraÄunaj MSE in koeficient determinacije
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

To nam mora dati najboljÅ¡i koeficient determinacije skoraj 97 % in MSE=2.23 (~8 % napake napovedi).

| Model | MSE | Koeficient determinacije |
|-------|-----|--------------------------|
| Linearni `DayOfYear` | 2.77 (17,2 %) | 0.07 |
| Polinomski `DayOfYear` | 2.73 (17,0 %) | 0.08 |
| Linearni `Variety` | 5.24 (19,7 %) | 0.77 |
| Linearni - vse znaÄilnosti | 2.84 (10,5 %) | 0.94 |
| Polinomski - vse znaÄilnosti | 2.23 (8,25 %) | 0.97 |

ğŸ† OdliÄno! V eni lekciji ste ustvarili Å¡tiri regresijske modele in izboljÅ¡ali kakovost modela na 97 %. V zadnjem delu o regresiji se boste nauÄili o logistiÄni regresiji za doloÄanje kategorij.

---
## ğŸš€Izazov

Preizkusite veÄ razliÄnih spremenljivk v tej zvezki, da vidite, kako korelacija ustreza natanÄnosti modela.

## [Kviz po predavanju](https://ff-quizzes.netlify.app/en/ml/)

## Pregled in samostojno uÄenje

V tej lekciji smo se nauÄili o linearni regresiji. Obstajajo Å¡e druge pomembne vrste regresije. Preberite o tehnikah Stepwise, Ridge, Lasso in Elasticnet. Dober teÄaj za nadaljnje uÄenje je [Stanford Statistical Learning course](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## Naloga

[Ustvari model](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Omejitev odgovornosti**:
Ta dokument je bil preveden z uporabo AI prevajalske storitve [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da avtomatizirani prevodi lahko vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem maternem jeziku velja za avtoritativni vir. Za kljuÄne informacije priporoÄamo strokovni ÄloveÅ¡ki prevod. Nismo odgovorni za kakrÅ¡nekoli nesporazume ali napaÄne interpretacije, ki izhajajo iz uporabe tega prevoda.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->