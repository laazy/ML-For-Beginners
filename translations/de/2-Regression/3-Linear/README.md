# Erstellen Sie ein Regressionsmodell mit Scikit-learn: Regression auf vier Arten

## Anf√§ngerhinweis

Lineare Regression wird verwendet, wenn wir einen **numerischen Wert** vorhersagen m√∂chten (zum Beispiel Hauspreis, Temperatur oder Umsatz).
Sie funktioniert, indem sie eine Gerade findet, die am besten die Beziehung zwischen Eingabemerkmalen und Ausgabe darstellt.

In dieser Lektion konzentrieren wir uns darauf, das Konzept zu verstehen, bevor wir fortgeschrittenere Regressionsmethoden erkunden.
![Lineare vs polynomiale Regression Infografik](../../../../translated_images/de/linear-polynomial.5523c7cb6576ccab.webp)
> Infografik von [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Vorlesungsquiz](https://ff-quizzes.netlify.app/en/ml/)

> ### [Diese Lektion ist auch in R verf√ºgbar!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Einf√ºhrung

Bisher haben Sie erkundet, was Regression mit Beispieldaten aus dem K√ºrbisdaten-Set ist, das wir im Verlauf dieser Lektion verwenden werden. Sie haben es auch mit Matplotlib visualisiert.

Nun sind Sie bereit, tiefer in Regression f√ºr ML einzutauchen. W√§hrend Visualisierung Ihnen hilft, Daten zu verstehen, liegt die wahre Kraft des maschinellen Lernens im _Trainieren von Modellen_. Modelle werden anhand historischer Daten trainiert, um Datenabh√§ngigkeiten automatisch zu erfassen, und erm√∂glichen es Ihnen, Ergebnisse f√ºr neue Daten vorherzusagen, die das Modell noch nicht gesehen hat.

In dieser Lektion lernen Sie zwei Arten der Regression n√§her kennen: _einfache lineare Regression_ und _polynomiale Regression_, zusammen mit der Mathematik hinter diesen Techniken. Diese Modelle erm√∂glichen es uns, K√ºrbisspreise basierend auf verschiedenen Eingabedaten vorherzusagen.

[![ML f√ºr Anf√§nger - Verst√§ndnis der linearen Regression](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML f√ºr Anf√§nger - Verst√§ndnis der linearen Regression")

> üé• Klicken Sie auf das Bild oben f√ºr eine kurze Video√ºbersicht zur linearen Regression.

> Im gesamten Curriculum gehen wir von minimalen Mathematikkenntnissen aus und bem√ºhen uns, es f√ºr Studierende aus anderen Fachrichtungen zug√§nglich zu machen. Achten Sie daher auf Hinweise, üßÆ Mathe-Abschnitte, Diagramme und andere Lernhilfen zur besseren Verst√§ndlichkeit.

### Voraussetzungen

Sie sollten inzwischen mit der Struktur der K√ºrbisdaten vertraut sein, die wir untersuchen. Sie finden diese vorab geladen und bereinigt in der _notebook.ipynb_-Datei dieser Lektion. Dort wird der K√ºrbisspreis pro Scheffel in einem neuen DataFrame angezeigt. Stellen Sie sicher, dass Sie diese Notebooks in Visual Studio Code ausf√ºhren k√∂nnen.

### Vorbereitung

Zur Erinnerung: Sie laden diese Daten, um Fragen an sie zu stellen.

- Wann ist die beste Zeit, K√ºrbisse zu kaufen?
- Welchen Preis kann ich f√ºr eine Kiste Miniaturk√ºrbisse erwarten?
- Sollte ich sie in halben Scheffel-K√∂rben oder im 1 1/9 Scheffel-Karton kaufen?
Lassen Sie uns weiter in diese Daten eintauchen.

In der vorherigen Lektion haben Sie einen Pandas-DataFrame erstellt und mit Teilen des urspr√ºnglichen Datensatzes gef√ºllt, wobei die Preise standardisiert pro Scheffel angegeben wurden. Dadurch hatten Sie allerdings nur etwa 400 Datenpunkte und nur f√ºr die Herbstmonate.

Sehen Sie sich die Daten an, die wir im begleitenden Notebook dieser Lektion vorladen. Die Daten sind vorab geladen und ein erster Streudiagramm wird geplottet, um Monatsdaten anzuzeigen. Vielleicht k√∂nnen wir durch weitere Bereinigung mehr Details √ºber die Art der Daten herausfinden.

## Eine lineare Regressionslinie

Wie Sie in Lektion 1 gelernt haben, ist das Ziel einer linearen Regression, eine Linie zu plotten, die:

- **Variable Beziehungen zeigt.** Zeigt die Beziehung zwischen Variablen.
- **Vorhersagen macht.** Macht genaue Vorhersagen dar√ºber, wo ein neuer Datenpunkt in Bezug auf diese Linie liegen w√ºrde.

Typischerweise wird diese Art Linie mit **Methode der kleinsten Quadrate** (Least-Squares Regression) gezeichnet. Der Begriff ‚Äûkleinste Quadrate‚Äú bezieht sich auf den Prozess, den Gesamtfehler unseres Modells zu minimieren. F√ºr jeden Datenpunkt messen wir die vertikale Entfernung (Residual genannt) zwischen dem tats√§chlichen Punkt und unserer Regressionslinie.

Diese Abst√§nde quadrieren wir aus zwei Hauptgr√ºnden:

1. **Betrag vor Richtung:** Wir wollen einen Fehler von -5 genauso behandeln wie einen Fehler von +5. Das Quadrieren macht alle Werte positiv.

2. **Bestrafung von Ausrei√üern:** Durch Quadrieren erhalten gr√∂√üere Fehler ein gr√∂√üeres Gewicht, was die Linie dazu zwingt, n√§her an entfernten Punkten zu bleiben.

Anschlie√üend summieren wir alle quadrierten Werte. Unser Ziel ist es, genau die Linie zu finden, bei der diese endg√ºltige Summe am kleinsten ist (der kleinstm√∂gliche Wert) ‚Äì daher der Name ‚ÄûMethode der kleinsten Quadrate‚Äú.

> **üßÆ Zeig mir die Mathematik**
> 
> Diese Linie, auch als _Line of Best Fit_ bezeichnet, kann durch [eine Gleichung](https://en.wikipedia.org/wiki/Simple_linear_regression) ausgedr√ºckt werden:
> 
> ```
> Y = a + bX
> ```
>
> `X` ist die ‚Äöerkl√§rende Variable‚Äò. `Y` ist die ‚Äöabh√§ngige Variable‚Äò. Die Steigung der Linie ist `b` und `a` ist der y-Achsenabschnitt, also der Wert von `Y`, wenn `X = 0` ist.
>
>![Berechnung der Steigung](../../../../translated_images/de/slope.f3c9d5910ddbfcf9.webp)
>
> Berechnung der Steigung `b`. Infografik von [Jen Looper](https://twitter.com/jenlooper)
>
> Anders ausgedr√ºckt und im Hinblick auf die urspr√ºngliche Fragestellung unserer K√ºrbisdaten: "Vorhersage des Preises eines K√ºrbisses pro Scheffel nach Monat" w√ºrde `X` f√ºr den Preis stehen und `Y` f√ºr den Verkaufsmonat.
>
>![Gleichung vervollst√§ndigen](../../../../translated_images/de/calculation.a209813050a1ddb1.webp)
>
> Berechnung des Werts von Y. Wenn man ungef√§hr 4 $ bezahlt, muss es April sein! Infografik von [Jen Looper](https://twitter.com/jenlooper)
>
> Die Formel, die die Linie berechnet, muss die Steigung der Linie darstellen, die auch vom Achsenabschnitt abh√§ngt, also dem Ort, wo `Y` liegt, wenn `X = 0`.
>
> Die Berechnung der Werte kann auf der Webseite [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) nachvollzogen werden. Besuchen Sie auch [diesen Methode-der-kleinsten-Quadrate-Rechner](https://www.mathsisfun.com/data/least-squares-calculator.html), um zu sehen, wie sich Werte auf die Linie auswirken.

## Korrelation

Ein weiterer Begriff, den Sie verstehen sollten, ist der **Korrelationskoeffizient** zwischen bestimmten X- und Y-Variablen. Mittels Streudiagramm k√∂nnen Sie diesen Koeffizienten schnell visualisieren. Ein Plot mit Punkten, die auf einer sauberen Linie liegen, weist eine hohe Korrelation auf, w√§hrend Punkte, die √ºberall verstreut sind, eine niedrige Korrelation anzeigen.

Ein gutes lineares Regressionsmodell weist mit der Methode der kleinsten Quadrate eine hohe (n√§her an 1 als an 0) Korrelation f√ºr die Regressionslinie auf.

‚úÖ F√ºhren Sie das zugeh√∂rige Notebook dieser Lektion aus und betrachten Sie das Streudiagramm von Monat zu Preis. Scheint laut Ihrer visuellen Interpretation der Punktverteilung die Zuordnung Monat zu Preis bei K√ºrbisverk√§ufen eine hohe oder niedrige Korrelation zu haben? √Ñndert sich das, wenn Sie anstatt des `Monats` eine feinere Messgr√∂√üe verwenden, z. B. den *Tag des Jahres* (also die Anzahl der Tage seit Jahresbeginn)?

Im folgenden Code nehmen wir an, dass wir die Daten bereinigt haben und einen DataFrame namens `new_pumpkins` erhalten haben, der etwa so aussieht:

ID | Monat | TagImJahr | Sorte | Stadt | Verpackung | Niedrigster Preis | H√∂chster Preis | Preis
---|-------|-----------|-------|--------|------------|------------------|----------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 Scheffel Kartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 Scheffel Kartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 Scheffel Kartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 Scheffel Kartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 Scheffel Kartons | 15.0 | 15.0 | 13.636364

> Der Code zum Bereinigen der Daten ist in [`notebook.ipynb`](notebook.ipynb) verf√ºgbar. Wir haben die gleichen Bereinigungsschritte wie in der vorherigen Lektion durchgef√ºhrt und die Spalte `TagImJahr` mit folgendem Ausdruck berechnet:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Jetzt, wo Sie das mathematische Fundament der linearen Regression verstanden haben, erstellen wir ein Regressionsmodell, um zu sehen, ob wir vorhersagen k√∂nnen, welches Verpackungspaket die besten K√ºrbisspreise haben wird. Jemand, der f√ºr ein Urlaubsk√ºrbisfeld K√ºrbisse kauft, m√∂chte diese Information, um seine K√§ufe zu optimieren.

## Suche nach Korrelation

[![ML f√ºr Anf√§nger ‚Äì Suche nach Korrelation: Der Schl√ºssel zur linearen Regression](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML f√ºr Anf√§nger ‚Äì Suche nach Korrelation: Der Schl√ºssel zur linearen Regression")

> üé• Klicken Sie auf das Bild oben f√ºr eine kurze Video√ºbersicht zur Korrelation.

Aus der vorherigen Lektion wissen Sie wahrscheinlich, dass der Durchschnittspreis f√ºr verschiedene Monate so aussieht:

<img alt="Durchschnittspreis nach Monat" src="../../../../translated_images/de/barchart.a833ea9194346d76.webp" width="50%"/>

Das deutet darauf hin, dass eine Korrelation bestehen sollte, und wir k√∂nnen versuchen, ein lineares Regressionsmodell zu trainieren, um die Beziehung zwischen `Monat` und `Preis` oder zwischen `TagImJahr` und `Preis` vorherzusagen. Hier ist das Streudiagramm, das letztere Beziehung zeigt:

<img alt="Streudiagramm von Preis vs. Tag im Jahr" src="../../../../translated_images/de/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" /> 

Sehen wir nach, ob eine Korrelation mit der Funktion `corr` besteht:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Die Korrelation ist offenbar recht klein, -0,15 beim `Monat` und -0,17 beim `TagImMonat`, aber es k√∂nnte einen weiteren wichtigen Zusammenhang geben. Es scheint verschiedene Preisgruppen zu geben, die verschiedenen K√ºrbissorten entsprechen. Um diese Hypothese zu best√§tigen, plotten wir jede K√ºrbissorte mit einer anderen Farbe. Indem wir der `scatter`-Plotfunktion einen `ax`-Parameter √ºbergeben, k√∂nnen wir alle Punkte auf demselben Diagramm darstellen:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Streudiagramm von Preis vs. Tag im Jahr" src="../../../../translated_images/de/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" /> 

Unsere Untersuchung legt nahe, dass die Sorte einen gr√∂√üeren Einfluss auf den Gesamtpreis hat als das tats√§chliche Verkaufsdatum. Das sehen wir auch in einem Balkendiagramm:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Balkendiagramm von Preis vs. Sorte" src="../../../../translated_images/de/price-by-variety.744a2f9925d9bcb4.webp" width="50%" /> 

Fokussieren wir uns jetzt nur auf eine K√ºrbissorte, die 'pie type', und betrachten den Einfluss des Datums auf den Preis:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Streudiagramm von Preis vs. Tag im Jahr" src="../../../../translated_images/de/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" /> 

Wenn wir nun die Korrelation zwischen `Preis` und `TagImJahr` mit der Funktion `corr` berechnen, erhalten wir etwa `-0.27` ‚Äì was bedeutet, dass es sinnvoll ist, ein Vorhersagemodell zu trainieren.

> Bevor Sie ein lineares Regressionsmodell trainieren, ist es wichtig sicherzustellen, dass unsere Daten sauber sind. Lineare Regression funktioniert nicht gut mit fehlenden Werten, daher macht es Sinn, alle leeren Zellen zu entfernen:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Eine weitere M√∂glichkeit w√§re, diese leeren Werte mit Mittelwerten aus der entsprechenden Spalte zu f√ºllen.

## Einfache Lineare Regression

[![ML f√ºr Anf√§nger ‚Äì Lineare und Polynomiale Regression mit Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML f√ºr Anf√§nger ‚Äì Lineare und Polynomiale Regression mit Scikit-learn")

> üé• Klicken Sie auf das Bild oben f√ºr eine kurze Video√ºbersicht zu linearer und polynomialer Regression.

Um unser lineares Regressionsmodell zu trainieren, verwenden wir die **Scikit-learn**-Bibliothek.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Wir beginnen damit, Eingabewerte (Features) und die erwartete Ausgabe (Label) in separate numpy-Arrays aufzuteilen:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Beachten Sie, dass wir die Eingabedaten `reshape`-en mussten, damit das Linear Regression Paket sie korrekt versteht. Lineare Regression erwartet ein 2D-Array als Eingabe, bei dem jede Zeile einem Vektor von Eingabe-Features entspricht. Da wir nur ein Feature haben, ben√∂tigen wir ein Array der Form N&times;1, wobei N die Datensatzgr√∂√üe ist.

Anschlie√üend teilen wir die Daten in Trainings- und Testdatens√§tze auf, damit wir unser Modell nach dem Training validieren k√∂nnen:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Das Training des eigentlichen linearen Regressionsmodells ben√∂tigt nur zwei Codezeilen. Wir definieren das `LinearRegression`-Objekt und passen es mit der Methode `fit` an unsere Daten an:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Das `LinearRegression`-Objekt enth√§lt nach dem `fit`-ten alle Koeffizienten der Regression, auf die √ºber die `.coef_`-Eigenschaft zugegriffen werden kann. In unserem Fall gibt es nur einen Koeffizienten, der etwa `-0.017` sein sollte. Das bedeutet, dass die Preise mit der Zeit etwas zu sinken scheinen, aber nicht zu stark, etwa 2 Cent pro Tag. Wir k√∂nnen auch den Schnittpunkt der Regression mit der Y-Achse √ºber `lin_reg.intercept_` abrufen ‚Äì dieser wird in unserem Fall etwa `21` betragen und zeigt den Preis zu Beginn des Jahres an.

Um zu sehen, wie genau unser Modell ist, k√∂nnen wir die Preise auf einem Testdatensatz vorhersagen und dann messen, wie nah unsere Vorhersagen an den erwarteten Werten liegen. Dies kann mit der mittleren quadratischen Abweichung (MSE) gemessen werden, die der Mittelwert aller quadrierten Differenzen zwischen erwarteten und vorhergesagten Werten ist.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

Unser Fehler scheint bei etwa 2 Punkten zu liegen, was ~17 % entspricht. Nicht besonders gut. Ein weiterer Indikator f√ºr die Modellg√ºte ist der **Bestimmtheitsma√ü**, der wie folgt erhalten werden kann:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Wenn der Wert 0 ist, bedeutet das, dass das Modell die Eingabedaten nicht ber√ºcksichtigt und als *schlechtester linearer Pr√§diktor* fungiert, der einfach der Mittelwert der Ergebnisse ist. Ein Wert von 1 bedeutet, dass wir alle erwarteten Ausgaben perfekt vorhersagen k√∂nnen. In unserem Fall liegt der Koeffizient bei etwa 0,06, was ziemlich niedrig ist.

Wir k√∂nnen auch die Testdaten zusammen mit der Regressionslinie plotten, um besser zu sehen, wie die Regression in unserem Fall funktioniert:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Lineare Regression" src="../../../../translated_images/de/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## Polynomiale Regression

Eine weitere Form der linearen Regression ist die polynomiale Regression. W√§hrend manchmal eine lineare Beziehung zwischen Variablen besteht ‚Äì je gr√∂√üer der K√ºrbis im Volumen, desto h√∂her der Preis ‚Äì, k√∂nnen diese Zusammenh√§nge manchmal nicht als Ebene oder Gerade dargestellt werden.

‚úÖ Hier sind [einige weitere Beispiele](https://online.stat.psu.edu/stat501/lesson/9/9.8) f√ºr Daten, bei denen polynomiale Regression verwendet werden k√∂nnte.

Werfen Sie einen weiteren Blick auf die Beziehung zwischen Datum und Preis. Sieht dieses Streudiagramm so aus, als sollte es unbedingt durch eine Gerade analysiert werden? K√∂nnen sich Preise nicht schwanken? In diesem Fall k√∂nnen Sie polynomiale Regression ausprobieren.

‚úÖ Polynome sind mathematische Ausdr√ºcke, die aus einer oder mehreren Variablen und Koeffizienten bestehen k√∂nnen.

Die polynomiale Regression erstellt eine gebogene Linie, um nichtlineare Daten besser zu modellieren. In unserem Fall, wenn wir eine quadrierte `DayOfYear`-Variable in die Eingabedaten aufnehmen, sollten wir in der Lage sein, unsere Daten mit einer parabolischen Kurve zu approximieren, die einen Minimalwert an einem bestimmten Punkt im Jahr hat.

Scikit-learn bietet eine hilfreiche [Pipeline-API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline), um verschiedene Schritte der Datenverarbeitung zu kombinieren. Eine **Pipeline** ist eine Kette von **Estimatoren**. In unserem Fall erstellen wir eine Pipeline, die zuerst polynomiale Merkmale zum Modell hinzuf√ºgt und dann die Regression trainiert:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Die Verwendung von `PolynomialFeatures(2)` bedeutet, dass alle Polynome zweiten Grades aus den Eingabedaten enthalten sind. In unserem Fall hei√üt das nur `DayOfYear`<sup>2</sup>, aber bei zwei Eingabevariablen X und Y f√ºgt dies X<sup>2</sup>, XY und Y<sup>2</sup> hinzu. Wir k√∂nnen auch Polynome h√∂heren Grades verwenden, wenn wir m√∂chten.

Pipelines k√∂nnen auf die gleiche Weise wie das urspr√ºngliche `LinearRegression`-Objekt verwendet werden, d.h. wir k√∂nnen die Pipeline `fit`-ten und dann mit `predict` die Vorhersagen erhalten. Hier ist die Grafik, die die Testdaten und die Approximationskurve zeigt:

<img alt="Polynomiale Regression" src="../../../../translated_images/de/poly-results.ee587348f0f1f60b.webp" width="50%" />

Mit Polynomial Regression k√∂nnen wir etwas niedrigere MSE und h√∂here Bestimmtheitsma√üwerte erreichen, aber nicht signifikant. Wir m√ºssen auch andere Merkmale ber√ºcksichtigen!

> Sie k√∂nnen sehen, dass die minimalen K√ºrbisspreise irgendwo rund um Halloween beobachtet werden. Wie k√∂nnen Sie das erkl√§ren?

üéÉ Herzlichen Gl√ºckwunsch, Sie haben gerade ein Modell erstellt, das helfen kann, den Preis von Pie-K√ºrbissen vorherzusagen. Sie k√∂nnen das gleiche Verfahren wahrscheinlich f√ºr alle K√ºrbissorten wiederholen, aber das w√§re m√ºhsam. Lernen wir nun, wie wir K√ºrbissorten in unser Modell einbeziehen k√∂nnen!

## Kategorielle Merkmale

In einer idealen Welt m√∂chten wir in der Lage sein, Preise f√ºr verschiedene K√ºrbissorten mit demselben Modell vorherzusagen. Die Spalte `Variety` ist jedoch etwas anders als Spalten wie `Month`, da sie nichtnumerische Werte enth√§lt. Solche Spalten nennt man **kategorisch**.

[![Maschinelles Lernen f√ºr Anf√§nger ‚Äì Vorhersagen mit kategorialen Merkmalen und linearer Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "Maschinelles Lernen f√ºr Anf√§nger ‚Äì Vorhersagen mit kategorialen Merkmalen und linearer Regression")

> üé• Klicken Sie auf das obige Bild f√ºr eine kurze Video√ºbersicht zur Verwendung kategorialer Merkmale.

Hier sehen Sie, wie der Durchschnittspreis von der Sorte abh√§ngt:

<img alt="Durchschnittspreis nach Sorte" src="../../../../translated_images/de/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

Um die Sorte zu ber√ºcksichtigen, m√ºssen wir sie zun√§chst in eine numerische Form umwandeln oder **kodieren**. Es gibt mehrere M√∂glichkeiten, dies zu tun:

* Eine einfache **numerische Kodierung** erstellt eine Tabelle mit verschiedenen Sorten und ersetzt dann den Sortennamen durch einen Index in dieser Tabelle. Dies ist keine gute Idee f√ºr lineare Regression, da lineare Regression den tats√§chlichen numerischen Wert des Index nimmt und diesen mit einem Koeffizienten multipliziert und zum Ergebnis addiert. In unserem Fall ist der Zusammenhang zwischen der Indexnummer und dem Preis eindeutig nicht linear, selbst wenn wir sicherstellen, dass die Indizes in einer bestimmten Reihenfolge angeordnet sind.
* **One-Hot-Kodierung** ersetzt die Spalte `Variety` durch 4 verschiedene Spalten, eine f√ºr jede Sorte. Jede Spalte enth√§lt `1`, wenn die entsprechende Zeile zu dieser Sorte geh√∂rt, und `0` sonst. Das bedeutet, dass es vier Koeffizienten in der linearen Regression gibt, einen f√ºr jede K√ºrbissorte, der f√ºr den ‚ÄûStartpreis‚Äú (bzw. ‚Äûzus√§tzlichen Preis‚Äú) f√ºr diese Sorte verantwortlich ist.

Der folgende Code zeigt, wie wir eine Sorte one-hot kodieren k√∂nnen:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Um die lineare Regression mit der one-hot-kodierten Sorte als Eingabe zu trainieren, m√ºssen wir `X` und `y` nur richtig initialisieren:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Der Rest des Codes ist derselbe wie oben beim Trainieren der linearen Regression. Wenn Sie es ausprobieren, werden Sie sehen, dass die mittlere quadratische Abweichung ungef√§hr gleich ist, aber wir einen viel h√∂heren Bestimmtheitsma√ü (~77 %) erreichen. Um noch genauere Vorhersagen zu erhalten, k√∂nnen wir weitere kategoriale Merkmale sowie numerische Merkmale wie `Month` oder `DayOfYear` ber√ºcksichtigen. Um ein gro√ües Array von Merkmalen zu erhalten, k√∂nnen wir `join` verwenden:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Hier ber√ºcksichtigen wir auch die `City` und den `Package`-Typ, was uns eine MSE von 2,84 (10 %) und eine Bestimmtheit von 0,94 ergibt!

## Alles zusammenf√ºhren

Um das beste Modell zu erstellen, k√∂nnen wir kombinierte (one-hot-kodierte kategoriale + numerische) Daten aus dem obigen Beispiel zusammen mit polynomialer Regression verwenden. Hier ist der vollst√§ndige Code f√ºr Ihre Bequemlichkeit:

```python
# Trainingsdaten einrichten
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# Zug- und Testaufteilung durchf√ºhren
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Pipeline einrichten und trainieren
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# Ergebnisse f√ºr Testdaten vorhersagen
pred = pipeline.predict(X_test)

# MSE und Bestimmtheitsma√ü berechnen
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Dies sollte uns den besten Bestimmtheitsma√ü von fast 97 % und eine MSE von 2,23 (~8 % Vorhersagefehler) liefern.

| Modell | MSE | Bestimmtheitsma√ü |
|--------|-----|------------------|
| `DayOfYear` Linear | 2,77 (17,2 %) | 0,07 |
| `DayOfYear` Polynomial | 2,73 (17,0 %) | 0,08 |
| `Variety` Linear | 5,24 (19,7 %) | 0,77 |
| Alle Merkmale Linear | 2,84 (10,5 %) | 0,94 |
| Alle Merkmale Polynomial | 2,23 (8,25 %) | 0,97 |

üèÜ Gut gemacht! Sie haben in einer Lektion vier Regressionsmodelle erstellt und die Modellqualit√§t auf 97 % verbessert. Im abschlie√üenden Abschnitt √ºber Regression werden Sie etwas √ºber logistische Regression lernen, um Kategorien zu bestimmen.

---
## üöÄHerausforderung

Testen Sie in diesem Notebook verschiedene Variablen, um zu sehen, wie die Korrelation mit der Modellgenauigkeit zusammenh√§ngt.

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ml/)

## R√ºckblick & Selbststudium

In dieser Lektion haben wir lineare Regression kennengelernt. Es gibt weitere wichtige Arten von Regressionen. Lesen Sie √ºber Stepwise, Ridge, Lasso und Elasticnet-Techniken. Ein guter Kurs zum Weiterlernen ist der [Stanford Statistical Learning Kurs](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Aufgabe

[Modell erstellen](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Haftungsausschluss**:  
Dieses Dokument wurde mithilfe des KI-√úbersetzungsdienstes [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, bitten wir zu beachten, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache ist als ma√ügebliche Quelle zu betrachten. Bei wichtigen Informationen wird eine professionelle, menschliche √úbersetzung empfohlen. F√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Nutzung dieser √úbersetzung entstehen, √ºbernehmen wir keine Haftung.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->