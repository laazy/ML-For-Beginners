# Bygg en regresjonsmodell med Scikit-learn: regresjon p√• fire m√•ter

## Nybegynnermerknad

Line√¶r regresjon brukes n√•r vi √∏nsker √• predikere en **numerisk verdi** (for eksempel huspris, temperatur eller salg).
Den fungerer ved √• finne en rett linje som best representerer forholdet mellom inngangsfunksjoner og utgangen.

I denne leksjonen fokuserer vi p√• √• forst√• konseptet f√∏r vi utforsker mer avanserte regresjonsteknikker.
![Line√¶r vs polynom regresjon infografikk](../../../../translated_images/no/linear-polynomial.5523c7cb6576ccab.webp)
> Infografikk av [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Pre-forelesningsquiz](https://ff-quizzes.netlify.app/en/ml/)

> ### [Denne leksjonen er tilgjengelig i R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introduksjon 

S√• langt har du utforsket hva regresjon er med pr√∏veutvalg samlet fra gresskarprisdatasettet som vi skal bruke gjennom denne leksjonen. Du har ogs√• visualisert det med Matplotlib.

N√• er du klar til √• dykke dypere inn i regresjon for ML. Mens visualisering lar deg forst√• data, kommer den virkelige kraften i Maskinl√¶ring fra _trening av modeller_. Modeller trenes p√• historiske data for automatisk √• fange opp dataavhengigheter, og de tillater deg √• forutsi utfall for nye data, som modellen ikke har sett f√∏r.

I denne leksjonen vil du l√¶re mer om to typer regresjon: _grunnleggende line√¶r regresjon_ og _polynomisk regresjon_, sammen med noe av matematikken som ligger til grunn for disse teknikkene. Disse modellene vil tillate oss √• forutsi gresskarpriser basert p√• forskjellige inngangsdata. 

[![ML for beginners - Understanding Linear Regression](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML for beginners - Understanding Linear Regression")

> üé• Klikk p√• bildet ovenfor for en kort videooversikt over line√¶r regresjon.

> Gjennom dette pensumet forutsetter vi minimal kunnskap i matematikk, og s√∏ker √• gj√∏re det tilgjengelig for studenter fra andre fagfelt, s√• hold √∏ye med notater, üßÆ markeringer, diagrammer og andre l√¶ringsverkt√∏y for √• hjelpe forst√•elsen.

### Forutsetninger

Du b√∏r n√• v√¶re kjent med strukturen p√• gresskardataene som vi unders√∏ker. Du kan finne det forh√•ndslastet og forh√•ndsrenset i denne leksjonens _notebook.ipynb_-fil. I filen vises gresskarpris per bushel i en ny dataramme. S√∏rg for at du kan kj√∏re disse notatb√∏kene i kjerne i Visual Studio Code.

### Forberedelse

Som en p√•minnelse laster du inn disse dataene for √• stille sp√∏rsm√•l til dem. 

- N√•r er det beste tidspunktet √• kj√∏pe gresskar?
- Hvilken pris kan jeg forvente p√• en kasse med miniatyrgresskar?
- B√∏r jeg kj√∏pe dem i halvbukettkurver eller i 1 1/9 bushel-boksen?
La oss fortsette √• grave i disse dataene.

I forrige leksjon opprettet du en Pandas-dataramme og fylte den med deler av det originale datasettet, og standardiserte prisene per bushel. Ved √• gj√∏re det, klarte du imidlertid bare √• samle inn omtrent 400 datapunkter og kun for h√∏stm√•nedene. 

Ta en titt p√• dataene vi forh√•ndslastet i denne leksjonens medf√∏lgende notatbok. Dataene er forh√•ndslastet, og en innledende spredningsdiagram er tegnet for √• vise m√•nedsdata. Kanskje vi kan f√• litt mer detalj om datanaturen ved √• rense den mer.

## En line√¶r regresjonslinje

Som du l√¶rte i leksjon 1, er m√•let med en line√¶r regresjons√∏velse √• kunne plotte en linje for √•:

- **Vis variable relasjoner**. Vis forholdet mellom variabler
- **Gj√∏re prediksjoner**. Gj√∏r n√∏yaktige prediksjoner om hvor et nytt datapunkt vil falle i forhold til den linjen. 
 
Det er vanlig med **minste kvadraters regresjon** √• tegne denne typen linje. Begrepet "minste kvadraters" refererer til prosessen med √• minimere den totale feilen i modellen v√•r. For hvert datapunkt m√•ler vi den vertikale avstanden (kalt et residual) mellom det faktiske punktet og regresjonslinjen v√•r.  

Vi kvadrerer disse avstandene av to hovedgrunner:  

1. **St√∏rrelse over retning:** Vi √∏nsker √• behandle en feil p√• -5 det samme som en feil p√• +5. Kvadrering gj√∏r alle verdier positive.  

2. **Straff for uteliggere:** Kvadrering gir mer vekt til st√∏rre feil, og tvinger linjen til √• holde seg n√¶rmere punkter som ligger langt unna.  

Deretter legger vi sammen alle disse kvadrerte verdiene. M√•let v√•rt er √• finne den spesifikke linjen hvor denne endelige summen er p√• sitt laveste (den minste mulige verdien) ‚Äî derav navnet "minste kvadraters".  

> **üßÆ Vis meg matematikken** 
> 
> Denne linjen, kalt _best fit-linjen_ kan uttrykkes ved [en ligning](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` er den 'forklarende variabelen'. `Y` er den 'avhengige variabelen'. Stigningstallet til linjen er `b` og `a` er y-avskj√¶ringen, som refererer til verdien av `Y` n√•r `X = 0`. 
>
>![beregn stigningen](../../../../translated_images/no/slope.f3c9d5910ddbfcf9.webp)
>
> F√∏rst beregnes stigningen `b`. Infografikk av [Jen Looper](https://twitter.com/jenlooper)
>
> Med andre ord, og med henvisning til v√•rt gresskardatas opprinnelige sp√∏rsm√•l: "forutsi prisen p√• et gresskar per bushel etter m√•ned", vil `X` referere til prisen og `Y` til salgsm√•neden. 
>
>![fullf√∏r ligningen](../../../../translated_images/no/calculation.a209813050a1ddb1.webp)
>
> Beregn verdien av Y. Hvis du betaler rundt 4 dollar, m√• det v√¶re april! Infografikk av [Jen Looper](https://twitter.com/jenlooper)
>
> Matematikk som beregner linjen m√• demonstrere linjens stigning, som ogs√• avhenger av avskj√¶ringen, eller hvor `Y` er plassert n√•r `X = 0`.
>
> Du kan observere metoden for beregning av disse verdiene p√• [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) nettstedet. Bes√∏k ogs√• [denne minste kvadraters kalkulator](https://www.mathsisfun.com/data/least-squares-calculator.html) for √• se hvordan tallverdiene p√•virker linjen.

## Korrelasjon

Et annet begrep √• forst√• er **korrelasjonskoeffisienten** mellom gitte X og Y variabler. Ved √• bruke et spredningsdiagram kan du raskt visualisere denne koeffisienten. Et plott med datapunkter fordelt i en pen linje har h√∏y korrelasjon, men et plott med datapunkter spredt overalt mellom X og Y har lav korrelasjon.

En god line√¶r regresjonsmodell vil v√¶re en som har h√∏y (n√¶rmere 1 enn 0) korrelasjonskoeffisient ved bruk av minste kvadraters regresjonsmetode med en regresjonslinje.

‚úÖ Kj√∏r notatboken som f√∏lger med denne leksjonen og se p√• spredningsplottet for M√•ned mot Pris. Ser dataene som kobler M√•ned til Pris for gresskarsalg ut til √• ha h√∏y eller lav korrelasjon, i henhold til din visuelle tolkning av spredningsplottet? Endres det hvis du bruker et mer detaljert m√•l i stedet for `Month`, f.eks. *dag i √•ret* (dvs. antall dager siden begynnelsen av √•ret)?

I koden nedenfor antar vi at vi har renset dataene, og oppn√•dd en dataramme kalt `new_pumpkins`, lik f√∏lgende:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> Koden for √• rense dataene er tilgjengelig i [`notebook.ipynb`](notebook.ipynb). Vi har utf√∏rt de samme renseprosedyrene som i forrige leksjon, og har beregnet kolonnen `DayOfYear` ved hjelp av f√∏lgende uttrykk: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

N√• som du har forst√•else for matematikken bak line√¶r regresjon, la oss lage en regresjonsmodell for √• se om vi kan forutsi hvilken pakke med gresskar som vil ha de beste gresskarprisene. Noen som kj√∏per gresskar til en h√∏stgresskarhage kan √∏nske denne informasjonen for √• kunne optimalisere sine kj√∏p av gresskarpakker til hagen.

## Ser etter korrelasjon

[![ML for beginners - Looking for Correlation: The Key to Linear Regression](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML for beginners - Looking for Correlation: The Key to Linear Regression")

> üé• Klikk p√• bildet ovenfor for en kort videooversikt over korrelasjon.

Fra forrige leksjon har du sannsynligvis sett at gjennomsnittsprisen for ulike m√•neder ser slik ut:

<img alt="Gjennomsnittspris per m√•ned" src="../../../../translated_images/no/barchart.a833ea9194346d76.webp" width="50%"/>

Dette antyder at det b√∏r v√¶re en viss korrelasjon, og vi kan pr√∏ve √• trene en line√¶r regresjonsmodell for √• predikere forholdet mellom `Month` og `Price`, eller mellom `DayOfYear` og `Price`. Her er spredningsplottet som viser det sistnevnte forholdet:

<img alt="Spredningsplott av Pris vs. Dag i √•ret" src="../../../../translated_images/no/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" /> 

La oss se om det er en korrelasjon ved √• bruke `corr`-funksjonen:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Det ser ut som korrelasjonen er ganske liten, -0.15 basert p√• `Month` og -0.17 basert p√• `DayOfYear`, men det kan v√¶re et annet viktig forhold. Det ser ut til √• v√¶re forskjellige klynger av priser som tilsvarer forskjellige gresskarsorter. For √• bekrefte denne hypotesen, la oss tegne hver gresskarkategori med en annen farge. Ved √• sende inn en `ax`-parameter til `scatter`-plottfunksjonen kan vi tegne alle punktene p√• samme graf:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Spredningsplott av Pris vs. Dag i √•ret" src="../../../../translated_images/no/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" /> 

V√•r unders√∏kelse antyder at sort har st√∏rre effekt p√• den totale prisen enn den faktiske salgsdatoen. Vi kan se dette med et stolpediagram:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Stolpediagram av pris vs sort" src="../../../../translated_images/no/price-by-variety.744a2f9925d9bcb4.webp" width="50%" /> 

La oss fokusere for √∏yeblikket bare p√• √©n gresskarsort, 'pie type', og se hvilken effekt datoen har p√• prisen:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Spredningsplott av Pris vs. Dag i √•ret" src="../../../../translated_images/no/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" /> 

Hvis vi n√• beregner korrelasjonen mellom `Price` og `DayOfYear` ved bruk av `corr`-funksjonen, f√•r vi noe som `-0.27` ‚Äì noe som betyr at det gir mening √• trene en prediksjonsmodell.

> F√∏r vi trener en line√¶r regresjonsmodell, er det viktig √• s√∏rge for at dataene v√•re er rene. Line√¶r regresjon fungerer ikke godt med manglende verdier, s√• det gir mening √• kvitte seg med alle tomme celler:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

En annen tiln√¶rming kan v√¶re √• fylle de tomme verdiene med gjennomsnittsverdier fra den tilsvarende kolonnen.

## Enkel line√¶r regresjon

[![ML for beginners - Linear and Polynomial Regression using Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML for beginners - Linear and Polynomial Regression using Scikit-learn")

> üé• Klikk p√• bildet ovenfor for en kort videooversikt over line√¶r og polynomisk regresjon.

For √• trene v√•r line√¶re regresjonsmodell, vil vi bruke **Scikit-learn**-biblioteket.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Vi starter med √• separere inngangsverdier (egenskaper) og forventet utgang (etikett) i separate numpy-arrays:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Merk at vi m√•tte utf√∏re `reshape` p√• inngangsdataene for at Linear Regression-pakken skulle forst√• det korrekt. Line√¶r regresjon forventer et 2D-array som input, der hver rad av arrayet tilsvarer en vektor med inngangsegenskaper. I v√•rt tilfelle, siden vi bare har √©n input, trenger vi et array med formen N&times;1, der N er datast√∏rrelsen.

Deretter m√• vi splitte dataene i trenings- og testdatasett, slik at vi kan validere modellen v√•r etter trening:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Til slutt tar trening av den faktiske line√¶re regresjonsmodellen bare to kodelinjer. Vi definerer `LinearRegression`-objektet, og tilpasser det til dataene v√•re med `fit`-metoden:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Objektet `LinearRegression` etter at det er blitt `fit`-tet inneholder alle koeffisientene til regresjonen, som kan aksesseres ved hjelp av `.coef_`-egenskapen. I v√•rt tilfelle er det bare √©n koeffisient, som b√∏r v√¶re rundt `-0.017`. Det betyr at prisene ser ut til √• synke litt med tiden, men ikke s√• mye, omtrent 2 cent per dag. Vi kan ogs√• aksessere skj√¶ringspunktet til regresjonen med Y-aksen ved √• bruke `lin_reg.intercept_` - det vil v√¶re rundt `21` i v√•rt tilfelle, noe som indikerer prisen ved begynnelsen av √•ret.

For √• se hvor n√∏yaktig modellen v√•r er, kan vi predikere priser p√• et testdatasett, og deretter m√•le hvor n√¶r v√•re prediksjoner er til de forventede verdiene. Dette kan gj√∏res ved hjelp av gjennomsnittlig kvadratfeil (MSE) metrikken, som er gjennomsnittet av alle kvadrerte forskjeller mellom forventet og predikert verdi.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

Feilen v√•r ser ut til √• v√¶re rundt 2 poeng, som er ~17%. Ikke s√• bra. En annen indikator p√• modellkvalitet er **determinasjonskoeffisienten**, som kan oppn√•s p√• denne m√•ten:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
 Hvis verdien er 0, betyr det at modellen ikke tar hensyn til innndataene, og fungerer som den *verste line√¶re prediktoren*, som rett og slett er et gjennomsnitt av resultatet. Verdien 1 betyr at vi kan perfekt predikere alle forventede utganger. I v√•rt tilfelle er koeffisienten rundt 0.06, som er ganske lav.

Vi kan ogs√• plotte testdata sammen med regresjonslinjen for bedre √• se hvordan regresjon fungerer i v√•rt tilfelle:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linear regression" src="../../../../translated_images/no/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## Polynomregresjon

En annen type line√¶r regresjon er polynomregresjon. Selv om det noen ganger er et line√¶rt forhold mellom variabler ‚Äì jo st√∏rre gresskaret er i volum, desto h√∏yere er prisen ‚Äì kan disse forholdene noen ganger ikke plottes som et plan eller en rett linje.

‚úÖ Her er [noen flere eksempler](https://online.stat.psu.edu/stat501/lesson/9/9.8) p√• data som kan bruke polynomregresjon

Ta en ny titt p√• forholdet mellom dato og pris. Virker ikke dette scatterplottet som om det n√∏dvendigvis b√∏r analyseres med en rett linje? Kan ikke prisene svinge? I s√• fall kan du pr√∏ve polynomregresjon.

‚úÖ Polynomier er matematiske uttrykk som kan best√• av en eller flere variabler og koeffisienter

Polynomregresjon lager en buet linje for bedre √• tilpasse ikke-line√¶re data. I v√•rt tilfelle, hvis vi inkluderer en kvadratisk `DayOfYear`-variabel i inndataene, b√∏r vi kunne tilpasse v√•re data med en parabolsk kurve, som vil ha et minimum p√• et visst punkt i l√∏pet av √•ret.

Scikit-learn inkluderer en nyttig [pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) for √• kombinere forskjellige trinn i databehandling. En **pipeline** er en kjede av **estimatorer**. I v√•rt tilfelle vil vi lage en pipeline som f√∏rst legger til polynomfunksjoner til modellen v√•r, og deretter trener regresjonen:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

√Ö bruke `PolynomialFeatures(2)` betyr at vi inkluderer alle andregradspolynomer fra inndataene. I v√•rt tilfelle betyr det bare `DayOfYear`<sup>2</sup>, men gitt to inngangsvariabler X og Y, vil dette legge til X<sup>2</sup>, XY og Y<sup>2</sup>. Vi kan ogs√• bruke polynomer med h√∏yere grad hvis vi √∏nsker det.

Pipelines kan brukes p√• samme m√•te som det originale `LinearRegression`-objektet, dvs. vi kan `fit`-te pipelinen, og deretter bruke `predict` for √• f√• prediksjonsresultater. Her er grafen som viser testdataene og tiln√¶rmingskurven:

<img alt="Polynomial regression" src="../../../../translated_images/no/poly-results.ee587348f0f1f60b.webp" width="50%" />

Ved √• bruke polynomregresjon kan vi f√• litt lavere MSE og h√∏yere determinering, men ikke signifikant. Vi m√• ta hensyn til andre funksjoner!

> Du kan se at de laveste gresskarprisene observeres et sted rundt Halloween. Hvordan kan du forklare dette?

üéÉ Gratulerer, du har nettopp laget en modell som kan hjelpe til med √• forutsi prisen p√• paigresskar. Du kan sannsynligvis gjenta samme prosedyre for alle gresskartyper, men det ville v√¶re tidkrevende. La oss n√• l√¶re hvordan vi tar hensyn til gresskarvariant i modellen v√•r!

## Kategoriske variabler

I en ideell verden √∏nsker vi √• kunne forutsi priser for forskjellige gresskarvarianter ved √• bruke den samme modellen. Men kolonnen `Variety` er litt annerledes enn kolonner som `Month`, fordi den inneholder ikke-numeriske verdier. Slike kolonner kalles **kategoriske**.

[![ML for beginners - Categorical Feature Predictions with Linear Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML for beginners - Categorical Feature Predictions with Linear Regression")

> üé• Klikk p√• bildet over for en kort videooversikt over bruk av kategoriske variabler.

Her kan du se hvordan gjennomsnittsprisen avhenger av variasjon:

<img alt="Average price by variety" src="../../../../translated_images/no/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

For √• ta varianter i betraktning, m√• vi f√∏rst konvertere det til numerisk form, eller **kode** det. Det finnes flere m√•ter vi kan gj√∏re det p√•:

* Enkel **numerisk koding** bygger en tabell med forskjellige varianter, og erstatter s√• variantnavnet med en indeks i tabellen. Dette er ikke den beste id√©en for line√¶r regresjon, fordi line√¶r regresjon tar den faktiske numeriske verdien av indeksen, og legger det til resultatet multiplisert med en koeffisient. I v√•rt tilfelle er forholdet mellom indeksnummer og pris klart ikke-line√¶rt, selv om vi s√∏rger for at indeksene er ordnet p√• en bestemt m√•te.
* **One-hot-koding** erstatter kolonnen `Variety` med 4 forskjellige kolonner, √©n for hver variant. Hver kolonne inneholder `1` hvis raden tilsvarer den aktuelle varianten, og `0` ellers. Det betyr at det vil v√¶re fire koeffisienter i den line√¶re regresjonen, √©n for hver gresskarvariant, ansvarlig for "startpris" (eller heller "tillegg" pris) for den aktuelle varianten.

Koden nedenfor viser hvordan vi kan en-til-en kode en variant:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

For √• trene line√¶r regresjon med one-hot kodet variant som input, m√• vi bare initialisere `X` og `y` data korrekt:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Resten av koden er den samme som vi brukte ovenfor for √• trene line√¶r regresjon. Hvis du pr√∏ver det, vil du se at gjennomsnittlig kvadratfeil er omtrent den samme, men vi f√•r mye h√∏yere determinering (~77%). For √• f√• enda mer n√∏yaktige prediksjoner kan vi ta med flere kategoriske variabler, samt numeriske funksjoner, som `Month` eller `DayOfYear`. For √• f√• et stort array med funksjoner kan vi bruke `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Her tar vi ogs√• hensyn til `City` og `Package`-type, noe som gir oss MSE 2.84 (10%), og determinering 0.94!

## Sette det hele sammen

For √• lage den beste modellen kan vi bruke kombinert (one-hot kodet kategorisk + numeriske) data fra eksempelet ovenfor sammen med polynomregresjon. Her er den komplette koden for din bekvemmelighet:

```python
# sett opp treningsdata
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# lag trenings-test deling
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# sett opp og tren pipelinen
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# prediker resultater for testdata
pred = pipeline.predict(X_test)

# beregn MSE og determinering
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Dette b√∏r gi oss den beste determinering-koeffisienten p√• nesten 97%, og MSE=2.23 (~8% prediksjonsfeil).

| Modell | MSE | Determinering |
|--------|-----|---------------|
| `DayOfYear` Line√¶r | 2.77 (17.2%) | 0.07 |
| `DayOfYear` Polynom | 2.73 (17.0%) | 0.08 |
| `Variety` Line√¶r | 5.24 (19.7%) | 0.77 |
| Alle funksjoner Line√¶r | 2.84 (10.5%) | 0.94 |
| Alle funksjoner Polynom | 2.23 (8.25%) | 0.97 |

üèÜ Bra jobbet! Du laget fire regresjonsmodeller i √©n leksjon, og forbedret modellkvaliteten til 97%. I den siste delen om regresjon vil du l√¶re om logistisk regresjon for √• bestemme kategorier.

---
## üöÄUtfordring

Test flere forskjellige variable i denne notebooken for √• se hvordan korrelasjon samsvarer med modelln√∏yaktighet.

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ml/)

## Gjennomgang og selvstudie

I denne leksjonen l√¶rte vi om line√¶r regresjon. Det finnes andre viktige typer regresjon. Les om Stepwise, Ridge, Lasso og Elasticnet teknikker. Et godt kurs √• studere for √• l√¶re mer er [Stanford Statistical Learning-kurset](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## Oppgave

[Bygg en modell](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfraskrivelse**:
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, vennligst v√¶r oppmerksom p√• at automatiserte oversettelser kan inneholde feil eller un√∏yaktigheter. Det opprinnelige dokumentet p√• originalspr√•ket b√∏r betraktes som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r fra bruken av denne oversettelsen.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->