# Construir un modelo de regresi√≥n usando Scikit-learn: regresi√≥n de cuatro maneras

## Nota para principiantes

La regresi√≥n lineal se usa cuando queremos predecir un **valor num√©rico** (por ejemplo, precio de una casa, temperatura o ventas).
Funciona encontrando una l√≠nea recta que represente mejor la relaci√≥n entre las caracter√≠sticas de entrada y la salida.

En esta lecci√≥n, nos enfocamos en entender el concepto antes de explorar t√©cnicas de regresi√≥n m√°s avanzadas.
![Linear vs polynomial regression infographic](../../../../translated_images/es/linear-polynomial.5523c7cb6576ccab.webp)
> Infograf√≠a por [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Cuestionario previo a la lecci√≥n](https://ff-quizzes.netlify.app/en/ml/)

> ### [¬°Esta lecci√≥n est√° disponible en R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introducci√≥n

Hasta ahora has explorado qu√© es la regresi√≥n con datos de ejemplo obtenidos del conjunto de datos de precios de calabazas que usaremos a lo largo de esta lecci√≥n. Tambi√©n lo has visualizado usando Matplotlib.

Ahora est√°s listo para profundizar en regresi√≥n para ML. Mientras que la visualizaci√≥n te permite entender los datos, el verdadero poder del Aprendizaje Autom√°tico proviene de _entrenar modelos_. Los modelos se entrenan con datos hist√≥ricos para capturar autom√°ticamente las dependencias de los datos y permiten predecir resultados para datos nuevos, que el modelo no ha visto antes.

En esta lecci√≥n, aprender√°s m√°s sobre dos tipos de regresi√≥n: _regresi√≥n lineal b√°sica_ y _regresi√≥n polin√≥mica_, junto con algo de la matem√°tica que subyace a estas t√©cnicas. Estos modelos nos permitir√°n predecir los precios de las calabazas dependiendo de diferentes datos de entrada.

[![ML para principiantes - Entendiendo la regresi√≥n lineal](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML para principiantes - Entendiendo la regresi√≥n lineal")

> üé• Haz clic en la imagen arriba para un video corto con una visi√≥n general de la regresi√≥n lineal.

> A lo largo de este curr√≠culo, asumimos un conocimiento m√≠nimo de matem√°ticas y buscamos hacerlo accesible para estudiantes de otras √°reas, as√≠ que presta atenci√≥n a notas, üßÆ llamadas, diagramas y otras herramientas de aprendizaje que ayudan en la comprensi√≥n.

### Prerrequisitos

Ya deber√≠as estar familiarizado con la estructura de los datos de calabazas que estamos examinando. Puedes encontrarlos precargados y preprocesados en el archivo _notebook.ipynb_ de esta lecci√≥n. En el archivo, el precio de la calabaza est√° mostrado por bushel en un nuevo dataframe. Aseg√∫rate de poder ejecutar estos notebooks en kernels de Visual Studio Code.

### Preparaci√≥n

Como recordatorio, cargas estos datos para poder hacer preguntas sobre ellos.

- ¬øCu√°ndo es el mejor momento para comprar calabazas?
- ¬øQu√© precio puedo esperar para un paquete de calabazas peque√±as?
- ¬øDeber√≠a comprarlas en cestas de medio bushel o en cajas de 1 1/9 bushel?
Sigamos indagando en estos datos.

En la lecci√≥n anterior, creaste un dataframe de Pandas y lo llenaste con parte del conjunto de datos original, estandarizando el precio por bushel. Sin embargo, al hacer eso, solo pudiste obtener alrededor de 400 puntos de datos y solo para los meses de oto√±o.

Mira los datos que precargamos en el notebook que acompa√±a esta lecci√≥n. Los datos est√°n precargados y se ha graficado un diagrama de dispersi√≥n inicial para mostrar los datos del mes. Quiz√° podamos obtener un poco m√°s de detalle sobre la naturaleza de los datos limpi√°ndolos m√°s.

## Una l√≠nea de regresi√≥n lineal

Como aprendiste en la Lecci√≥n 1, el objetivo de un ejercicio de regresi√≥n lineal es poder trazar una l√≠nea para:

- **Mostrar relaciones entre variables**. Mostrar la relaci√≥n entre variables.
- **Hacer predicciones**. Hacer predicciones precisas sobre d√≥nde caer√° un nuevo punto de datos en relaci√≥n con esa l√≠nea.

Es t√≠pico de la **regresi√≥n por m√≠nimos cuadrados** dibujar este tipo de l√≠nea. El t√©rmino "m√≠nimos cuadrados" se refiere al proceso de minimizar el error total en nuestro modelo. Para cada punto de datos, medimos la distancia vertical (llamada residual) entre el punto real y nuestra l√≠nea de regresi√≥n.

Elevamos al cuadrado estas distancias por dos razones principales:

1. **Magnitud sobre direcci√≥n:** Queremos tratar un error de -5 igual que un error de +5. Al elevar al cuadrado todas las valores se vuelven positivos.

2. **Penalizaci√≥n de valores at√≠picos:** Elevar al cuadrado da m√°s peso a errores grandes, forzando a la l√≠nea a mantenerse m√°s cerca de puntos que est√°n lejos.

Luego sumamos todos estos valores al cuadrado. Nuestro objetivo es encontrar la l√≠nea espec√≠fica donde esta suma final sea la menor (el valor m√°s peque√±o posible), de ah√≠ el nombre "m√≠nimos cuadrados".

> **üßÆ Mu√©strame las matem√°ticas**
> 
> Esta l√≠nea, llamada _l√≠nea de mejor ajuste_, puede expresarse con [una ecuaci√≥n](https://en.wikipedia.org/wiki/Simple_linear_regression):
> 
> ```
> Y = a + bX
> ```
>
> `X` es la 'variable explicativa'. `Y` es la 'variable dependiente'. La pendiente de la l√≠nea es `b` y `a` es la intersecci√≥n en y, que se refiere al valor de `Y` cuando `X = 0`.
>
>![calcular la pendiente](../../../../translated_images/es/slope.f3c9d5910ddbfcf9.webp)
>
> Primero, calcula la pendiente `b`. Infograf√≠a por [Jen Looper](https://twitter.com/jenlooper)
>
> En otras palabras, y refiri√©ndonos a la pregunta original de nuestros datos de calabazas: "predecir el precio de una calabaza por bushel seg√∫n el mes", `X` se referir√≠a al precio y `Y` al mes de venta.
>
>![completar la ecuaci√≥n](../../../../translated_images/es/calculation.a209813050a1ddb1.webp)
>
> Calcula el valor de Y. Si est√°s pagando alrededor de $4, ¬°debe ser abril! Infograf√≠a por [Jen Looper](https://twitter.com/jenlooper)
>
> Las matem√°ticas que calculan la l√≠nea deben mostrar la pendiente de la l√≠nea, que tambi√©n depende de la intersecci√≥n, o d√≥nde se sit√∫a `Y` cuando `X = 0`.
>
> Puedes observar el m√©todo de c√°lculo para estos valores en el sitio web [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). Tambi√©n visita [este calculador de m√≠nimos cuadrados](https://www.mathsisfun.com/data/least-squares-calculator.html) para ver c√≥mo los valores de los n√∫meros impactan la l√≠nea.

## Correlaci√≥n

Un t√©rmino m√°s que hay que entender es el **Coeficiente de Correlaci√≥n** entre dos variables X y Y dadas. Usando un diagrama de dispersi√≥n, puedes visualizar r√°pidamente este coeficiente. Un gr√°fico con puntos de datos alineados en una l√≠nea ordenada tiene alta correlaci√≥n, pero un gr√°fico con puntos dispersos por todo el plano entre X y Y tiene baja correlaci√≥n.

Un buen modelo de regresi√≥n lineal ser√° aquel que tenga un alto coeficiente de correlaci√≥n (m√°s cercano a 1 que a 0) usando el m√©todo de regresi√≥n por m√≠nimos cuadrados con una l√≠nea de regresi√≥n.

‚úÖ Ejecuta el notebook que acompa√±a esta lecci√≥n y mira el diagrama de dispersi√≥n entre Mes y Precio. ¬øParece que hay una alta o baja correlaci√≥n entre Mes y Precio para las ventas de calabazas, seg√∫n tu interpretaci√≥n visual del diagrama? ¬øCambia si usas una medida m√°s detallada en vez de `Mes`, por ejemplo, *d√≠a del a√±o* (es decir, n√∫mero de d√≠as desde el inicio del a√±o)?

En el c√≥digo a continuaci√≥n, asumiremos que hemos limpiado los datos y obtenido un dataframe llamado `new_pumpkins`, similar al siguiente:

ID | Mes | D√≠aDelA√±o | Variedad | Ciudad | Paquete | Precio Bajo | Precio Alto | Precio
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | TIPO PARA PASTEL | BALTIMORE | Cartones de 1 1/9 bushel | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | TIPO PARA PASTEL | BALTIMORE | Cartones de 1 1/9 bushel | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | TIPO PARA PASTEL | BALTIMORE | Cartones de 1 1/9 bushel | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | TIPO PARA PASTEL | BALTIMORE | Cartones de 1 1/9 bushel | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | TIPO PARA PASTEL | BALTIMORE | Cartones de 1 1/9 bushel | 15.0 | 15.0 | 13.636364

> El c√≥digo para limpiar los datos est√° disponible en [`notebook.ipynb`](notebook.ipynb). Hemos realizado los mismos pasos de limpieza que en la lecci√≥n anterior, y hemos calculado la columna `DayOfYear` usando la siguiente expresi√≥n:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Ahora que tienes una comprensi√≥n de las matem√°ticas detr√°s de la regresi√≥n lineal, vamos a crear un modelo de Regresi√≥n para ver si podemos predecir qu√© paquete de calabazas tendr√° los mejores precios. Alguien que compre calabazas para un parche de calabazas en una fiesta podr√≠a querer esta informaci√≥n para optimizar sus compras de paquetes de calabazas para el parche.

## Buscando correlaci√≥n

[![ML para principiantes - Buscando correlaci√≥n: La clave para la regresi√≥n lineal](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML para principiantes - Buscando correlaci√≥n: La clave para la regresi√≥n lineal")

> üé• Haz clic en la imagen arriba para un video corto con una visi√≥n general de la correlaci√≥n.

Probablemente en la lecci√≥n anterior viste que el precio promedio por diferentes meses se ve as√≠:

<img alt="Precio promedio por mes" src="../../../../translated_images/es/barchart.a833ea9194346d76.webp" width="50%"/>

Esto sugiere que deber√≠a haber alguna correlaci√≥n, y podemos intentar entrenar un modelo de regresi√≥n lineal para predecir la relaci√≥n entre `Mes` y `Precio`, o entre `D√≠aDelA√±o` y `Precio`. Aqu√≠ est√° el gr√°fico de dispersi√≥n que muestra esta √∫ltima relaci√≥n:

<img alt="Diagrama de dispersi√≥n de Precio vs D√≠a del A√±o" src="../../../../translated_images/es/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" /> 

Vamos a ver si hay correlaci√≥n usando la funci√≥n `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Parece que la correlaci√≥n es bastante baja, -0.15 por `Mes` y -0.17 por el `D√≠aDelA√±o`, pero podr√≠a haber otra relaci√≥n importante. Parece que hay diferentes grupos de precios correspondientes a diferentes variedades de calabazas. Para confirmar esta hip√≥tesis, grafiquemos cada categor√≠a de calabaza usando colores diferentes. Pasando un par√°metro `ax` a la funci√≥n de gr√°fica `scatter`, podemos trazar todos los puntos en el mismo gr√°fico:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Diagrama de dispersi√≥n de Precio vs D√≠a del A√±o" src="../../../../translated_images/es/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" /> 

Nuestra investigaci√≥n sugiere que la variedad tiene m√°s efecto en el precio general que la fecha de venta real. Podemos ver esto con un gr√°fico de barras:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Gr√°fico de barras de precio vs variedad" src="../../../../translated_images/es/price-by-variety.744a2f9925d9bcb4.webp" width="50%" /> 

Vamos a enfocarnos por el momento solo en una variedad de calabaza, la 'tipo pastel', y ver qu√© efecto tiene la fecha sobre el precio:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Diagrama de dispersi√≥n de Precio vs D√≠a del A√±o" src="../../../../translated_images/es/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" /> 

Si ahora calculamos la correlaci√≥n entre `Precio` y `D√≠aDelA√±o` usando la funci√≥n `corr`, obtendremos algo como `-0.27` ‚Äì lo que significa que entrenar un modelo predictivo tiene sentido.

> Antes de entrenar un modelo de regresi√≥n lineal, es importante asegurarse de que nuestros datos est√©n limpios. La regresi√≥n lineal no funciona bien con valores faltantes, por lo que tiene sentido eliminar todas las celdas vac√≠as:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Otra opci√≥n ser√≠a llenar esos valores vac√≠os con los valores promedio de la columna correspondiente.

## Regresi√≥n lineal simple

[![ML para principiantes - Regresi√≥n lineal y polin√≥mica usando Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML para principiantes - Regresi√≥n lineal y polin√≥mica usando Scikit-learn")

> üé• Haz clic en la imagen arriba para un video corto con una visi√≥n general de la regresi√≥n lineal y polin√≥mica.

Para entrenar nuestro modelo de Regresi√≥n Lineal usaremos la biblioteca **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Comenzamos separando los valores de entrada (caracter√≠sticas) y la salida esperada (etiqueta) en arreglos numpy separados:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Ten en cuenta que tuvimos que realizar un `reshape` en los datos de entrada para que el paquete de Regresi√≥n Lineal lo entienda correctamente. Regresi√≥n Lineal espera un arreglo 2D como entrada, donde cada fila del arreglo corresponde a un vector de caracter√≠sticas de entrada. En nuestro caso, dado que tenemos solo una entrada, necesitamos un arreglo con forma N&times;1, donde N es el tama√±o del conjunto de datos.

Luego, necesitamos dividir los datos en conjuntos de entrenamiento y prueba, para poder validar nuestro modelo despu√©s del entrenamiento:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Finalmente, entrenar el modelo actual de Regresi√≥n Lineal toma solo dos l√≠neas de c√≥digo. Definimos el objeto `LinearRegression` y lo ajustamos a nuestros datos usando el m√©todo `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

El objeto `LinearRegression` despu√©s de ajustar (`fit`) contiene todos los coeficientes de la regresi√≥n, a los cuales se puede acceder mediante la propiedad `.coef_`. En nuestro caso, hay solo un coeficiente, que deber√≠a estar alrededor de `-0.017`. Esto significa que los precios parecen bajar un poco con el tiempo, pero no mucho, alrededor de 2 centavos por d√≠a. Tambi√©n podemos acceder al punto de intersecci√≥n de la regresi√≥n con el eje Y usando `lin_reg.intercept_` ‚Äî que estar√° alrededor de `21` en nuestro caso, indicando el precio al comienzo del a√±o.

Para ver qu√© tan preciso es nuestro modelo, podemos predecir precios en un conjunto de datos de prueba, y luego medir qu√© tan cercanas est√°n nuestras predicciones a los valores esperados. Esto se puede hacer usando la m√©trica del error cuadr√°tico medio (MSE), que es la media de todas las diferencias cuadradas entre los valores esperados y predichos.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

Nuestro error parece estar alrededor de 2 puntos, lo que es ~17%. No muy bueno. Otro indicador de la calidad del modelo es el **coeficiente de determinaci√≥n**, que se puede obtener as√≠:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```

Si el valor es 0, significa que el modelo no toma en cuenta los datos de entrada y act√∫a como el *peor predictor lineal*, que es simplemente el valor medio del resultado. El valor 1 significa que podemos predecir perfectamente todas las salidas esperadas. En nuestro caso, el coeficiente es alrededor de 0.06, que es bastante bajo.

Tambi√©n podemos graficar los datos de prueba junto con la l√≠nea de regresi√≥n para ver mejor c√≥mo funciona la regresi√≥n en nuestro caso:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linear regression" src="../../../../translated_images/es/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## Regresi√≥n Polin√≥mica

Otro tipo de Regresi√≥n Lineal es la Regresi√≥n Polin√≥mica. Aunque a veces hay una relaci√≥n lineal entre variables ‚Äî mientras m√°s grande el volumen de la calabaza, mayor el precio ‚Äî a veces estas relaciones no pueden representarse como un plano o l√≠nea recta.

‚úÖ Aqu√≠ hay [m√°s ejemplos](https://online.stat.psu.edu/stat501/lesson/9/9.8) de datos que podr√≠an utilizar Regresi√≥n Polin√≥mica

Observa nuevamente la relaci√≥n entre Fecha y Precio. ¬øParece que este diagrama de dispersi√≥n deba analizarse necesariamente con una l√≠nea recta? ¬øNo pueden fluctuar los precios? En este caso, puedes probar la regresi√≥n polin√≥mica.

‚úÖ Los polinomios son expresiones matem√°ticas que pueden consistir en una o m√°s variables y coeficientes

La regresi√≥n polin√≥mica crea una curva para ajustarse mejor a datos no lineales. En nuestro caso, si incluimos una variable cuadr√°tica `DayOfYear` en los datos de entrada, deber√≠amos poder ajustar nuestros datos con una curva parab√≥lica, que tendr√° un m√≠nimo en un cierto punto dentro del a√±o.

Scikit-learn incluye una √∫til [API de pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) para combinar diferentes pasos del procesamiento de datos juntos. Un **pipeline** es una cadena de **estimadores**. En nuestro caso, crearemos un pipeline que primero agrega caracter√≠sticas polin√≥micas al modelo y luego entrena la regresi√≥n:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Usar `PolynomialFeatures(2)` significa que incluiremos todos los polinomios de segundo grado a partir de los datos de entrada. En nuestro caso solo significar√° `DayOfYear`<sup>2</sup>, pero dado dos variables de entrada X y Y, esto agregar√° X<sup>2</sup>, XY y Y<sup>2</sup>. Tambi√©n podemos usar polinomios de grado superior si queremos.

Los pipelines pueden usarse de la misma manera que el objeto original `LinearRegression`, es decir, podemos ajustar (`fit`) el pipeline y luego usar `predict` para obtener los resultados de la predicci√≥n. Aqu√≠ est√° el gr√°fico que muestra los datos de prueba y la curva de aproximaci√≥n:

<img alt="Polynomial regression" src="../../../../translated_images/es/poly-results.ee587348f0f1f60b.webp" width="50%" />

Usando Regresi√≥n Polin√≥mica, podemos obtener un MSE ligeramente menor y un coeficiente de determinaci√≥n m√°s alto, pero no de forma significativa. ¬°Necesitamos tener en cuenta otras caracter√≠sticas!

> Puedes ver que los precios m√≠nimos de las calabazas se observan alrededor de Halloween. ¬øC√≥mo puedes explicar esto?

üéÉ ¬°Felicidades, acabas de crear un modelo que puede ayudar a predecir el precio de las calabazas para pastel! Probablemente puedas repetir el mismo procedimiento para todos los tipos de calabaza, pero eso ser√≠a tedioso. ¬°Ahora aprendamos c√≥mo tener en cuenta la variedad de calabaza en nuestro modelo!

## Caracter√≠sticas Categ√≥ricas

En un mundo ideal, queremos poder predecir precios para diferentes variedades de calabaza usando el mismo modelo. Sin embargo, la columna `Variety` es algo diferente de columnas como `Month`, porque contiene valores no num√©ricos. Estas columnas se llaman **categ√≥ricas**.

[![ML para principiantes - Predicciones con caracter√≠sticas categ√≥ricas usando Regresi√≥n Lineal](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML para principiantes - Predicciones con caracter√≠sticas categ√≥ricas usando Regresi√≥n Lineal")

> üé• Haz clic en la imagen arriba para un breve video sobre el uso de caracter√≠sticas categ√≥ricas.

Aqu√≠ puedes ver c√≥mo depende el precio promedio de la variedad:

<img alt="Average price by variety" src="../../../../translated_images/es/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

Para tener en cuenta la variedad, primero necesitamos convertirla a forma num√©rica, o **codificarla**. Hay varias formas de hacerlo:

* La simple **codificaci√≥n num√©rica** construir√° una tabla de diferentes variedades y luego reemplazar√° el nombre de la variedad por un √≠ndice en esa tabla. Esto no es lo mejor para regresi√≥n lineal, porque la regresi√≥n lineal toma el valor num√©rico real del √≠ndice y lo agrega al resultado, multiplicado por alg√∫n coeficiente. En nuestro caso, la relaci√≥n entre el n√∫mero de √≠ndice y el precio es claramente no lineal, incluso si nos aseguramos de que los √≠ndices est√©n ordenados de alguna forma espec√≠fica.
* La **codificaci√≥n one-hot** reemplazar√° la columna `Variety` por 4 columnas diferentes, una para cada variedad. Cada columna contendr√° `1` si la fila correspondiente es de esa variedad, y `0` de lo contrario. Esto significa que habr√° cuatro coeficientes en la regresi√≥n lineal, uno para cada variedad de calabaza, responsables del "precio inicial" (o m√°s bien "precio adicional") para esa variedad en particular.

El c√≥digo a continuaci√≥n muestra c√≥mo codificar one-hot una variedad:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Para entrenar la regresi√≥n lineal usando la variedad codificada one-hot como entrada, solo necesitamos inicializar datos `X` y `y` correctamente:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

El resto del c√≥digo es igual al que usamos antes para entrenar la Regresi√≥n Lineal. Si lo pruebas, ver√°s que el error cuadr√°tico medio es aproximadamente el mismo, pero obtenemos un coeficiente de determinaci√≥n mucho m√°s alto (~77%). Para obtener predicciones a√∫n m√°s precisas, podemos tener en cuenta m√°s caracter√≠sticas categ√≥ricas, as√≠ como caracter√≠sticas num√©ricas, como `Month` o `DayOfYear`. Para obtener un gran arreglo de caracter√≠sticas, podemos usar `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Aqu√≠ tambi√©n tomamos en cuenta `City` y tipo de `Package`, lo que nos da MSE de 2.84 (10%), y determinaci√≥n de 0.94!

## Integrando todo junto

Para crear el mejor modelo, podemos usar datos combinados (categ√≥ricos codificados one-hot + num√©ricos) del ejemplo anterior junto con Regresi√≥n Polin√≥mica. Aqu√≠ tienes el c√≥digo completo para tu comodidad:

```python
# configurar datos de entrenamiento
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# realizar la divisi√≥n de entrenamiento-prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# configurar y entrenar la tuber√≠a
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predecir resultados para datos de prueba
pred = pipeline.predict(X_test)

# calcular MSE y determinaci√≥n
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Esto deber√≠a darnos el mejor coeficiente de determinaci√≥n de casi 97%, y MSE=2.23 (~8% de error de predicci√≥n).

| Modelo | MSE | Determinaci√≥n |
|-------|-----|---------------|
| `DayOfYear` Lineal | 2.77 (17.2%) | 0.07 |
| `DayOfYear` Polin√≥mico | 2.73 (17.0%) | 0.08 |
| `Variety` Lineal | 5.24 (19.7%) | 0.77 |
| Todas las caracter√≠sticas Lineal | 2.84 (10.5%) | 0.94 |
| Todas las caracter√≠sticas Polin√≥mico | 2.23 (8.25%) | 0.97 |

üèÜ ¬°Bien hecho! Creaste cuatro modelos de Regresi√≥n en una lecci√≥n, y mejoraste la calidad del modelo a 97%. En la secci√≥n final sobre Regresi√≥n, aprender√°s sobre Regresi√≥n Log√≠stica para determinar categor√≠as.

---
## üöÄDesaf√≠o

Prueba varias variables diferentes en este cuaderno para ver c√≥mo la correlaci√≥n corresponde a la precisi√≥n del modelo.

## [Cuestionario posterior a la clase](https://ff-quizzes.netlify.app/en/ml/)

## Repaso y Autoestudio

En esta lecci√≥n aprendimos sobre Regresi√≥n Lineal. Existen otros tipos importantes de Regresi√≥n. Lee sobre las t√©cnicas Stepwise, Ridge, Lasso y Elasticnet. Un buen curso para profundizar es el [curso de Aprendizaje Estad√≠stico de Stanford](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## Tarea

[Construir un Modelo](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Aviso Legal**:
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por la precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda la traducci√≥n profesional realizada por humanos. No nos responsabilizamos por malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->