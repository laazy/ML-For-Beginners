# VytvoÅ™enÃ­ regresnÃ­ho modelu pomocÃ­ Scikit-learn: regrese ÄtyÅ™mi zpÅ¯soby

## PoznÃ¡mka pro zaÄÃ¡teÄnÃ­ky

LineÃ¡rnÃ­ regrese se pouÅ¾Ã­vÃ¡, kdyÅ¾ chceme pÅ™edpovÄ›dÄ›t **ÄÃ­selnou hodnotu** (napÅ™Ã­klad cenu domu, teplotu nebo prodeje). Funguje tak, Å¾e najde pÅ™Ã­mku, kterÃ¡ nejlÃ©pe reprezentuje vztah mezi vstupnÃ­mi rysy a vÃ½stupem.

V tÃ©to lekci se zamÄ›Å™Ã­me na pochopenÃ­ konceptu pÅ™ed tÃ­m, neÅ¾ prozkoumÃ¡me pokroÄilejÅ¡Ã­ regresnÃ­ techniky.
![Infografika lineÃ¡rnÃ­ vs. polynomiÃ¡lnÃ­ regrese](../../../../translated_images/cs/linear-polynomial.5523c7cb6576ccab.webp)
> Infografika od [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [PÅ™ednÃ¡Å¡kovÃ½ kvÃ­z](https://ff-quizzes.netlify.app/en/ml/)

> ### [Tato lekce je dostupnÃ¡ takÃ© v R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Ãšvod

Dosud jste prozkoumali, co je regrese, na ukÃ¡zkovÃ½ch datech ze sady dat o cenÃ¡ch dÃ½nÃ­, kterou budeme pouÅ¾Ã­vat v celÃ© tÃ©to lekci. TakÃ© jste si ji vizualizovali pomocÃ­ Matplotlib.

NynÃ­ jste pÅ™ipraveni ponoÅ™it se hloubÄ›ji do regrese pro strojovÃ© uÄenÃ­. ZatÃ­mco vizualizace vÃ¡m umoÅ¾nÃ­ porozumÄ›t datÅ¯m, skuteÄnÃ¡ sÃ­la strojovÃ©ho uÄenÃ­ spoÄÃ­vÃ¡ v _trÃ©novÃ¡nÃ­ modelÅ¯_. Modely jsou trÃ©novÃ¡ny na historickÃ½ch datech, aby automaticky zachytily zÃ¡vislosti v datech, a umoÅ¾ÅˆujÃ­ vÃ¡m pÅ™edpovÃ­dat vÃ½sledky pro novÃ¡ data, kterÃ¡ model dosud nevidÄ›l.

V tÃ©to lekci se nauÄÃ­te vÃ­ce o dvou typech regrese: _zÃ¡kladnÃ­ lineÃ¡rnÃ­ regresi_ a _polynomiÃ¡lnÃ­ regresi_, spoleÄnÄ› s nÄ›kterou z matematiky, kterÃ¡ stojÃ­ za tÄ›mito technikami. Tyto modely nÃ¡m umoÅ¾nÃ­ pÅ™edpovÃ­dat ceny dÃ½nÃ­ v zÃ¡vislosti na rÅ¯znÃ½ch vstupnÃ­ch datech. 

[![StrojovÃ© uÄenÃ­ pro zaÄÃ¡teÄnÃ­ky â€“ PochopenÃ­ lineÃ¡rnÃ­ regrese](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "StrojovÃ© uÄenÃ­ pro zaÄÃ¡teÄnÃ­ky â€“ PochopenÃ­ lineÃ¡rnÃ­ regrese")

> ğŸ¥ KliknÄ›te na obrÃ¡zek vÃ½Å¡e pro krÃ¡tkÃ© video pÅ™ehled lineÃ¡rnÃ­ regrese.

> V celÃ©m tomto kurzu pÅ™edpoklÃ¡dÃ¡me minimÃ¡lnÃ­ znalost matematiky a snaÅ¾Ã­me se ji zpÅ™Ã­stupnit studentÅ¯m z jinÃ½ch oborÅ¯, proto sledujte poznÃ¡mky, ğŸ§® upozornÄ›nÃ­, diagramy a dalÅ¡Ã­ uÄebnÃ­ pomÅ¯cky pro lepÅ¡Ã­ porozumÄ›nÃ­.

### PÅ™edpoklady

MÄ›li byste uÅ¾ bÃ½t obeznÃ¡meni se strukturou dat o dÃ½nÃ­ch, kterÃ¡ zkoumÃ¡me. Najdete je pÅ™ednaÄtenÃ¡ a pÅ™edvyÄiÅ¡tÄ›nÃ¡ v souboru _notebook.ipynb_ tÃ©to lekce. V souboru je cena dÃ½nÃ­ zobrazena za bushel v novÃ©m datovÃ©m rÃ¡mci. UjistÄ›te se, Å¾e mÅ¯Å¾ete spouÅ¡tÄ›t tyto notebooky v kernelu ve Visual Studio Code.

### PÅ™Ã­prava

Pro pÅ™ipomenutÃ­, tato data naÄÃ­tÃ¡te proto, abyste si na nÄ› mohli klÃ¡st otÃ¡zky.

- Kdy je nejlepÅ¡Ã­ Äas koupit dÃ½nÄ›? 
- Jakou cenu mohu oÄekÃ¡vat za balenÃ­ mini dÃ½nÃ­?
- MÄ›l bych je kupovat v polovinÄ› bushelovÃ©ho koÅ¡e, nebo v 1 1/9 bushelovÃ© krabici?
PojÄme v tomto zkoumÃ¡nÃ­ dat pokraÄovat.

V pÅ™edchozÃ­ lekci jste vytvoÅ™ili Pandas datovÃ½ rÃ¡mec a naplnili jej ÄÃ¡stÃ­ pÅ¯vodnÃ­ch dat, pÅ™iÄemÅ¾ jste ceny standardizovali za bushel. TÃ­m jste vÅ¡ak zÃ­skali pouze asi 400 datovÃ½ch bodÅ¯ a jen pro podzimnÃ­ mÄ›sÃ­ce.

PodÃ­vejte se na data, kterÃ¡ jsme pÅ™ednaÄetli v pÅ™idruÅ¾enÃ©m notebooku tÃ©to lekce. Data jsou pÅ™ednaÄtenÃ¡ a je vykreslen prvotnÃ­ scatterplot zobrazujÃ­cÃ­ data podle mÄ›sÃ­cÅ¯. MoÅ¾nÃ¡ mÅ¯Å¾eme zÃ­skat podrobnÄ›jÅ¡Ã­ informace o povaze dat jejich dalÅ¡Ã­m ÄiÅ¡tÄ›nÃ­m.

## LineÃ¡rnÃ­ regresnÃ­ pÅ™Ã­mka

Jak jste se nauÄili v Lekci 1, cÃ­lem lineÃ¡rnÃ­ regrese je bÃ½t schopen vykreslit pÅ™Ã­mku, kterÃ¡:

- **UkÃ¡Å¾e vztahy promÄ›nnÃ½ch**. UkÃ¡Å¾e vztah mezi promÄ›nnÃ½mi
- **UmoÅ¾nÃ­ pÅ™edpovÄ›di**. UmoÅ¾nÃ­ pÅ™esnÄ› pÅ™edpovÄ›dÄ›t, kde by se novÃ½ datovÃ½ bod mohl nachÃ¡zet vzhledem k tÃ©to pÅ™Ã­mce.
 
Je typickÃ© pro **metodu nejmenÅ¡Ã­ch ÄtvercÅ¯**, Å¾e se takovÃ¡to pÅ™Ã­mka kreslÃ­. TermÃ­n "nejmenÅ¡Ã­ Ätverce" odkazuje na proces minimalizace celkovÃ© chyby v naÅ¡em modelu. Pro kaÅ¾dÃ½ datovÃ½ bod mÄ›Å™Ã­me vertikÃ¡lnÃ­ vzdÃ¡lenost (nazÃ½vanou reziduÃ¡l) mezi skuteÄnÃ½m bodem a naÅ¡Ã­ regresnÃ­ pÅ™Ã­mkou.  

Tyto vzdÃ¡lenosti umocÅˆujeme na druhou ze dvou hlavnÃ­ch dÅ¯vodÅ¯:  

1. **Velikost nad smÄ›rem:** Chceme, aby chyba -5 byla stejnÄ› vÃ¡Å¾nÃ¡ jako chyba +5. UmocnÄ›nÃ­ na druhou zaruÄÃ­, Å¾e vÅ¡echny hodnoty jsou kladnÃ©.  

2. **TrestÃ¡nÃ­ odlehlÃ½ch hodnot:** UmocnÄ›nÃ­ na druhou dÃ¡vÃ¡ vÄ›tÅ¡Ã­ vÃ¡hu vÄ›tÅ¡Ã­m chybÃ¡m, coÅ¾ nutÃ­ pÅ™Ã­mku bÃ½t blÃ­Å¾e k bodÅ¯m, kterÃ© jsou daleko.  

PotÃ© vÅ¡echny tyto umocnÄ›nÃ© hodnoty seÄteme. NaÅ¡Ã­m cÃ­lem je najÃ­t specifickou pÅ™Ã­mku, kde je tenhle souÄet nejmenÅ¡Ã­ (nejmenÅ¡Ã­ moÅ¾nÃ¡ hodnota) - odtud nÃ¡zev "nejmenÅ¡Ã­ Ätverce".  

> **ğŸ§® UkÃ¡zat matematiku** 
> 
> Tato pÅ™Ã­mka, nazÃ½vanÃ¡ _pÅ™Ã­mka nejlepÅ¡Ã­ho pÅ™izpÅ¯sobenÃ­_, mÅ¯Å¾e bÃ½t vyjÃ¡dÅ™ena pomocÃ­ [rovnice](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` je 'vysvÄ›tlujÃ­cÃ­ promÄ›nnÃ¡'. `Y` je 'zÃ¡vislÃ¡ promÄ›nnÃ¡'. SmÄ›rnice pÅ™Ã­mky je `b` a `a` je prÅ¯seÄÃ­k s osou y, coÅ¾ je hodnota `Y`, kdyÅ¾ `X = 0`. 
>
>![vÃ½poÄet smÄ›rnice](../../../../translated_images/cs/slope.f3c9d5910ddbfcf9.webp)
>
> Nejprve vypoÄÃ­tejte smÄ›rnici `b`. Infografika od [Jen Looper](https://twitter.com/jenlooper)
>
> JinÃ½mi slovy, a s odkazem na naÅ¡i pÅ¯vodnÃ­ otÃ¡zku ohlednÄ› dat o dÃ½nÃ­ch: "pÅ™edpovÄ›Ä ceny dÃ½nÄ› za bushel podle mÄ›sÃ­ce", `X` by odkazovalo na mÄ›sÃ­c a `Y` by odkazovalo na cenu.
>
>![dokonÄenÃ­ rovnice](../../../../translated_images/cs/calculation.a209813050a1ddb1.webp)
>
> VypoÄÃ­tejte hodnotu Y. Pokud platÃ­te kolem 4 dolarÅ¯, musÃ­ to bÃ½t duben! Infografika od [Jen Looper](https://twitter.com/jenlooper)
>
> Matematika poÄÃ­tajÃ­cÃ­ pÅ™Ã­mku musÃ­ demonstrovat smÄ›rnici pÅ™Ã­mky, kterÃ¡ takÃ© zÃ¡visÃ­ na prÅ¯seÄÃ­ku, tedy kde je `Y` situovÃ¡no, kdyÅ¾ `X = 0`.
>
> Metodu vÃ½poÄtu tÄ›chto hodnot mÅ¯Å¾ete vidÄ›t na webu [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). NavÅ¡tivte takÃ© [tuto kalkulaÄku metody nejmenÅ¡Ã­ch ÄtvercÅ¯](https://www.mathsisfun.com/data/least-squares-calculator.html) a sledujte, jak hodnoty ÄÃ­sel ovlivÅˆujÃ­ pÅ™Ã­mku.

## Korelace

JeÅ¡tÄ› jeden termÃ­n k pochopenÃ­ je **koeficient korelace** mezi danÃ½mi promÄ›nnÃ½mi X a Y. PomocÃ­ scatterplotu mÅ¯Å¾ete tento koeficient rychle vizualizovat. Graf s body rozprostÅ™enÃ½mi po ÃºhlednÃ© pÅ™Ã­mce mÃ¡ vysokou korelaci, ale graf s body rozptÃ½lenÃ½mi vÅ¡ude po ose X a Y mÃ¡ korelaci nÃ­zkou.

DobrÃ½ model lineÃ¡rnÃ­ regrese bude ten, kterÃ½ mÃ¡ vysokÃ½ (blÃ­Å¾e k 1 neÅ¾ k 0) koeficient korelace pouÅ¾Ã­vajÃ­cÃ­ metodu nejmenÅ¡Ã­ch ÄtvercÅ¯ s regresnÃ­ pÅ™Ã­mkou.

âœ… SpusÅ¥te si notebook pÅ™idruÅ¾enÃ½ k tÃ©to lekci a podÃ­vejte se na scatterplot spojenÃ­ MÄ›sÃ­c vs. Cena. ZdÃ¡ se podle vaÅ¡eho vizuÃ¡lnÃ­ho hodnocenÃ­ scatterplotu, Å¾e data spojujÃ­cÃ­ mÄ›sÃ­c s cenou pro prodej dÃ½nÃ­ majÃ­ vysokou nebo nÃ­zkou korelaci? ZmÄ›nÃ­ se to, kdyÅ¾ pouÅ¾ijete jemnÄ›jÅ¡Ã­ mÄ›Å™Ã­tko namÃ­sto `Month`, napÅ™. *den v roce* (tedy poÄet dnÃ­ od zaÄÃ¡tku roku)?

V nÃ­Å¾e uvedenÃ©m kÃ³du pÅ™edpoklÃ¡dÃ¡me, Å¾e jsme data vyÄistili a zÃ­skali datovÃ½ rÃ¡mec nazvanÃ½ `new_pumpkins`, podobnÃ½ nÃ¡sledujÃ­cÃ­mu:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> KÃ³d pro vyÄiÅ¡tÄ›nÃ­ dat je dostupnÃ½ v [`notebook.ipynb`](notebook.ipynb). Provedli jsme stejnÃ© kroky ÄiÅ¡tÄ›nÃ­ jako v pÅ™edchozÃ­ lekci a dopoÄÃ­tali sloupec `DayOfYear` pomocÃ­ nÃ¡sledujÃ­cÃ­ho vÃ½razu: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

NynÃ­, kdyÅ¾ mÃ¡te pochopenÃ­ matematiky stojÃ­cÃ­ za lineÃ¡rnÃ­ regresÃ­, vytvoÅ™Ã­me regresnÃ­ model, abychom zjistili, zda mÅ¯Å¾eme pÅ™edpovÄ›dÄ›t, kterÃ© balenÃ­ dÃ½nÃ­ bude mÃ­t nejlepÅ¡Ã­ ceny. NÄ›kdo, kdo kupuje dÃ½nÄ› pro svÃ¡teÄnÃ­ dÃ½Åˆovou zahradu, by mohl chtÃ­t tyto informace, aby mohl optimalizovat nÃ¡kupy balenÃ­ dÃ½nÃ­ pro svou zahradu.

## HledÃ¡nÃ­ korelace

[![StrojovÃ© uÄenÃ­ pro zaÄÃ¡teÄnÃ­ky â€“ HledÃ¡nÃ­ korelace: KlÃ­Ä k lineÃ¡rnÃ­ regresi](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "StrojovÃ© uÄenÃ­ pro zaÄÃ¡teÄnÃ­ky â€“ HledÃ¡nÃ­ korelace: KlÃ­Ä k lineÃ¡rnÃ­ regresi")

> ğŸ¥ KliknÄ›te na obrÃ¡zek vÃ½Å¡e pro krÃ¡tkÃ© video pÅ™ehled korelace.

Z pÅ™edchozÃ­ lekce jste pravdÄ›podobnÄ› vidÄ›li, Å¾e prÅ¯mÄ›rnÃ¡ cena za rÅ¯znÃ© mÄ›sÃ­ce vypadÃ¡ takto:

<img alt="PrÅ¯mÄ›rnÃ¡ cena podle mÄ›sÃ­ce" src="../../../../translated_images/cs/barchart.a833ea9194346d76.webp" width="50%"/>

To naznaÄuje, Å¾e by mÄ›la bÃ½t nÄ›jakÃ¡ korelace, a mÅ¯Å¾eme vyzkouÅ¡et natrÃ©novat lineÃ¡rnÃ­ regresnÃ­ model k pÅ™edpovÄ›di vztahu mezi `Month` a `Price`, nebo mezi `DayOfYear` a `Price`. Zde je scatter plot, kterÃ½ ukazuje druhÃ½ zmÃ­nÄ›nÃ½ vztah:

<img alt="Scatter plot Cena vs. Den v roce" src="../../../../translated_images/cs/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" /> 

PodÃ­vejme se, zda existuje korelace pomocÃ­ funkce `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

ZdÃ¡ se, Å¾e korelace je pomÄ›rnÄ› malÃ¡, -0.15 podle `Month` a -0.17 podle `DayOfMonth`, ale mohlo by existovat jinÃ© dÅ¯leÅ¾itÃ© spojenÃ­. ZdÃ¡ se, Å¾e existujÃ­ rÅ¯znÃ© shluky cen odpovÃ­dajÃ­cÃ­ rÅ¯znÃ½m odrÅ¯dÃ¡m dÃ½nÃ­. Abychom tuto hypotÃ©zu potvrdili, vykreslÃ­me kaÅ¾dou kategorii dÃ½nÃ­ s jinou barvou. PÅ™edÃ¡nÃ­m parametru `ax` do funkce `scatter` mÅ¯Å¾eme vykreslit vÅ¡echny body na stejnÃ©m grafu:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Scatter plot Cena vs. Den v roce s barvami" src="../../../../translated_images/cs/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" /> 

NaÅ¡e vyÅ¡etÅ™ovÃ¡nÃ­ naznaÄuje, Å¾e odrÅ¯da mÃ¡ vÄ›tÅ¡Ã­ vliv na celkovou cenu neÅ¾ skuteÄnÃ© datum prodeje. VidÃ­me to i na sloupcovÃ©m grafu:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="SloupcovÃ½ graf cen podle odrÅ¯dy" src="../../../../translated_images/cs/price-by-variety.744a2f9925d9bcb4.webp" width="50%" /> 

ZamÄ›Å™Ã­me se prozatÃ­m pouze na jednu odrÅ¯du dÃ½nÃ­, 'pie type', a podÃ­vÃ¡me se, jakÃ½ vliv mÃ¡ datum na cenu:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Scatter plot Cena vs. Den v roce pro dÃ½nÄ› pie type" src="../../../../translated_images/cs/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" /> 

Pokud nynÃ­ vypoÄÃ­tÃ¡me korelaci mezi `Price` a `DayOfYear` pomocÃ­ funkce `corr`, dostaneme asi `-0.27` â€“ coÅ¾ znamenÃ¡, Å¾e trÃ©novÃ¡nÃ­ prediktivnÃ­ho modelu dÃ¡vÃ¡ smysl.

> PÅ™ed trÃ©novÃ¡nÃ­m modelu lineÃ¡rnÃ­ regrese je dÅ¯leÅ¾itÃ© mÃ­t jistotu, Å¾e jsou data vyÄiÅ¡tÄ›nÃ¡. LineÃ¡rnÃ­ regrese nefunguje dobÅ™e s chybÄ›jÃ­cÃ­mi hodnotami, proto je vhodnÃ© se zbavit vÅ¡ech prÃ¡zdnÃ½ch bunÄ›k:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

DalÅ¡Ã­m pÅ™Ã­stupem by bylo nahradit tyto chybÄ›jÃ­cÃ­ hodnoty prÅ¯mÄ›rnÃ½mi hodnotami pÅ™Ã­sluÅ¡nÃ©ho sloupce.

## JednoduchÃ¡ lineÃ¡rnÃ­ regrese

[![StrojovÃ© uÄenÃ­ pro zaÄÃ¡teÄnÃ­ky â€“ LineÃ¡rnÃ­ a polynomiÃ¡lnÃ­ regrese pomocÃ­ Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "StrojovÃ© uÄenÃ­ pro zaÄÃ¡teÄnÃ­ky â€“ LineÃ¡rnÃ­ a polynomiÃ¡lnÃ­ regrese pomocÃ­ Scikit-learn")

> ğŸ¥ KliknÄ›te na obrÃ¡zek vÃ½Å¡e pro krÃ¡tkÃ© video pÅ™ehled lineÃ¡rnÃ­ a polynomiÃ¡lnÃ­ regrese.

Pro natrÃ©novÃ¡nÃ­ naÅ¡eho modelu lineÃ¡rnÃ­ regrese pouÅ¾ijeme knihovnu **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

ZaÄneme oddÄ›lenÃ­m vstupnÃ­ch hodnot (rysy) a oÄekÃ¡vanÃ½ch vÃ½stupÅ¯ (Å¡tÃ­tky) do samostatnÃ½ch numpy polÃ­:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> VÅ¡imnÄ›te si, Å¾e jsme museli provÃ©st zmÄ›nu tvaru (`reshape`) vstupnÃ­ch dat, aby je balÃ­Äek lineÃ¡rnÃ­ regrese sprÃ¡vnÄ› pochopil. LineÃ¡rnÃ­ regrese oÄekÃ¡vÃ¡ 2D pole jako vstup, kde kaÅ¾dÃ½ Å™Ã¡dek pole odpovÃ­dÃ¡ vektoru vstupnÃ­ch rysÅ¯. V naÅ¡em pÅ™Ã­padÄ›, protoÅ¾e mÃ¡me pouze jeden vstup - potÅ™ebujeme pole o tvaru N&times;1, kde N je velikost datovÃ© sady.

NÃ¡slednÄ› je tÅ™eba data rozdÄ›lit na trÃ©novacÃ­ a testovacÃ­ sady, abychom mohli model po trÃ©ninku ovÄ›Å™it:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Nakonec samotnÃ© trÃ©novÃ¡nÃ­ lineÃ¡rnÃ­ho regresnÃ­ho modelu trvÃ¡ jen dva Å™Ã¡dky kÃ³du. Definujeme objekt `LinearRegression` a pÅ™izpÅ¯sobÃ­me ho naÅ¡im datÅ¯m pomocÃ­ metody `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Objekt `LinearRegression` po provedenÃ­ `fit` obsahuje vÅ¡echny koeficienty regrese, ke kterÃ½m lze pÅ™istupovat pomocÃ­ vlastnosti `.coef_`. V naÅ¡em pÅ™Ã­padÄ› je jen jeden koeficient, kterÃ½ by mÄ›l bÃ½t pÅ™ibliÅ¾nÄ› `-0.017`. To znamenÃ¡, Å¾e ceny se zdajÃ­ s Äasem mÃ­rnÄ› sniÅ¾ovat, ale ne pÅ™Ã­liÅ¡, kolem 2 centÅ¯ za den. K prÅ¯seÄÃ­ku regrese s osou Y se lze takÃ© dostat pomocÃ­ `lin_reg.intercept_` - v naÅ¡em pÅ™Ã­padÄ› bude pÅ™ibliÅ¾nÄ› `21`, coÅ¾ ukazuje cenu na zaÄÃ¡tku roku.

Abychom mohli zjistit, jak pÅ™esnÃ½ nÃ¡Å¡ model je, mÅ¯Å¾eme pÅ™edpovÃ­dat ceny na testovacÃ­ datovÃ© sadÄ› a pak zmÄ›Å™it, jak blÃ­zko jsou naÅ¡e pÅ™edpovÄ›di ke skuteÄnÃ½m hodnotÃ¡m. To lze provÃ©st pomocÃ­ metriky stÅ™ednÃ­ kvadratickÃ© chyby (MSE), coÅ¾ je prÅ¯mÄ›r vÅ¡ech druhÃ½ch mocnin rozdÃ­lÅ¯ mezi oÄekÃ¡vanou a pÅ™edpovÄ›zenou hodnotou.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

NaÅ¡e chyba se zdÃ¡ bÃ½t kolem 2 bodÅ¯, coÅ¾ je asi 17 %. NenÃ­ to moc dobrÃ©. DalÅ¡Ã­m ukazatelem kvality modelu je **koeficient determinace**, kterÃ½ lze zÃ­skat takto:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Pokud je hodnota 0, znamenÃ¡ to, Å¾e model nevnÃ­mÃ¡ vstupnÃ­ data a chovÃ¡ se jako *nejhorÅ¡Ã­ lineÃ¡rnÃ­ prediktor*, coÅ¾ je jednoduÅ¡e prÅ¯mÄ›r vÃ½sledku. Hodnota 1 znamenÃ¡, Å¾e mÅ¯Å¾eme dokonale pÅ™edpovÄ›dÄ›t vÅ¡echny oÄekÃ¡vanÃ© vÃ½stupy. V naÅ¡em pÅ™Ã­padÄ› je koeficient kolem 0,06, coÅ¾ je pomÄ›rnÄ› nÃ­zkÃ©.

MÅ¯Å¾eme takÃ© vykreslit testovacÃ­ data spolu s regresnÃ­ pÅ™Ã­mkou, abychom lÃ©pe vidÄ›li, jak regrese v naÅ¡em pÅ™Ã­padÄ› funguje:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linear regression" src="../../../../translated_images/cs/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## PolynomickÃ¡ regrese

DalÅ¡Ã­m typem lineÃ¡rnÃ­ regrese je polynomickÃ¡ regrese. ZatÃ­mco nÄ›kdy existuje mezi veliÄinami lineÃ¡rnÃ­ vztah â€“ ÄÃ­m vÄ›tÅ¡Ã­ dÃ½nÄ› objemem, tÃ­m vyÅ¡Å¡Ã­ cena â€“ jindy tyto vztahy nelze zobrazit jako rovinu nebo pÅ™Ã­mku. 

âœ… Zde je [nÄ›kolik dalÅ¡Ã­ch pÅ™Ã­kladÅ¯](https://online.stat.psu.edu/stat501/lesson/9/9.8) dat, kterÃ¡ by mohla vyuÅ¾Ã­t polynomickou regresi

PodÃ­vejte se znovu na vztah mezi Datem a Cenou. ZdÃ¡ se, Å¾e by tento bodovÃ½ graf nutnÄ› mÄ›l bÃ½t analyzovÃ¡n pomocÃ­ pÅ™Ã­mky? Nemohou ceny kolÃ­sat? V takovÃ©m pÅ™Ã­padÄ› mÅ¯Å¾ete vyzkouÅ¡et polynomickou regresi.

âœ… Polynom jsou matematickÃ© vÃ½razy, kterÃ© mohou obsahovat jednu nebo vÃ­ce promÄ›nnÃ½ch a koeficientÅ¯

PolynomickÃ¡ regrese vytvÃ¡Å™Ã­ zakÅ™ivenou kÅ™ivku, aby lÃ©pe sedÄ›la na nelineÃ¡rnÃ­ data. V naÅ¡em pÅ™Ã­padÄ›, pokud do vstupnÃ­ch dat zahrneme druhou mocninu promÄ›nnÃ© `DayOfYear`, mÄ›li bychom bÃ½t schopni pÅ™izpÅ¯sobit naÅ¡e data parabole, kterÃ¡ bude mÃ­t minimum v urÄitÃ©m bodÄ› bÄ›hem roku.

Scikit-learn obsahuje uÅ¾iteÄnÃ© [pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) pro spojovÃ¡nÃ­ rÅ¯znÃ½ch krokÅ¯ zpracovÃ¡nÃ­ dat dohromady. **Pipeline** je Å™etÄ›zec **estimatorÅ¯**. V naÅ¡em pÅ™Ã­padÄ› vytvoÅ™Ã­me pipeline, kterÃ¡ nejprve pÅ™idÃ¡ polynomickÃ© rysy k naÅ¡emu modelu a pak natrÃ©nuje regresi:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

PouÅ¾itÃ­ `PolynomialFeatures(2)` znamenÃ¡, Å¾e zahrneme vÅ¡echny polynomy druhÃ©ho stupnÄ› ze vstupnÃ­ch dat. V naÅ¡em pÅ™Ã­padÄ› to bude jen `DayOfYear`<sup>2</sup>, ale pokud mÃ¡me dvÄ› vstupnÃ­ promÄ›nnÃ© X a Y, pÅ™idÃ¡ to X<sup>2</sup>, XY a Y<sup>2</sup>. MÅ¯Å¾eme takÃ© pouÅ¾Ã­t vyÅ¡Å¡Ã­ mocniny, pokud chceme.

Pipeline lze pouÅ¾Ã­vat stejnÃ½m zpÅ¯sobem jako pÅ¯vodnÃ­ objekt `LinearRegression`, tj. mÅ¯Å¾eme pipeline `fit`-nout a pak pouÅ¾Ã­t `predict` pro zÃ­skÃ¡nÃ­ predikcÃ­. Zde je graf ukazujÃ­cÃ­ testovacÃ­ data a aproximaÄnÃ­ kÅ™ivku:

<img alt="Polynomial regression" src="../../../../translated_images/cs/poly-results.ee587348f0f1f60b.webp" width="50%" />

S vyuÅ¾itÃ­m polynomickÃ© regrese mÅ¯Å¾eme zÃ­skat mÃ­rnÄ› niÅ¾Å¡Ã­ MSE a vyÅ¡Å¡Ã­ koeficient determinace, ale ne dramaticky. MusÃ­me vzÃ­t v Ãºvahu i dalÅ¡Ã­ rysy!

> VidÃ­te, Å¾e minimÃ¡lnÃ­ ceny dÃ½nÃ­ jsou pozorovÃ¡ny nÄ›kde kolem Halloweenu. Jak byste tento jev vysvÄ›tlili? 

ğŸƒ Gratulujeme, prÃ¡vÄ› jste vytvoÅ™ili model, kterÃ½ mÅ¯Å¾e pomoci pÅ™edpovÄ›dÄ›t cenu dÃ½nÃ­ na kolÃ¡Äe. PravdÄ›podobnÄ› byste stejnÃ½ postup mohli opakovat pro vÅ¡echny druhy dÃ½nÃ­, ale to by bylo zdlouhavÃ©. NauÄme se nynÃ­, jak brÃ¡t v Ãºvahu odrÅ¯du dÃ½nÄ› v naÅ¡em modelu!

## KategorickÃ© rysy

V ideÃ¡lnÃ­m svÄ›tÄ› chceme bÃ½t schopni pÅ™edpovÃ­dat ceny pro rÅ¯znÃ© odrÅ¯dy dÃ½nÃ­ pomocÃ­ stejnÃ©ho modelu. Sloupec `Variety` je vÅ¡ak trochu odliÅ¡nÃ½ od sloupcÅ¯ jako `Month`, protoÅ¾e obsahuje neÄÃ­selnÃ© hodnoty. TakovÃ© sloupce se nazÃ½vajÃ­ **kategorickÃ©**.

[![ML for beginners - Categorical Feature Predictions with Linear Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML for beginners - Categorical Feature Predictions with Linear Regression")

> ğŸ¥ KliknÄ›te na obrÃ¡zek vÃ½Å¡e pro krÃ¡tkÃ© video o pouÅ¾itÃ­ kategorickÃ½ch rysÅ¯.

Zde vidÃ­te, jak prÅ¯mÄ›rnÃ¡ cena zÃ¡visÃ­ na odrÅ¯dÄ›:

<img alt="Average price by variety" src="../../../../translated_images/cs/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

Abychom mohli zohlednit odrÅ¯du, musÃ­me ji nejdÅ™Ã­ve pÅ™evÃ©st do ÄÃ­selnÃ© podoby, tedy **zakÃ³dovat** ji. Existuje nÄ›kolik zpÅ¯sobÅ¯, jak to udÄ›lat:

* JednoduchÃ© **ÄÃ­selnÃ© kÃ³dovÃ¡nÃ­** vytvoÅ™Ã­ tabulku rÅ¯znÃ½ch odrÅ¯d a pak nahradÃ­ nÃ¡zev odrÅ¯dy indexem v tÃ©to tabulce. To vÅ¡ak nenÃ­ nejlepÅ¡Ã­ nÃ¡pad pro lineÃ¡rnÃ­ regresi, protoÅ¾e lineÃ¡rnÃ­ regrese vezme skuteÄnou ÄÃ­selnou hodnotu indexu a pÅ™idÃ¡ ji k vÃ½sledku, vynÃ¡sobÃ­ nÄ›jakÃ½m koeficientem. V naÅ¡em pÅ™Ã­padÄ› je vztah mezi ÄÃ­slem indexu a cenou jasnÄ› nelineÃ¡rnÃ­, i kdyÅ¾ zajistÃ­me, Å¾e indexy jsou uspoÅ™Ã¡dÃ¡ny nÄ›jak specificky.
* **One-hot encoding** nahradÃ­ sloupec `Variety` ÄtyÅ™mi rÅ¯znÃ½mi sloupci, jednÃ­m pro kaÅ¾dou odrÅ¯du. KaÅ¾dÃ½ sloupec bude obsahovat `1`, pokud pÅ™Ã­sluÅ¡nÃ½ Å™Ã¡dek odpovÃ­dÃ¡ danÃ© odrÅ¯dÄ›, a `0` jinak. To znamenÃ¡, Å¾e v lineÃ¡rnÃ­ regresi budou ÄtyÅ™i koeficienty, po jednom pro kaÅ¾dou odrÅ¯du dÃ½nÃ­, zodpovÄ›dnÃ© za "startovacÃ­ cenu" (nebo spÃ­Å¡e "pÅ™Ã­davnou cenu") pro danou odrÅ¯du.

NÃ­Å¾e uvedenÃ½ kÃ³d ukazuje, jak lze odrÅ¯du zakÃ³dovat one-hot metodou:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Pro trÃ©novÃ¡nÃ­ lineÃ¡rnÃ­ regrese s one-hot zakÃ³dovanou odrÅ¯dou jako vstupem staÄÃ­ sprÃ¡vnÄ› inicializovat `X` a `y` data:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Zbytek kÃ³du je stejnÃ½ jako ten, kterÃ½ jsme pouÅ¾ili vÃ½Å¡e pro trÃ©novÃ¡nÃ­ lineÃ¡rnÃ­ regrese. Pokud to vyzkouÅ¡Ã­te, uvidÃ­te, Å¾e stÅ™ednÃ­ kvadratickÃ¡ chyba je pÅ™ibliÅ¾nÄ› stejnÃ¡, ale koeficient determinace je podstatnÄ› vyÅ¡Å¡Ã­ (~77 %). Pro jeÅ¡tÄ› pÅ™esnÄ›jÅ¡Ã­ pÅ™edpovÄ›di mÅ¯Å¾eme vzÃ­t v Ãºvahu vÃ­ce kategorickÃ½ch rysÅ¯ i ÄÃ­selnÃ© rysy, jako jsou `Month` nebo `DayOfYear`. Pro zÃ­skÃ¡nÃ­ jednÃ© velkÃ© mnoÅ¾iny rysÅ¯ mÅ¯Å¾eme pouÅ¾Ã­t `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Zde takÃ© bereme v Ãºvahu `City` a typ `Package`, coÅ¾ nÃ¡m dÃ¡vÃ¡ MSE 2,84 (10 %) a koeficient determinace 0,94!

## SpojenÃ­ vÅ¡eho dohromady

Pro vytvoÅ™enÃ­ nejlepÅ¡Ã­ho modelu mÅ¯Å¾eme pouÅ¾Ã­t kombinovanÃ¡ (one-hot zakÃ³dovanÃ¡ kategorickÃ¡ + ÄÃ­selnÃ¡) data z vÃ½Å¡e uvedenÃ©ho pÅ™Ã­kladu spoleÄnÄ› s polynomickou regresÃ­. Pro vaÅ¡e pohodlÃ­ je zde ÃºplnÃ½ kÃ³d:

```python
# pÅ™ipravit trÃ©ninkovÃ¡ data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# provÃ©st rozdÄ›lenÃ­ na trÃ©ninkovou a testovacÃ­ sadu
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# nastavit a natrÃ©novat pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# pÅ™edpovÄ›dÄ›t vÃ½sledky pro testovacÃ­ data
pred = pipeline.predict(X_test)

# vypoÄÃ­tat MSE a koeficient determinace
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

To by mÄ›lo dÃ¡t nejlepÅ¡Ã­ koeficient determinace tÃ©mÄ›Å™ 97 % a MSE=2,23 (~8 % chybovost pÅ™edpovÄ›di).

| Model | MSE | Determinace |
|-------|-----|---------------|
| LineÃ¡rnÃ­ s `DayOfYear` | 2,77 (17,2 %) | 0,07 |
| PolynomickÃ¡ s `DayOfYear` | 2,73 (17,0 %) | 0,08 |
| LineÃ¡rnÃ­ s `Variety` | 5,24 (19,7 %) | 0,77 |
| LineÃ¡rnÃ­ se vÅ¡emi rysy | 2,84 (10,5 %) | 0,94 |
| PolynomickÃ¡ se vÅ¡emi rysy | 2,23 (8,25 %) | 0,97 |

ğŸ† VÃ½bornÄ›! VytvoÅ™ili jste ÄtyÅ™i regresnÃ­ modely v jednÃ© lekci a vylepÅ¡ili kvalitu modelu na 97 %. V poslednÃ­ ÄÃ¡sti o regresi se nauÄÃ­te o logistickÃ© regresi pro urÄenÃ­ kategoriÃ­. 

---
## ğŸš€VÃ½zva

Otestujte v tomto notebooku nÄ›kolik rÅ¯znÃ½ch promÄ›nnÃ½ch a sledujte, jak korelace odpovÃ­dÃ¡ pÅ™esnosti modelu.

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://ff-quizzes.netlify.app/en/ml/)

## Recenze a samostudium

V tÃ©to lekci jsme se nauÄili o lineÃ¡rnÃ­ regresi. ExistujÃ­ i jinÃ© dÅ¯leÅ¾itÃ© typy regrese. PÅ™eÄtÄ›te si o postupech Stepwise, Ridge, Lasso a Elasticnet. DobrÃ½m kurzem, kterÃ½ se mÅ¯Å¾ete dÃ¡le uÄit, je [StanfordskÃ½ kurz statistickÃ©ho uÄenÃ­](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## ZadÃ¡nÃ­

[Vybuduj model](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**ZÅ™eknutÃ­ se odpovÄ›dnosti**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ automatickÃ© pÅ™ekladovÃ© sluÅ¾by [Co-op Translator](https://github.com/Azure/co-op-translator). PÅ™estoÅ¾e usilujeme o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatizovanÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho mateÅ™skÃ©m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro kritickÃ© informace doporuÄujeme profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. Nejsme odpovÄ›dnÃ­ za jakÃ©koliv nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vzniklÃ© pouÅ¾itÃ­m tohoto pÅ™ekladu.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->