<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ba0f6e1019351351c8ee4c92867b6a0b",
  "translation_date": "2025-08-29T21:16:13+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "mo"
}
-->
# 後記：使用負責任的 AI 儀表板元件進行機器學習模型調試

## [課前測驗](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## 簡介

機器學習已深刻影響我們的日常生活。人工智慧正逐漸滲透到一些對個人及社會至關重要的系統中，例如醫療保健、金融、教育和就業等領域。例如，系統和模型參與了日常決策任務，如醫療診斷或欺詐檢測。因此，人工智慧的進步及其加速採用也伴隨著不斷演變的社會期望和日益增多的監管要求。我們經常看到人工智慧系統未能達到期望的領域，暴露出新的挑戰，並且政府開始對人工智慧解決方案進行監管。因此，分析這些模型以提供公平、可靠、包容、透明和負責任的結果對每個人都至關重要。

在本課程中，我們將探討一些實用工具，用於評估模型是否存在負責任的人工智慧問題。傳統的機器學習調試技術通常基於定量計算，例如整體準確率或平均誤差損失。想像一下，如果您用來構建這些模型的數據缺乏某些人口統計特徵，例如種族、性別、政治觀點、宗教，或者這些特徵被不成比例地代表，會發生什麼情況？如果模型的輸出被解釋為偏向某些人口統計特徵，又會如何？這可能導致敏感特徵群體的過度或不足代表，進而引發模型的公平性、包容性或可靠性問題。此外，機器學習模型通常被視為黑箱，這使得理解和解釋模型預測的驅動因素變得困難。當數據科學家和人工智慧開發者缺乏足夠的工具來調試和評估模型的公平性或可信度時，這些都是他們面臨的挑戰。

在本課程中，您將學習如何使用以下方法調試模型：

- **錯誤分析**：識別模型在數據分佈中錯誤率較高的區域。
- **模型概覽**：對不同數據群體進行比較分析，發現模型性能指標中的差異。
- **數據分析**：調查數據是否存在過度或不足代表的情況，這可能導致模型偏向某些數據群體。
- **特徵重要性**：了解哪些特徵在全局或局部層面上驅動模型的預測。

## 前置條件

作為前置條件，請先查看 [開發者的負責任人工智慧工具](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![負責任人工智慧工具的動態圖](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## 錯誤分析

傳統的模型性能指標用於衡量準確性，通常基於正確與錯誤預測的計算。例如，判斷一個模型的準確率為 89%，誤差損失為 0.001，可以被認為是良好的性能。然而，錯誤通常在底層數據集中並非均勻分佈。您可能獲得 89% 的模型準確率，但發現模型在某些數據區域的錯誤率高達 42%。這些特定數據群體的失敗模式可能導致公平性或可靠性問題。因此，了解模型表現良好或不佳的區域至關重要。模型在某些數據區域的高錯誤率可能揭示出重要的數據群體。

![分析和調試模型錯誤](../../../../translated_images/ea-error-distribution.117452e1177c1dd84fab2369967a68bcde787c76c6ea7fdb92fcf15d1fce8206.mo.png)

RAI 儀表板中的錯誤分析元件通過樹狀視覺化展示模型失敗在不同群體中的分佈情況。這有助於識別數據集中錯誤率較高的特徵或區域。通過查看模型大部分錯誤的來源，您可以開始調查根本原因。您還可以創建數據群體進行分析。這些數據群體有助於調試過程，確定模型在某些群體表現良好，而在其他群體表現不佳的原因。

![錯誤分析](../../../../translated_images/ea-error-cohort.6886209ea5d438c4daa8bfbf5ce3a7042586364dd3eccda4a4e3d05623ac702a.mo.png)

樹狀圖中的視覺指示器可以幫助更快地定位問題區域。例如，樹節點的紅色陰影越深，錯誤率越高。

熱圖是另一種視覺化功能，用於通過一個或兩個特徵調查錯誤率，找出整個數據集或群體中模型錯誤的貢獻因素。

![錯誤分析熱圖](../../../../translated_images/ea-heatmap.8d27185e28cee3830c85e1b2e9df9d2d5e5c8c940f41678efdb68753f2f7e56c.mo.png)

使用錯誤分析的情況：

* 深入了解模型失敗如何在數據集和多個輸入及特徵維度中分佈。
* 分解整體性能指標，自動發現錯誤群體，以便制定有針對性的緩解措施。

## 模型概覽

評估機器學習模型的性能需要全面了解其行為。這可以通過查看多個指標（例如錯誤率、準確率、召回率、精確度或 MAE 平均絕對誤差）來實現，以發現性能指標中的差異。一個指標可能看起來很好，但另一個指標可能暴露出不準確性。此外，比較整個數據集或群體中的指標差異有助於揭示模型表現良好或不佳的地方。這在查看模型對敏感特徵（例如患者的種族、性別或年齡）與非敏感特徵的表現時尤為重要，以揭示模型可能存在的潛在不公平性。例如，發現模型在具有敏感特徵的群體中錯誤率更高，可能揭示模型存在潛在的不公平性。

RAI 儀表板中的模型概覽元件不僅有助於分析數據群體中的性能指標，還使用戶能夠比較模型在不同群體中的行為。

![數據群體 - RAI 儀表板中的模型概覽](../../../../translated_images/model-overview-dataset-cohorts.dfa463fb527a35a0afc01b7b012fc87bf2cad756763f3652bbd810cac5d6cf33.mo.png)

該元件的基於特徵的分析功能允許用戶在特定特徵內縮小數據子群體，以便在更細粒度的層面上識別異常。例如，儀表板具有內置智能，可以自動為用戶選擇的特徵生成群體（例如 *"time_in_hospital < 3"* 或 *"time_in_hospital >= 7"*）。這使用戶能夠從更大的數據群體中隔離特定特徵，以查看它是否是模型錯誤結果的關鍵影響因素。

![特徵群體 - RAI 儀表板中的模型概覽](../../../../translated_images/model-overview-feature-cohorts.c5104d575ffd0c80b7ad8ede7703fab6166bfc6f9125dd395dcc4ace2f522f70.mo.png)

模型概覽元件支持兩類差異指標：

**模型性能差異**：這些指標計算所選性能指標在數據子群體之間的差異。以下是一些例子：

* 準確率差異
* 錯誤率差異
* 精確度差異
* 召回率差異
* 平均絕對誤差（MAE）差異

**選擇率差異**：此指標包含子群體之間選擇率（有利預測）的差異。例如，貸款批准率的差異。選擇率指的是每個類別中被分類為 1 的數據點比例（在二元分類中）或預測值的分佈（在回歸中）。

## 數據分析

> 「如果你對數據施加足夠的壓力，它會承認任何事情」——羅納德·科斯

這句話聽起來極端，但事實上數據確實可以被操縱以支持任何結論。有時這種操縱可能是無意的。作為人類，我們都有偏見，並且往往難以有意識地知道何時在數據中引入了偏見。保證人工智慧和機器學習的公平性仍然是一個複雜的挑戰。

數據是傳統模型性能指標的一個巨大盲點。您可能擁有高準確率，但這並不總是反映出數據集中可能存在的底層數據偏差。例如，如果一家公司高管職位的員工數據集中有 27% 的女性和 73% 的男性，基於此數據訓練的招聘人工智慧模型可能會主要針對男性群體進行高級職位的招聘。這種數據的不平衡使模型的預測偏向某一性別，揭示了模型存在性別偏見的公平性問題。

RAI 儀表板中的數據分析元件有助於識別數據集中過度或不足代表的區域。它幫助用戶診斷由數據不平衡或缺乏特定數據群體代表性引入的錯誤和公平性問題的根本原因。這使用戶能夠根據預測和實際結果、錯誤群體以及特定特徵來可視化數據集。有時發現一個代表性不足的數據群體也可能揭示模型未能很好地學習，因此錯誤率較高。擁有數據偏差的模型不僅是一個公平性問題，還表明模型不具包容性或可靠性。

![RAI 儀表板中的數據分析元件](../../../../translated_images/dataanalysis-cover.8d6d0683a70a5c1e274e5a94b27a71137e3d0a3b707761d7170eb340dd07f11d.mo.png)

使用數據分析的情況：

* 通過選擇不同的篩選器探索數據集統計，將數據切片為不同維度（也稱為群體）。
* 了解數據集在不同群體和特徵群體中的分佈。
* 確定與公平性、錯誤分析和因果關係相關的發現（來自其他儀表板元件）是否是數據集分佈的結果。
* 決定在哪些區域收集更多數據，以減少由代表性問題、標籤噪音、特徵噪音、標籤偏差等因素引起的錯誤。

## 模型可解釋性

機器學習模型通常被視為黑箱。理解哪些關鍵數據特徵驅動模型的預測可能具有挑戰性。提供模型做出某一預測的透明性至關重要。例如，如果人工智慧系統預測某位糖尿病患者有可能在 30 天內再次入院，它應該能夠提供支持其預測的數據。提供支持數據指標可以幫助醫生或醫院做出明智的決策。此外，能夠解釋模型對個別患者做出預測的原因，有助於符合健康監管要求的問責性。當您使用機器學習模型影響人們的生活時，了解和解釋模型行為的驅動因素至關重要。模型的可解釋性和可解釋性有助於回答以下場景中的問題：

* 模型調試：為什麼我的模型會犯這個錯誤？我該如何改進模型？
* 人工智慧與人類合作：我如何理解並信任模型的決策？
* 監管合規：我的模型是否符合法律要求？

RAI 儀表板中的特徵重要性元件幫助您調試並全面了解模型如何做出預測。它也是機器學習專業人士和決策者解釋和展示影響模型行為的特徵證據的有用工具，以符合監管要求。接下來，用戶可以探索全局和局部解釋，驗證哪些特徵驅動模型的預測。全局解釋列出影響模型整體預測的主要特徵。局部解釋顯示模型對個別案例做出預測的原因。評估局部解釋的能力在調試或審核特定案例時也很有幫助，以更好地理解和解釋模型做出準確或不準確預測的原因。

![RAI 儀表板中的特徵重要性元件](../../../../translated_images/9-feature-importance.cd3193b4bba3fd4bccd415f566c2437fb3298c4824a3dabbcab15270d783606e.mo.png)

* 全局解釋：例如，哪些特徵影響糖尿病患者再次入院模型的整體行為？
* 局部解釋：例如，為什麼一位年齡超過 60 歲且有過住院史的糖尿病患者被預測為會或不會在 30 天內再次入院？

在調試模型性能的過程中，特徵重要性顯示特徵在不同群體中的影響程度。它有助於揭示特徵在驅動模型錯誤預測時的影響程度異常。特徵重要性元件可以顯示特徵中的哪些值對模型結果產生了正面或負面影響。例如，如果模型做出了不準確的預測，該元件使您能夠深入分析並確定驅動預測的特徵或特徵值。這種細節不僅有助於調試，還在審計情況下提供透明性和問責性。最後，該元件可以幫助您識別公平性問題。例如，如果某些敏感特徵（如種族或性別）在驅動模型預測中具有高度影響力，這可能表明模型存在種族或性別偏見。

![特徵重要性](../../../../translated_images/9-features-influence.3ead3d3f68a84029f1e40d3eba82107445d3d3b6975d4682b23d8acc905da6d0.mo.png)

使用可解釋性的情況：

* 通過了解哪些特徵對預測最重要，確定人工智慧系統的預測是否值得信賴。
* 通過首先理解模型並確定模型是否使用健康特徵或僅僅是錯誤相關性，來進行模型調試。
* 通過了解模型是否基於敏感特徵或與敏感特徵高度相關的特徵進行預測，揭示潛在的不公平性來源。
* 通過生成局部解釋來展示模型結果，建立用戶對模型決策的信任。
* 完成人工智慧系統的監管審計，以驗證模型並監控模型決策對人類的影響。

## 結論

RAI 儀表板的所有元件都是幫助您構建對社會更少傷害、更值得信賴的機器學習模型的實用工具。它有助於防止對人權的威脅；避免歧視或排除某些群體的生活機會；以及減少身體或心理傷害的風險。它還通過生成局部解釋來展示模型結果，幫助建立對模型決策的信任。一些潛在的傷害可以分類為：

- **分配**：例如，某一性別或種族被偏袒。
- **服務質量**：如果您僅針對一個特定場景訓練數據，但現實情況更為複雜，則會導致服務性能不佳。
- **刻板印象**：將某一群體與預先分配的屬性聯繫起來。
- **貶低**：不公平地批評或標籤某事或某人。
- **過度或不足的代表性**。這個概念指的是某些群體在某些職業中未被看到，而任何持續推動這種情況的服務或功能都可能造成傷害。

### Azure RAI 儀表板

[Azure RAI 儀表板](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) 是基於由領先的學術機構和組織（包括 Microsoft）開發的開源工具構建的，這些工具對於資料科學家和 AI 開發者理解模型行為、發現並減輕 AI 模型中的不良問題至關重要。

- 透過查看 RAI 儀表板的[文件](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)，了解如何使用不同的組件。

- 查看一些 RAI 儀表板的[範例筆記本](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks)，以便在 Azure 機器學習中調試更負責任的 AI 情境。

---
## 🚀 挑戰

為了防止統計或資料偏差從一開始就被引入，我們應該：

- 確保系統開發人員擁有多元背景和觀點
- 投資於反映社會多樣性的資料集
- 開發更好的方法來檢測和修正偏差

思考在模型構建和使用中不公平現象明顯的真實情境。我們還應該考慮什麼？

## [課後測驗](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## 回顧與自學

在本課中，您已學習了一些將負責任 AI 融入機器學習的實用工具。

觀看這場工作坊以更深入了解相關主題：

- 負責任 AI 儀表板：實踐中操作化 RAI 的一站式平台，由 Besmira Nushi 和 Mehrnoosh Sameki 主講

[![負責任 AI 儀表板：實踐中操作化 RAI 的一站式平台](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "負責任 AI 儀表板：實踐中操作化 RAI 的一站式平台")

> 🎥 點擊上方圖片觀看影片：負責任 AI 儀表板：實踐中操作化 RAI 的一站式平台，由 Besmira Nushi 和 Mehrnoosh Sameki 主講

參考以下材料以了解更多關於負責任 AI 的資訊，以及如何構建更值得信賴的模型：

- Microsoft 的 RAI 儀表板工具，用於調試 ML 模型：[負責任 AI 工具資源](https://aka.ms/rai-dashboard)

- 探索負責任 AI 工具包：[Github](https://github.com/microsoft/responsible-ai-toolbox)

- Microsoft 的 RAI 資源中心：[負責任 AI 資源 – Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Microsoft 的 FATE 研究小組：[FATE：AI 中的公平性、問責性、透明性和倫理 - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## 作業

[探索 RAI 儀表板](assignment.md)

---

**免責聲明**：  
本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們努力確保翻譯的準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵信息，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或錯誤解釋不承擔責任。