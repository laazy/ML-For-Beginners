# Construir um modelo de regress√£o usando Scikit-learn: regress√£o de quatro maneiras

## Nota para iniciantes

A regress√£o linear √© usada quando queremos prever um **valor num√©rico** (por exemplo, pre√ßo da casa, temperatura ou vendas).
Ela funciona encontrando uma linha reta que melhor representa a rela√ß√£o entre as caracter√≠sticas de entrada e a sa√≠da.

Nesta li√ß√£o, focamos em entender o conceito antes de explorar t√©cnicas mais avan√ßadas de regress√£o.
![Infogr√°fico regress√£o linear vs polinomial](../../../../translated_images/pt-BR/linear-polynomial.5523c7cb6576ccab.webp)
> Infogr√°fico por [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ml/)

> ### [Esta li√ß√£o est√° dispon√≠vel em R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introdu√ß√£o

At√© agora, voc√™ explorou o que √© regress√£o com dados de exemplo coletados do conjunto de dados de pre√ßos de ab√≥boras que usaremos ao longo desta li√ß√£o. Voc√™ tamb√©m os visualizou usando Matplotlib.

Agora voc√™ est√° pronto para aprofundar em regress√£o para ML. Enquanto a visualiza√ß√£o permite entender os dados, o verdadeiro poder do Machine Learning vem do _treinamento de modelos_. Modelos s√£o treinados com dados hist√≥ricos para capturar automaticamente depend√™ncias dos dados, e eles permitem prever resultados para novos dados, que o modelo n√£o viu antes.

Nesta li√ß√£o, voc√™ aprender√° mais sobre dois tipos de regress√£o: _regress√£o linear b√°sica_ e _regress√£o polinomial_, junto com um pouco da matem√°tica que fundamenta essas t√©cnicas. Esses modelos nos permitir√£o prever pre√ßos de ab√≥boras dependendo de diferentes dados de entrada.

[![ML para iniciantes - Entendendo Regress√£o Linear](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML para iniciantes - Entendendo Regress√£o Linear")

> üé• Clique na imagem acima para um breve v√≠deo com vis√£o geral da regress√£o linear.

> Ao longo deste curr√≠culo, assumimos conhecimento m√≠nimo de matem√°tica e buscamos torn√°-lo acess√≠vel para estudantes de outras √°reas, ent√£o fique atento √†s notas, üßÆ chamadas, diagramas e outras ferramentas de aprendizagem para ajudar na compreens√£o.

### Pr√©-requisitos

Voc√™ j√° deve estar familiarizado com a estrutura dos dados de ab√≥boras que estamos examinando. Voc√™ pode encontr√°-los pr√©-carregados e pr√©-limpados no arquivo _notebook.ipynb_ desta li√ß√£o. No arquivo, o pre√ßo da ab√≥bora √© exibido por alqueire em um novo dataframe. Certifique-se de conseguir executar esses notebooks em kernels no Visual Studio Code.

### Prepara√ß√£o

Como lembrete, voc√™ est√° carregando esses dados para poder fazer perguntas sobre eles.

- Qual √© o melhor momento para comprar ab√≥boras?
- Qual pre√ßo posso esperar de uma caixa de ab√≥boras miniatura?
- Devo compr√°-las em cestas de meio alqueire ou por caixas de 1 1/9 alqueires?
Vamos continuar explorando esses dados.

Na li√ß√£o anterior, voc√™ criou um dataframe Pandas e o preencheu com parte do conjunto de dados original, padronizando o pre√ßo por alqueire. Fazendo isso, por√©m, voc√™ conseguiu reunir cerca de 400 pontos de dados e apenas para os meses de outono.

D√™ uma olhada nos dados que pr√©-carregamos no notebook acompanhado desta li√ß√£o. Os dados est√£o pr√©-carregados e um gr√°fico de dispers√£o inicial √© tra√ßado para mostrar os dados de meses. Talvez possamos obter um pouco mais de detalhes sobre a natureza dos dados limpando-os mais.

## Uma linha de regress√£o linear

Como voc√™ aprendeu na Li√ß√£o 1, o objetivo de um exerc√≠cio de regress√£o linear √© poder tra√ßar uma linha para:

- **Mostrar rela√ß√µes entre vari√°veis**. Mostrar a rela√ß√£o entre vari√°veis
- **Fazer previs√µes**. Fazer previs√µes precisas sobre onde um novo ponto de dados cairia em rela√ß√£o a essa linha.

√â t√≠pico da **Regress√£o dos M√≠nimos Quadrados** desenhar esse tipo de linha. O termo "M√≠nimos Quadrados" refere-se ao processo de minimizar o erro total em nosso modelo. Para cada ponto de dados, medimos a dist√¢ncia vertical (chamada res√≠duo) entre o ponto real e nossa linha de regress√£o.

Elevamos essas dist√¢ncias ao quadrado por duas raz√µes principais:

1. **Magnitude sobre dire√ß√£o:** Queremos tratar um erro de -5 da mesma forma que um erro de +5. Quadrar torna todos os valores positivos.

2. **Penaliza√ß√£o de outliers:** Quadrar atribui mais peso a erros maiores, for√ßando a linha a ficar mais pr√≥xima dos pontos que est√£o longe.

Depois somamos todos esses valores quadrados. Nosso objetivo √© encontrar a linha espec√≠fica onde essa soma final √© a menor poss√≠vel ‚Äî da√≠ o nome "M√≠nimos Quadrados".

> **üßÆ Mostre a matem√°tica** 
> 
> Essa linha, chamada _linha de melhor ajuste_, pode ser expressa por [uma equa√ß√£o](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` √© a 'vari√°vel explicativa'. `Y` √© a 'vari√°vel dependente'. A inclina√ß√£o da linha √© `b` e `a` √© a intercepta√ß√£o no eixo y, que se refere ao valor de `Y` quando `X = 0`. 
>
>![calcular a inclina√ß√£o](../../../../translated_images/pt-BR/slope.f3c9d5910ddbfcf9.webp)
>
> Primeiro, calcule a inclina√ß√£o `b`. Infogr√°fico por [Jen Looper](https://twitter.com/jenlooper)
>
> Em outras palavras, e referindo-se √† pergunta original dos dados das ab√≥boras: "prever o pre√ßo de uma ab√≥bora por alqueire por m√™s", `X` se referiria ao pre√ßo e `Y` ao m√™s de venda.
>
>![completar a equa√ß√£o](../../../../translated_images/pt-BR/calculation.a209813050a1ddb1.webp)
>
> Calcule o valor de Y. Se voc√™ est√° pagando cerca de $4, deve ser abril! Infogr√°fico por [Jen Looper](https://twitter.com/jenlooper)
>
> A matem√°tica que calcula a linha deve demonstrar a inclina√ß√£o da linha, que tamb√©m depende da intercepta√ß√£o, ou de onde `Y` est√° situado quando `X = 0`.
>
> Voc√™ pode observar o m√©todo de c√°lculo desses valores no site [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). Visite tamb√©m [este calculador de m√≠nimos quadrados](https://www.mathsisfun.com/data/least-squares-calculator.html) para ver como os valores num√©ricos impactam a linha.

## Correla√ß√£o

Mais um termo para entender √© o **Coeficiente de Correla√ß√£o** entre as vari√°veis X e Y dadas. Usando um gr√°fico de dispers√£o, voc√™ pode rapidamente visualizar esse coeficiente. Um gr√°fico com pontos de dados alinhados em uma linha bem definida tem alta correla√ß√£o, mas um gr√°fico com pontos de dados espalhados entre X e Y tem baixa correla√ß√£o.

Um bom modelo de regress√£o linear ser√° aquele com um alto Coeficiente de Correla√ß√£o (mais pr√≥ximo de 1 do que de 0) usando o m√©todo de Regress√£o dos M√≠nimos Quadrados com uma linha de regress√£o.

‚úÖ Execute o notebook que acompanha esta li√ß√£o e observe o gr√°fico de dispers√£o de M√™s para Pre√ßo. Os dados associando M√™s a Pre√ßo para vendas de ab√≥bora parecem ter alta ou baixa correla√ß√£o, segundo sua interpreta√ß√£o visual do gr√°fico? Isso muda se voc√™ usar uma medida mais detalhada em vez de `M√™s`, por exemplo, *dia do ano* (ou seja, n√∫mero de dias desde o in√≠cio do ano)?

No c√≥digo abaixo, assumiremos que limpamos os dados e obtivemos um dataframe chamado `new_pumpkins`, semelhante ao seguinte:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> O c√≥digo para limpar os dados est√° dispon√≠vel em [`notebook.ipynb`](notebook.ipynb). Fizemos os mesmos passos de limpeza da li√ß√£o anterior e calculamos a coluna `DayOfYear` usando a seguinte express√£o:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Agora que voc√™ entende a matem√°tica por tr√°s da regress√£o linear, vamos criar um modelo de Regress√£o para ver se conseguimos prever qual embalagem de ab√≥boras ter√° os melhores pre√ßos. Algu√©m comprando ab√≥boras para uma horta para o feriado pode querer essa informa√ß√£o para otimizar suas compras de embalagens de ab√≥bora para a horta.

## Procurando Correla√ß√£o

[![ML para iniciantes - Procurando Correla√ß√£o: A Chave para Regress√£o Linear](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML para iniciantes - Procurando Correla√ß√£o: A Chave para Regress√£o Linear")

> üé• Clique na imagem acima para um breve v√≠deo com vis√£o geral de correla√ß√£o.

Na li√ß√£o anterior, voc√™ provavelmente viu que o pre√ßo m√©dio para diferentes meses se parece com isto:

<img alt="Pre√ßo m√©dio por m√™s" src="../../../../translated_images/pt-BR/barchart.a833ea9194346d76.webp" width="50%"/>

Isso sugere que deveria haver alguma correla√ß√£o, e podemos tentar treinar um modelo de regress√£o linear para prever a rela√ß√£o entre `Month` e `Price`, ou entre `DayOfYear` e `Price`. Aqui est√° o gr√°fico de dispers√£o que mostra essa √∫ltima rela√ß√£o:

<img alt="Gr√°fico de dispers√£o de Pre√ßo vs. Dia do Ano" src="../../../../translated_images/pt-BR/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" /> 

Vamos ver se h√° correla√ß√£o usando a fun√ß√£o `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Parece que a correla√ß√£o √© bem pequena, -0,15 pelo `Month` e -0,17 pelo `DayOfMonth`, mas pode haver outra rela√ß√£o importante. Parece que existem diferentes grupos de pre√ßos correspondendo a diferentes variedades de ab√≥boras. Para confirmar essa hip√≥tese, vamos tra√ßar cada categoria de ab√≥bora usando uma cor diferente. Passando um par√¢metro `ax` para a fun√ß√£o `scatter` podemos tra√ßar todos os pontos no mesmo gr√°fico:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Gr√°fico de dispers√£o de Pre√ßo vs. Dia do Ano" src="../../../../translated_images/pt-BR/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" /> 

Nossa investiga√ß√£o sugere que a variedade tem mais efeito no pre√ßo geral do que a pr√≥pria data de venda. Podemos ver isso com um gr√°fico de barras:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Gr√°fico de barras de pre√ßo por variedade" src="../../../../translated_images/pt-BR/price-by-variety.744a2f9925d9bcb4.webp" width="50%" /> 

Vamos focar por enquanto apenas em uma variedade de ab√≥bora, a 'pie type', e ver qual efeito a data tem no pre√ßo:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Gr√°fico de dispers√£o de Pre√ßo vs. Dia do Ano" src="../../../../translated_images/pt-BR/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" /> 

Se agora calcularmos a correla√ß√£o entre `Price` e `DayOfYear` usando a fun√ß√£o `corr`, obteremos algo como `-0.27` ‚Äî o que significa que faz sentido treinar um modelo preditivo.

> Antes de treinar um modelo de regress√£o linear, √© importante garantir que nossos dados estejam limpos. A regress√£o linear n√£o funciona bem com valores ausentes, portanto faz sentido eliminar todas as c√©lulas vazias:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Outra abordagem seria preencher esses valores vazios com a m√©dia da respectiva coluna.

## Regress√£o Linear Simples

[![ML para iniciantes - Regress√£o Linear e Polinomial usando Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML para iniciantes - Regress√£o Linear e Polinomial usando Scikit-learn")

> üé• Clique na imagem acima para um breve v√≠deo com vis√£o geral de regress√£o linear e polinomial.

Para treinar nosso modelo de Regress√£o Linear, usaremos a biblioteca **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Come√ßamos separando os valores de entrada (features) e a sa√≠da esperada (r√≥tulo) em arrays numpy separados:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Observe que tivemos que realizar um `reshape` nos dados de entrada para que o pacote de Regress√£o Linear os entendesse corretamente. Regress√£o Linear espera um array 2D como entrada, onde cada linha corresponde a um vetor de caracter√≠sticas de entrada. No nosso caso, como temos apenas uma entrada, precisamos de um array com forma N&times;1, onde N √© o tamanho do conjunto de dados.

Em seguida, precisamos dividir os dados em conjuntos de treino e teste, para que possamos validar nosso modelo ap√≥s o treinamento:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Finalmente, o treinamento do modelo de Regress√£o Linear em si leva apenas duas linhas de c√≥digo. Definimos o objeto `LinearRegression`, e o ajustamos aos nossos dados usando o m√©todo `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

O objeto `LinearRegression` ap√≥s o `fit` cont√©m todos os coeficientes da regress√£o, que podem ser acessados usando a propriedade `.coef_`. No nosso caso, h√° apenas um coeficiente, que deve estar em torno de `-0.017`. Isso significa que os pre√ßos parecem cair um pouco com o tempo, mas n√£o muito, cerca de 2 centavos por dia. Tamb√©m podemos acessar o ponto de interse√ß√£o da regress√£o com o eixo Y usando `lin_reg.intercept_` - que ser√° em torno de `21` no nosso caso, indicando o pre√ßo no in√≠cio do ano.

Para ver qu√£o preciso nosso modelo √©, podemos prever os pre√ßos em um conjunto de dados de teste, e ent√£o medir o qu√£o pr√≥ximas est√£o nossas previs√µes dos valores esperados. Isso pode ser feito usando a m√©trica de erro m√©dio quadr√°tico (MSE), que √© a m√©dia de todas as diferen√ßas quadr√°ticas entre o valor esperado e o valor previsto.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

Nosso erro parece estar em torno de 2 pontos, o que √© ~17%. Nada muito bom. Outro indicador da qualidade do modelo √© o **coeficiente de determina√ß√£o**, que pode ser obtido assim:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Se o valor for 0, significa que o modelo n√£o leva em conta os dados de entrada, e age como o *pior preditor linear*, que √© simplesmente o valor m√©dio do resultado. O valor 1 significa que podemos prever perfeitamente todas as sa√≠das esperadas. No nosso caso, o coeficiente est√° em torno de 0.06, o que √© bem baixo.

Tamb√©m podemos plotar os dados de teste junto com a linha da regress√£o para ver melhor como a regress√£o funciona no nosso caso:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linear regression" src="../../../../translated_images/pt-BR/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## Regress√£o Polinomial

Outro tipo de Regress√£o Linear √© a Regress√£o Polinomial. Embora √†s vezes haja uma rela√ß√£o linear entre as vari√°veis - quanto maior a ab√≥bora em volume, maior o pre√ßo - √†s vezes essas rela√ß√µes n√£o podem ser representadas por um plano ou uma linha reta.

‚úÖ Aqui est√£o [mais alguns exemplos](https://online.stat.psu.edu/stat501/lesson/9/9.8) de dados que poderiam usar Regress√£o Polinomial

D√™ outra olhada na rela√ß√£o entre Data e Pre√ßo. Esse gr√°fico de dispers√£o parece que necessariamente deveria ser analisado por uma linha reta? Os pre√ßos n√£o podem flutuar? Nesse caso, voc√™ pode tentar regress√£o polinomial.

‚úÖ Polin√¥mios s√£o express√µes matem√°ticas que podem consistir de uma ou mais vari√°veis e coeficientes

A regress√£o polinomial cria uma linha curva para ajustar melhor dados n√£o lineares. No nosso caso, se incluirmos uma vari√°vel `DayOfYear` ao quadrado nos dados de entrada, poderemos ajustar nossos dados com uma curva parab√≥lica, que ter√° um m√≠nimo em um certo ponto dentro do ano.

O Scikit-learn inclui uma √∫til [API pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) para combinar diferentes etapas do processamento de dados. Um **pipeline** √© uma cadeia de **estimadores**. No nosso caso, vamos criar um pipeline que primeiro adiciona caracter√≠sticas polinomiais ao nosso modelo, e depois treina a regress√£o:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Usar `PolynomialFeatures(2)` significa que incluiremos todos os polin√¥mios de segundo grau dos dados de entrada. No nosso caso, isso significar√° apenas `DayOfYear`<sup>2</sup>, mas dado duas vari√°veis de entrada X e Y, isso adicionar√° X<sup>2</sup>, XY e Y<sup>2</sup>. Tamb√©m podemos usar polin√¥mios de grau mais elevado se quisermos.

Pipelines podem ser usados da mesma forma que o objeto original `LinearRegression`, ou seja, podemos `fit` o pipeline, e depois usar `predict` para obter os resultados da previs√£o. Aqui est√° o gr√°fico mostrando os dados de teste e a curva de aproxima√ß√£o:

<img alt="Polynomial regression" src="../../../../translated_images/pt-BR/poly-results.ee587348f0f1f60b.webp" width="50%" />

Usando Regress√£o Polinomial, podemos obter um MSE ligeiramente menor e coeficiente de determina√ß√£o maior, mas n√£o significativamente. Precisamos levar em conta outras caracter√≠sticas!

> Voc√™ pode ver que os pre√ßos m√≠nimos das ab√≥boras s√£o observados mais ou menos perto do Halloween. Como voc√™ pode explicar isso? 

üéÉ Parab√©ns, voc√™ acabou de criar um modelo que pode ajudar a prever o pre√ßo das ab√≥boras para torta. Provavelmente, voc√™ pode repetir o mesmo procedimento para todos os tipos de ab√≥bora, mas isso seria tedioso. Vamos aprender agora como levar a variedade de ab√≥bora em considera√ß√£o no nosso modelo!

## Caracter√≠sticas Categ√≥ricas

No mundo ideal, queremos ser capazes de prever pre√ßos para diferentes variedades de ab√≥bora usando o mesmo modelo. Contudo, a coluna `Variety` √© um pouco diferente de colunas como `Month`, porque cont√©m valores n√£o num√©ricos. Essas colunas s√£o chamadas de **categ√≥ricas**.

[![ML para iniciantes - Predi√ß√µes com regress√£o linear para caracter√≠sticas categ√≥ricas](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML para iniciantes - Predi√ß√µes com regress√£o linear para caracter√≠sticas categ√≥ricas")

> üé• Clique na imagem acima para um v√≠deo curto sobre o uso de caracter√≠sticas categ√≥ricas.

Aqui voc√™ pode ver como o pre√ßo m√©dio depende da variedade:

<img alt="Average price by variety" src="../../../../translated_images/pt-BR/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

Para levar a variedade em considera√ß√£o, primeiro precisamos convert√™-la para forma num√©rica, ou **codific√°-la**. Existem v√°rias formas de fazer isso:

* A simples **codifica√ß√£o num√©rica** construir√° uma tabela das diferentes variedades, e depois substituir√° o nome da variedade por um √≠ndice nessa tabela. Isso n√£o √© a melhor ideia para regress√£o linear, porque a regress√£o linear considera o valor num√©rico real do √≠ndice, e o adiciona ao resultado, multiplicando por algum coeficiente. No nosso caso, a rela√ß√£o entre o n√∫mero do √≠ndice e o pre√ßo √© claramente n√£o linear, mesmo que nos certifiquemos que os √≠ndices est√£o ordenados de algum jeito espec√≠fico.
* A **codifica√ß√£o one-hot** substituir√° a coluna `Variety` por 4 colunas diferentes, uma para cada variedade. Cada coluna conter√° `1` se a linha correspondente corresponder √†quela variedade, e `0` caso contr√°rio. Isso significa que haver√° quatro coeficientes na regress√£o linear, um para cada variedade de ab√≥bora, respons√°veis pelo "pre√ßo inicial" (ou melhor, "pre√ßo adicional") para aquela variedade em particular.

O c√≥digo abaixo mostra como podemos codificar one-hot uma variedade:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Para treinar a regress√£o linear usando a variedade codificada one-hot como entrada, s√≥ precisamos inicializar os dados `X` e `y` corretamente:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

O restante do c√≥digo √© o mesmo que usamos acima para treinar Regress√£o Linear. Se voc√™ tentar, ver√° que o erro m√©dio quadr√°tico √© mais ou menos o mesmo, mas obtemos um coeficiente de determina√ß√£o muito maior (~77%). Para obter previs√µes ainda mais precisas, podemos levar mais caracter√≠sticas categ√≥ricas em considera√ß√£o, assim como caracter√≠sticas num√©ricas, como `Month` ou `DayOfYear`. Para obter um grande array de caracter√≠sticas, podemos usar `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Aqui tamb√©m levamos em conta `City` e tipo de `Package`, o que nos d√° MSE de 2.84 (10%), e determina√ß√£o 0.94!

## Juntando tudo

Para fazer o melhor modelo, podemos usar dados combinados (categ√≥ricos codificados one-hot + num√©ricos) do exemplo acima junto com a Regress√£o Polinomial. Aqui est√° o c√≥digo completo para sua conveni√™ncia:

```python
# preparar dados de treinamento
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# fazer divis√£o treino-teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# configurar e treinar o pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# prever resultados para os dados de teste
pred = pipeline.predict(X_test)

# calcular MSE e determina√ß√£o
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Isso deve nos dar o melhor coeficiente de determina√ß√£o de quase 97%, e MSE=2.23 (~8% de erro de previs√£o).

| Modelo | MSE | Determina√ß√£o |
|-------|-----|---------------|
| `DayOfYear` Linear | 2.77 (17.2%) | 0.07 |
| `DayOfYear` Polinomial | 2.73 (17.0%) | 0.08 |
| `Variety` Linear | 5.24 (19.7%) | 0.77 |
| Todas as caracter√≠sticas Linear | 2.84 (10.5%) | 0.94 |
| Todas as caracter√≠sticas Polinomial | 2.23 (8.25%) | 0.97 |

üèÜ Muito bem! Voc√™ criou quatro modelos de Regress√£o em uma li√ß√£o, e melhorou a qualidade do modelo para 97%. Na se√ß√£o final sobre Regress√£o, voc√™ aprender√° sobre Regress√£o Log√≠stica para determinar categorias. 

---
## üöÄDesafio

Teste v√°rias vari√°veis diferentes neste notebook para ver como a correla√ß√£o corresponde √† precis√£o do modelo.

## [Quiz p√≥s-aula](https://ff-quizzes.netlify.app/en/ml/)

## Revis√£o & Autoestudo

Nesta li√ß√£o aprendemos sobre Regress√£o Linear. Existem outros tipos importantes de Regress√£o. Leia sobre as t√©cnicas Stepwise, Ridge, Lasso e Elasticnet. Um bom curso para estudar e aprender mais √© o [curso de Aprendizado Estat√≠stico da Stanford](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## Tarefa

[Construa um Modelo](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informa√ß√µes cr√≠ticas, recomenda-se tradu√ß√£o profissional humana. N√£o nos responsabilizamos por quaisquer equ√≠vocos ou interpreta√ß√µes erradas decorrentes do uso desta tradu√ß√£o.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->