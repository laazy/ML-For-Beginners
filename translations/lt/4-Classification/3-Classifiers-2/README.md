# VirtuvÄ—s klasifikatoriai 2

Å ioje antroje klasifikacijos pamokoje jÅ«s iÅ¡nagrinÄ—site daugiau bÅ«dÅ³, kaip klasifikuoti skaitmeninius duomenis. Taip pat suÅ¾inosite apie pasekmes, pasirenkant vienÄ… klasifikatoriÅ³ vietoj kito.

## [PrieÅ¡ paskaitos testas](https://ff-quizzes.netlify.app/en/ml/)

### Reikalavimai prieÅ¡ pradedant

Tarkime, kad jÅ«s jau baigÄ—te ankstesnes pamokas ir turite iÅ¡valytÄ… duomenÅ³ rinkinÄ¯ savo `data` aplanke, pavadintÄ… _cleaned_cuisines.csv_ Å¡iame 4 pamokÅ³ aplanko Å¡aknyje.

### ParuoÅ¡imas

Mes Ä¯kÄ—lÄ—me jÅ«sÅ³ _notebook.ipynb_ failÄ… su iÅ¡valytais duomenimis ir padalinome juos Ä¯ X ir y duomenÅ³ rinkinius, paruoÅ¡tus modelio kÅ«rimo procesui.

## Klasifikacijos Å¾emÄ—lapis

AnksÄiau suÅ¾inojote apie Ä¯vairias galimybes klasifikuoti duomenis naudodami Microsoft sukurtÄ… pagalbinÄ¯ lapÄ…. Scikit-learn siÅ«lo panaÅ¡Å³, bet detalesnÄ¯ pagalbinÄ¯ lapÄ…, kuris gali dar labiau padÄ—ti susiaurinti jÅ«sÅ³ estimatorius (kitas klasifikatoriÅ³ pavadinimas):

![ML Map from Scikit-learn](../../../../translated_images/lt/map.e963a6a51349425a.webp)
> Patarimas: [apsilankykite Å¡iame Å¾emÄ—lapyje internete](https://scikit-learn.org/stable/tutorial/machine_learning_map/) ir spustelÄ—kite kelio taÅ¡kus, kad perskaitytumÄ—te dokumentacijÄ….

### Planas

Å is Å¾emÄ—lapis labai naudingas, kai jÅ«s aiÅ¡kiai suprantate savo duomenis, nes galite â€Å¾ingsniuotiâ€œ po jo kelius iki sprendimo:

- Turime >50 pavyzdÅ¾iÅ³
- Norime prognozuoti kategorijÄ…
- Turime paÅ¾ymÄ—tus duomenis
- Turime maÅ¾iau nei 100 tÅ«kst. pavyzdÅ¾iÅ³
- âœ¨ Galime pasirinkti Linear SVC
- Jei tai neveikia, kadangi turime skaitmeninius duomenis
    - Galime iÅ¡bandyti âœ¨ KNeighbors klasifikatoriÅ³
      - Jei ir tai neveikia, iÅ¡bandykite âœ¨ SVC ir âœ¨ Ensemble klasifikatorius

Tai labai naudingas kelias, kurio verta laikytis.

## UÅ¾duotis - padalinkite duomenis

Sekdami Å¡iuo keliu, turÄ—tume pradÄ—ti nuo reikalingÅ³ bibliotekÅ³ importavimo.

1. Importuokite reikiamas bibliotekas:

    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    import numpy as np
    ```

1. Padalinkite treniruoÄiÅ³ ir testinius duomenis:

    ```python
    X_train, X_test, y_train, y_test = train_test_split(cuisines_features_df, cuisines_label_df, test_size=0.3)
    ```

## Linear SVC klasifikatorius

Paramos vektoriÅ³ klasterizacija (SVC) yra ParamÅ³ vektoriÅ³ maÅ¡inÅ³ (Support-Vector machines) Å¡eimos maÅ¡ininio mokymosi metodas (daugiau apie juos skaitykite Å¾emiau). Å iame metode galite pasirinkti â€branduolÄ¯â€œ (kernel), kuris nusprendÅ¾ia, kaip klasterizuoti etiketes. Parametras â€Câ€œ reiÅ¡kia â€reguliarizacijÄ…â€œ, kuri reguliuoja parametrÅ³ Ä¯takÄ…. Branduolys gali bÅ«ti vienas iÅ¡ [keliÅ³](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC); Äia mes nustatome jÄ¯ Ä¯ â€linearâ€œ, kad naudotume linijinÄ¯ SVC. TikimybÄ— pagal nutylÄ—jimÄ… yra â€falseâ€œ; Äia jÄ… nustatome Ä¯ â€trueâ€œ, kad gautume tikimybiÅ³ Ä¯vertinimus. Atsitiktinumo bÅ«sena nustatyta Ä¯ â€0â€œ, kad duomenys bÅ«tÅ³ permaiÅ¡yti ir gautume tikimybes.

### UÅ¾duotis - pritaikykite linijinÄ¯ SVC

PradÄ—kite kurdami klasifikatoriÅ³ masyvÄ…. JÄ¯ palaipsniui papildysite, kai testuosime.

1. PradÄ—kite nuo Linear SVC:

    ```python
    C = 10
    # Sukurkite skirtingus klasifikatorius.
    classifiers = {
        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
    }
    ```

2. Apmokykite savo modelÄ¯ naudodami Linear SVC ir atspausdinkite ataskaitÄ…:

    ```python
    n_classifiers = len(classifiers)
    
    for index, (name, classifier) in enumerate(classifiers.items()):
        classifier.fit(X_train, np.ravel(y_train))
    
        y_pred = classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
        print(classification_report(y_test,y_pred))
    ```

    Rezultatas gana geras:

    ```output
    Accuracy (train) for Linear SVC: 78.6% 
                  precision    recall  f1-score   support
    
         chinese       0.71      0.67      0.69       242
          indian       0.88      0.86      0.87       234
        japanese       0.79      0.74      0.76       254
          korean       0.85      0.81      0.83       242
            thai       0.71      0.86      0.78       227
    
        accuracy                           0.79      1199
       macro avg       0.79      0.79      0.79      1199
    weighted avg       0.79      0.79      0.79      1199
    ```

## K-Neighbors klasifikatorius

K-Neighbors priklauso â€kaimynÅ³â€œ (neighbors) Å¡eimai maÅ¡ininio mokymosi metodÅ³, kuriuos galima naudoti tiek priÅ¾iÅ«rimam, tiek nepriÅ¾iÅ«rimam mokymuisi. Å iame metode sukuriamas iÅ¡ anksto apibrÄ—Å¾tas taÅ¡kÅ³ skaiÄius, o duomenys surenkami aplink Å¡iuos taÅ¡kus, kad bÅ«tÅ³ galima prognozuoti bendrines etiketes duomenims.

### UÅ¾duotis - pritaikykite K-Neighbors klasifikatoriÅ³

Ankstesnis klasifikatorius buvo geras ir gerai veikÄ— su duomenimis, bet galbÅ«t galime pasiekti geresnÄ¯ tikslumÄ…. IÅ¡bandykite K-Neighbors klasifikatoriÅ³.

1. Ä®traukite eilutÄ™ Ä¯ savo klasifikatoriÅ³ masyvÄ… (po Linear SVC elemento pridÄ—kite kablelÄ¯):

    ```python
    'KNN classifier': KNeighborsClassifier(C),
    ```

    Rezultatas Å¡iek tiek prastesnis:

    ```output
    Accuracy (train) for KNN classifier: 73.8% 
                  precision    recall  f1-score   support
    
         chinese       0.64      0.67      0.66       242
          indian       0.86      0.78      0.82       234
        japanese       0.66      0.83      0.74       254
          korean       0.94      0.58      0.72       242
            thai       0.71      0.82      0.76       227
    
        accuracy                           0.74      1199
       macro avg       0.76      0.74      0.74      1199
    weighted avg       0.76      0.74      0.74      1199
    ```

    âœ… SuÅ¾inokite apie [K-Neighbors](https://scikit-learn.org/stable/modules/neighbors.html#neighbors)

## Support Vector klasifikatorius

ParamÅ³ vektoriÅ³ klasifikatoriai yra maÅ¡ininio mokymosi metodÅ³, skirtÅ³ klasifikacijai ir regresijai, Å¡eimos dalis [ParamÅ³ vektoriÅ³ maÅ¡inÅ³](https://wikipedia.org/wiki/Support-vector_machine) (SVM). SVM â€Å¾emÄ—lapiuoja treniruoÄiÅ³ pavyzdÅ¾ius Ä¯ erdvÄ—s taÅ¡kusâ€œ, kad maksimaliai padidintÅ³ atstumÄ… tarp dviejÅ³ kategorijÅ³. Tolimesni duomenys taip pat Å¾emÄ—lapiuojami Ä¯ Å¡iÄ… erdvÄ™, kad bÅ«tÅ³ galima prognozuoti jÅ³ kategorijas.

### UÅ¾duotis - pritaikykite Support Vector klasifikatoriÅ³

Pabandykime gauti Å¡iek tiek geresnÄ¯ tikslumÄ… su Support Vector klasifikatoriumi.

1. Po K-Neighbors elemento pridÄ—kite kablelÄ¯ ir tada pridÄ—kite Å¡iÄ… eilutÄ™:

    ```python
    'SVC': SVC(),
    ```

    Rezultatas gana geras!

    ```output
    Accuracy (train) for SVC: 83.2% 
                  precision    recall  f1-score   support
    
         chinese       0.79      0.74      0.76       242
          indian       0.88      0.90      0.89       234
        japanese       0.87      0.81      0.84       254
          korean       0.91      0.82      0.86       242
            thai       0.74      0.90      0.81       227
    
        accuracy                           0.83      1199
       macro avg       0.84      0.83      0.83      1199
    weighted avg       0.84      0.83      0.83      1199
    ```

    âœ… SuÅ¾inokite apie [Support-Vectors](https://scikit-learn.org/stable/modules/svm.html#svm)

## Ensemble klasifikatoriai

Sekime keliÄ… iki galo, nors ankstesnis testas buvo gana geras. IÅ¡bandykime kai kuriuos â€Ensemble klasifikatoriusâ€œ, ypatingai Random Forest ir AdaBoost:

```python
  'RFST': RandomForestClassifier(n_estimators=100),
  'ADA': AdaBoostClassifier(n_estimators=100)
```

Rezultatas labai geras, ypaÄ Random Forest atveju:

```output
Accuracy (train) for RFST: 84.5% 
              precision    recall  f1-score   support

     chinese       0.80      0.77      0.78       242
      indian       0.89      0.92      0.90       234
    japanese       0.86      0.84      0.85       254
      korean       0.88      0.83      0.85       242
        thai       0.80      0.87      0.83       227

    accuracy                           0.84      1199
   macro avg       0.85      0.85      0.84      1199
weighted avg       0.85      0.84      0.84      1199

Accuracy (train) for ADA: 72.4% 
              precision    recall  f1-score   support

     chinese       0.64      0.49      0.56       242
      indian       0.91      0.83      0.87       234
    japanese       0.68      0.69      0.69       254
      korean       0.73      0.79      0.76       242
        thai       0.67      0.83      0.74       227

    accuracy                           0.72      1199
   macro avg       0.73      0.73      0.72      1199
weighted avg       0.73      0.72      0.72      1199
```

âœ… SuÅ¾inokite apie [Ensemble klasifikatorius](https://scikit-learn.org/stable/modules/ensemble.html)

Å is maÅ¡ininio mokymosi metodas â€jungia keliÅ³ baziniÅ³ estimatoriÅ³ prognozesâ€œ, kad pagerintÅ³ modelio kokybÄ™. MÅ«sÅ³ pavyzdyje naudojome Atsitiktinius MedÅ¾ius (Random Trees) ir AdaBoost.

- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest) â€“ vidurkinimo metodas, kuris kuria â€miÅ¡kÄ…â€œ â€sprendimÅ³ medÅ¾iÅ³â€œ, Ä¯terptÅ³ su atsitiktinumu, kad bÅ«tÅ³ iÅ¡vengta perdavimo. n_estimators parametras nurodo medÅ¾iÅ³ skaiÄiÅ³.

- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) pritaiko klasifikatoriÅ³ duomenÅ³ rinkiniui, po to pritaiko Å¡io klasifikatoriaus kopijas tam paÄiam duomenÅ³ rinkiniui. Jis sutelkia dÄ—mesÄ¯ Ä¯ neteisingai klasifikuotÅ³ elementÅ³ svorius ir koreguoja tinkamumÄ… kitam klasifikatoriui.

---

## ğŸš€IÅ¡Å¡Å«kis

Kiekviena iÅ¡ Å¡iÅ³ technikÅ³ turi daugybÄ™ parametrÅ³, kuriuos galite koreguoti. IÅ¡nagrinÄ—kite kiekvienos numatytuosius parametrus ir pagalvokite, kÄ… jÅ³ pakoregavimas reikÅ¡tÅ³ modelio kokybei.

## [Po paskaitos testas](https://ff-quizzes.netlify.app/en/ml/)

## ApÅ¾valga ir savarankiÅ¡kas mokymasis

Å iose pamokose yra daug terminÅ³, todÄ—l skirkite minutÄ™ perÅ¾iÅ«rÄ—ti [Å¡Ä¯ terminÅ³ sÄ…raÅ¡Ä…](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott)!

## UÅ¾duotis 

[ParametrÅ³ Å¾aidimas](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**AtsakomybÄ—s apribojimas**:
Å is dokumentas buvo iÅ¡verstas naudojant dirbtinio intelekto vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatizuoti vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turi bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojamas profesionalus Å¾mogaus vertimas. Mes neatsakome uÅ¾ bet kokius nesusipratimus ar neteisingus interpretavimus, kylanÄius dÄ—l Å¡io vertimo naudojimo.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->