# Cuisine classifiers 2

T√§ss√§ toisessa luokittelutunnissa tutustut lis√§√§ tapoihin luokitella numeerista dataa. Opit my√∂s, mit√§ seurauksia yhden luokittimen valinnalla on toisen sijaan.

## [Esiluentokoe](https://ff-quizzes.netlify.app/en/ml/)

### Ennakkoedellytys

Oletetaan, ett√§ olet suorittanut edelliset oppitunnit ja sinulla on siivottu aineisto kansiossasi nimelt√§ _cleaned_cuisines.csv_ t√§m√§n nelj√§n oppitunnin kansion juurikansiossa.

### Valmistelut

Olemme ladanneet _notebook.ipynb_-tiedostosi siivotun aineiston kanssa ja jakaneet sen X- ja y-datafreimeiksi, valmiina mallin rakentamisprosessiin.

## Luokittelukartta

Aiemmin opit Microsoftin kikan avulla eri vaihtoehdoista datan luokittelussa. Scikit-learn tarjoaa samanlaisen mutta tarkemman kikan, joka voi auttaa kaventamaan arvioijiasi (toinen nimi luokittimille):

![ML Map from Scikit-learn](../../../../translated_images/fi/map.e963a6a51349425a.webp)
> Vinkki: [vieraile t√§ll√§ kartalla verkossa](https://scikit-learn.org/stable/tutorial/machine_learning_map/) ja klikkaa polkua lukeaksesi dokumentaatiota.

### Suunnitelma

T√§m√§ kartta on hyvin hy√∂dyllinen, kun ymm√§rr√§t datasi selke√§sti, sill√§ voit ‚Äôk√§vell√§‚Äô sen polkuja p√§√§t√∂kseen:

- Meill√§ on yli 50 n√§ytett√§
- Haluamme ennustaa kategorian
- Meill√§ on merkitty√§ dataa
- N√§ytteit√§ on alle 100 000
- ‚ú® Voimme valita Linear SVC:n
- Jos se ei toimi, koska meill√§ on numeerista dataa
    - Voimme kokeilla ‚ú® KNeighbors-luokitinta
      - Jos sek√§√§n ei toimi, kokeile ‚ú® SVC:t√§ ja ‚ú® yhdistelm√§luokittimia

T√§m√§ on hyvin hy√∂dyllinen polku seurata.

## Harjoitus - jaa data

Seuraamalla t√§t√§ polkua, aloitamme tuomalla k√§ytt√∂√∂n tarvittavat kirjastot.

1. Tuo tarvittavat kirjastot:

    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    import numpy as np
    ```

1. Jaa harjoitus- ja testidata:

    ```python
    X_train, X_test, y_train, y_test = train_test_split(cuisines_features_df, cuisines_label_df, test_size=0.3)
    ```

## Linear SVC luokitin

Support-Vector clustering (SVC) kuuluu Support-Vector machines -menetelmien perheeseen (lue lis√§√§ niist√§ alla). T√§ss√§ menetelm√§ss√§ voit valita ‚Äôytimen‚Äô, joka p√§√§tt√§√§, miten tunnisteet ryhmitell√§√§n. ‚ÄôC‚Äô-parametri viittaa ‚Äôregulointiin‚Äô, joka s√§√§telee parametrien vaikutusta. Ydin voi olla yksi [useista](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC); t√§ss√§ asetamme sen ‚Äôlineaariseksi‚Äô varmistaaksemme lineaarisen SVC:n k√§yt√∂n. Todenn√§k√∂isyysasetuksena on oletuksena ‚Äôfalse‚Äô; t√§ss√§ asetamme sen ‚Äôtrue‚Äô saadaksemme todenn√§k√∂isyyksi√§. Asetamme satunnaistilan ‚Äô0‚Äô:ksi, jotta data sekoittuu todenn√§k√∂isyyksien saadessa.

### Harjoitus - k√§yt√§ lineaarista SVC:t√§

Aloita luomalla taulukko luokittimista. Lis√§√§t taulukkoon v√§hitellen testatessasi.

1. Aloita Linear SVC:ll√§:

    ```python
    C = 10
    # Luo erilaisia luokittelijoita.
    classifiers = {
        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
    }
    ```

2. Kouluta mallisi k√§ytt√§en Linear SVC:t√§ ja tulosta raportti:

    ```python
    n_classifiers = len(classifiers)
    
    for index, (name, classifier) in enumerate(classifiers.items()):
        classifier.fit(X_train, np.ravel(y_train))
    
        y_pred = classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
        print(classification_report(y_test,y_pred))
    ```

    Tulokset ovat varsin hyv√§t:

    ```output
    Accuracy (train) for Linear SVC: 78.6% 
                  precision    recall  f1-score   support
    
         chinese       0.71      0.67      0.69       242
          indian       0.88      0.86      0.87       234
        japanese       0.79      0.74      0.76       254
          korean       0.85      0.81      0.83       242
            thai       0.71      0.86      0.78       227
    
        accuracy                           0.79      1199
       macro avg       0.79      0.79      0.79      1199
    weighted avg       0.79      0.79      0.79      1199
    ```

## K-Neighbors luokitin

K-Neighbors kuuluu ‚Äùnaapuri‚Äù-menetelmien koneoppimisperheeseen, jota voidaan k√§ytt√§√§ valvottuun ja valvomattomaan oppimiseen. T√§ss√§ menetelm√§ss√§ luodaan ennalta m√§√§ritelty m√§√§r√§ pisteit√§, joihin data ker√§t√§√§n niin, ett√§ datan yleist√§minen ja tunnisteiden ennustaminen onnistuu.

### Harjoitus - k√§yt√§ K-Neighbors luokitinta

Edellinen luokitin toimi hyvin aineistolla, mutta ehk√§ voimme saada paremman tarkkuuden. Kokeile K-Neighbors-luokitinta.

1. Lis√§√§ rivi luokittimien taulukkoon (lis√§√§ pilkku Linear SVC:n j√§lkeen):

    ```python
    'KNN classifier': KNeighborsClassifier(C),
    ```

    Tulokset ovat hieman heikommat:

    ```output
    Accuracy (train) for KNN classifier: 73.8% 
                  precision    recall  f1-score   support
    
         chinese       0.64      0.67      0.66       242
          indian       0.86      0.78      0.82       234
        japanese       0.66      0.83      0.74       254
          korean       0.94      0.58      0.72       242
            thai       0.71      0.82      0.76       227
    
        accuracy                           0.74      1199
       macro avg       0.76      0.74      0.74      1199
    weighted avg       0.76      0.74      0.74      1199
    ```

    ‚úÖ Lue lis√§√§ [K-Neighborsista](https://scikit-learn.org/stable/modules/neighbors.html#neighbors)

## Support Vector Classifier

Support-Vector -luokittimet kuuluvat [Support-Vector Machine](https://wikipedia.org/wiki/Support-vector_machine) -menetelmien koneoppimisperheeseen, joita k√§ytet√§√§n luokittelu- ja regressioteht√§viss√§. SVM:t ‚Äùkarttavat harjoitusesimerkit pisteiksi avaruuteen‚Äù maksimoidakseen kahden kategorian eron. Seuraavat datat kartoitetaan t√§h√§n avaruuteen, jotta niiden kategoria voidaan ennustaa.

### Harjoitus - k√§yt√§ Support Vector Classifieria

Yritet√§√§n hieman parempaa tarkkuutta Support Vector Classifierilla.

1. Lis√§√§ pilkku K-Neighborsin j√§lkeen ja lis√§√§ sitten t√§m√§ rivi:

    ```python
    'SVC': SVC(),
    ```

    Tulokset ovat varsin hyv√§t!

    ```output
    Accuracy (train) for SVC: 83.2% 
                  precision    recall  f1-score   support
    
         chinese       0.79      0.74      0.76       242
          indian       0.88      0.90      0.89       234
        japanese       0.87      0.81      0.84       254
          korean       0.91      0.82      0.86       242
            thai       0.74      0.90      0.81       227
    
        accuracy                           0.83      1199
       macro avg       0.84      0.83      0.83      1199
    weighted avg       0.84      0.83      0.83      1199
    ```

    ‚úÖ Lue lis√§√§ [Support-Vektoreista](https://scikit-learn.org/stable/modules/svm.html#svm)

## Yhdistelm√§luokittimet

Seurataan polkua aivan loppuun asti, vaikka edellinen testi oli varsin hyv√§. Kokeillaan ‚ÄôYhdistelm√§luokittimia‚Äô, erityisesti Random Forestia ja AdaBoostia:

```python
  'RFST': RandomForestClassifier(n_estimators=100),
  'ADA': AdaBoostClassifier(n_estimators=100)
```

Tulokset ovat eritt√§in hyv√§t, erityisesti Random Forestilla:

```output
Accuracy (train) for RFST: 84.5% 
              precision    recall  f1-score   support

     chinese       0.80      0.77      0.78       242
      indian       0.89      0.92      0.90       234
    japanese       0.86      0.84      0.85       254
      korean       0.88      0.83      0.85       242
        thai       0.80      0.87      0.83       227

    accuracy                           0.84      1199
   macro avg       0.85      0.85      0.84      1199
weighted avg       0.85      0.84      0.84      1199

Accuracy (train) for ADA: 72.4% 
              precision    recall  f1-score   support

     chinese       0.64      0.49      0.56       242
      indian       0.91      0.83      0.87       234
    japanese       0.68      0.69      0.69       254
      korean       0.73      0.79      0.76       242
        thai       0.67      0.83      0.74       227

    accuracy                           0.72      1199
   macro avg       0.73      0.73      0.72      1199
weighted avg       0.73      0.72      0.72      1199
```

‚úÖ Lue lis√§√§ [Yhdistelm√§luokittimista](https://scikit-learn.org/stable/modules/ensemble.html)

T√§m√§ koneoppimismenetelm√§ ‚Äùyhdist√§√§ usean perusarvioijan ennusteet‚Äù parantaakseen mallin laatua. Esimerkiss√§mme k√§ytimme satunnaisia puita ja AdaBoostia. 

- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest), keskiarvomenetelm√§, rakentaa ‚Äômets√§n‚Äô ‚Äôp√§√§t√∂spuita‚Äô, joita satunnaistetaan ylisovittamisen v√§ltt√§miseksi. n_estimators-parametri on puiden lukum√§√§r√§.

- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) sovittaa luokittimen aineistoon ja sovittaa sitten kopioita t√§st√§ luokittimesta samaan aineistoon. Se keskittyy virheellisesti luokiteltujen kohteiden painoihin ja s√§√§t√§√§ seuraavan luokittimen sovitusta korjatakseen ne.

---

## üöÄHaaste

Jokaisella n√§ist√§ menetelmist√§ on suuri m√§√§r√§ parametreja, joita voit s√§√§t√§√§. Tutki kunkin oletusparametreja ja mieti, mit√§ niiden muuttaminen merkitsisi mallin laadulle.

## [J√§lkiluennon koe](https://ff-quizzes.netlify.app/en/ml/)

## Kertaus & Itsen√§inen opiskelu

N√§iss√§ oppitunneissa on paljon ammattisanastoa, joten ota hetki ja kertaile [t√§t√§ listaa](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott) hy√∂dyllisist√§ termeist√§!

## Teht√§v√§ 

[Parametrileikki](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Vastuuvapauslauseke**:
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, automaattiset k√§√§nn√∂kset saattavat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§iskielell√§ tulee pit√§√§ virallisena l√§hteen√§. T√§rkeiss√§ asioissa suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§ johtuvista v√§√§rinymm√§rryksist√§ tai virhetulkinnoista.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->