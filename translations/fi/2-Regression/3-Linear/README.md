# Rakenna regressiomalli Scikit-learnill√§: regressio nelj√§ll√§ tavalla

## Aloittelijan huomautus

Lineaarista regressiota k√§ytet√§√§n, kun haluamme ennustaa **numeraalisen arvon** (esimerkiksi talon hinta, l√§mp√∂tila tai myynti).
Se toimii l√∂yt√§m√§ll√§ suoran viivan, joka parhaiten kuvaa sy√∂tteen ominaisuuksien ja tulosteen v√§lisen suhteen.

T√§ss√§ oppitunnissa keskitymme k√§sitteen ymm√§rt√§miseen ennen kuin tutustumme edistyneempiin regressiotekniikoihin.
![Lineaarinen vs polynominen regressio infograafi](../../../../translated_images/fi/linear-polynomial.5523c7cb6576ccab.webp)
> Infograafi tekij√§lt√§ [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Esiluentokysely](https://ff-quizzes.netlify.app/en/ml/)

> ### [T√§m√§ oppitunti on saatavilla R:ll√§!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Johdanto 

T√§h√§n menness√§ olet tutustunut regressioon n√§yteaineiston avulla, joka on poimittu kurpitsan hinnoitteluaineistosta ja jota k√§yt√§mme koko oppitunnin ajan. Olet my√∂s visualisoinut aineistoa Matplotlibill√§.

Nyt olet valmis sukeltamaan syvemm√§lle ML:n regressioon. Visualisointi auttaa sinua ymm√§rt√§m√§√§n dataa, mutta todellinen koneoppimisen voima tulee _mallien opettamisesta_. Mallit opetetaan historiatiedolla, jotta ne automaattisesti tunnistavat datariippuvuuksia, ja ne mahdollistavat uusien, aiemmin n√§kem√§tt√∂mien tietojen tulosten ennustamisen.

T√§ss√§ oppitunnissa opit lis√§√§ kahdesta regressiotyypist√§: _perus lineaarisesta regressiosta_ ja _polynomiregressiosta_, sek√§ joistakin n√§iden tekniikoiden taustalla olevasta matematiikasta. N√§m√§ mallit mahdollistavat kurpitsahintojen ennustamisen eri sy√∂tetietojen perusteella.

[![ML for beginners - Understanding Linear Regression](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML for beginners - Understanding Linear Regression")

> üé• Klikkaa yll√§ olevaa kuvaa n√§hd√§ksesi lyhyen videokatsauksen lineaarisesta regressiosta.

> T√§m√§n opetussuunnitelman aikana oletamme hyvin v√§h√§isi√§ matemaattisia ennakkotietoja ja pyrimme tekem√§√§n sis√§ll√∂st√§ saavutettavaa eri alojen opiskelijoille, joten kiinnit√§ huomiota huomautuksiin, üßÆ kutsuihin, kaavioihin ja muihin oppimisty√∂kaluihin, jotka tukevat ymm√§rryst√§.

### Esivaatimus

Sinun tulisi nyt olla perehtynyt tutkimamme kurpitsadatan rakenteeseen. Sen l√∂yd√§t ennakkoladattuna ja esipuhdistettuna t√§m√§n oppitunnin _notebook.ipynb_-tiedostosta. Tiedostossa kurpitsan hinta esitet√§√§n bushelilt√§ uudessa dataframessa. Varmista, ett√§ voit ajaa n√§m√§ notebookit Visual Studio Coden kernoissa.

### Valmistelu

Muistutuksena, lataat t√§t√§ dataa voidaksesi esitt√§√§ sille kysymyksi√§.

- Milloin on paras aika ostaa kurpitsoja? 
- Mink√§ hintaisen laatikon minikurpitsoja voin odottaa?
- Kannattaako ostaa puoli-bushelin koreissa vai 1 1/9 bushelin laatikoissa?
Tutkitaanpa dataa lis√§√§.

Edellisess√§ oppitunnissa loit Pandas-dataframen ja t√§ytit sen osalla alkuper√§isest√§ aineistosta, standardoiden hinnat bushelin mukaan. N√§in teit, mutta ker√§sit vain noin 400 datapistett√§ ja vain syys- ja loka-kuukausilta.

Katso t√§ss√§ oppitunnissa mukana olevasta notebookista esiladattu data. Data on esiladattu ja alkuper√§inen hajontakaavio on piirretty, joka n√§ytt√§√§ kuukaustiedot. Voimme ehk√§ saada tarkemman kuvan datan luonteesta puhdistamalla sit√§ lis√§√§.

## Lineaarinen regressioviiva

Kuten opit Oppitunnissa 1, lineaarisen regression tavoitteena on piirt√§√§ viiva, joka:

- **N√§ytt√§√§ muuttujien suhteet**. N√§ytt√§√§ muuttujien v√§lisen suhteen
- **Tekee ennusteita**. Tekee tarkkoja ennusteita siit√§, mihin uusi datapiste asettuu suhteessa viivaan.

On tyypillist√§, ett√§ **v√§hint√§√§n neli√∂iden menetelm√§√§** k√§ytet√§√§n t√§m√§n tyyppisen viivan piirt√§miseen. Termi "v√§hint√§√§n neli√∂iden menetelm√§" viittaa mallin virheen kokonaism√§√§r√§n minimointiin. Jokaisen datapisteen pystysuora et√§isyys (jota kutsutaan residuaaliksi) mitataan todellisen pisteen ja regressioviivan v√§lill√§.

Neli√∂imme n√§m√§ et√§isyydet kahdesta p√§√§syyst√§:

1. **Suuruus ilman suuntaa:** Haluamme k√§sitell√§ -5 virheen kuten +5 virhett√§. Neli√∂iminen tekee kaikista arvoista positiivisia.

2. **Poikkeamien rankaisu:** Neli√∂iminen antaa suuremman painon suurille virheille, pakottaen viivan pysym√§√§n l√§hemp√§n√§ kaukana olevia pisteit√§.

Lis√§√§mme sitten kaikki n√§m√§ neli√∂idyt arvot yhteen. Tavoitteemme on l√∂yt√§√§ tarkka viiva, jossa t√§m√§ summa on pienin mahdollinen ‚Äì siksi nimi "v√§hint√§√§n neli√∂iden menetelm√§".

> **üßÆ N√§yt√§ matematiikka** 
> 
> T√§m√§ viiva, jota kutsutaan _paras sovitusviivaksi_, voidaan esitt√§√§ [yht√§l√∂ll√§](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` on 'selitt√§v√§ muuttuja'. `Y` on 'riippuva muuttuja'. Viivan kulmakerroin on `b` ja `a` on y-akselin leikkauspiste, joka kuvaa `Y`:n arvoa kun `X = 0`.
>
>![laske kulmakerroin](../../../../translated_images/fi/slope.f3c9d5910ddbfcf9.webp)
>
> Lasketaan ensin kulmakerroin `b`. Infograafi tekij√§lt√§ [Jen Looper](https://twitter.com/jenlooper)
>
> Toisin sanoen ja viitaten kurpitsadatan alkuper√§iseen kysymykseen: "ennustetaan kurpitsan hinta per bushel kuukauden mukaan", `X` viittaa hintaan ja `Y` myyntikuukauteen.
>
>![t√§ydenn√§ yht√§l√∂](../../../../translated_images/fi/calculation.a209813050a1ddb1.webp)
>
> Lasketaan Y:n arvo. Jos maksat noin 4 dollaria, t√§ytyy olla huhtikuu! Infograafi tekij√§lt√§ [Jen Looper](https://twitter.com/jenlooper)
>
> Viivan laskennassa on n√§ytett√§v√§ sen kaltevuus, joka riippuu my√∂s leikkauskohdasta, eli siit√§ miss√§ `Y` sijaitsee, kun `X = 0`.
>
> Voit tarkastella n√§iden arvojen laskennan menetelm√§√§ verkkosivulta [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). K√§y my√∂s [t√§ll√§ v√§hint√§√§n neli√∂iden laskimella](https://www.mathsisfun.com/data/least-squares-calculator.html) n√§hd√§ksesi kuinka lukuarvot vaikuttavat viivaan.

## Korrelatiivisuus

Yksi termi, joka on syyt√§ ymm√§rt√§√§, on **korrelaatiokerroin** tietyille X- ja Y-muuttujille. Hajontakuvion avulla voit nopeasti havaita t√§m√§n kertoimen. Kuvio, jossa datapisteet ovat siistiss√§ viivassa, on korkea korrelaatio, mutta kun pisteet ovat hajallaan kaikkialla X:n ja Y:n v√§lill√§, korrelaatio on matala.

Hyv√§ lineaarinen regressiomalli on sellainen, jolla on korkea (l√§hell√§ 1:t√§ eik√§ 0:aa) korrelaatiokerroin, k√§ytett√§ess√§ v√§hint√§√§n neli√∂iden menetelm√§√§ ja regressioviivaa.

‚úÖ Suorita t√§m√§n oppitunnin mukana oleva notebook ja tarkastele Kuukausi vs. Hinta hajontakaaviota. Vaikuttaako kurpitsamyynnin Kuukauden ja Hinnan yhdistelm√§ korkealta vai matalalta korrelaatiolta, silm√§m√§√§r√§isen tulkintasi perusteella? Muuttuuko tulos, jos k√§yt√§t kuukauden sijaan yksityiskohtaisempaa mittaria, esim. *vuorokauden numeroa* (eli kuinka mones p√§iv√§ vuodesta)?

Alla koodissa oletamme, ett√§ olemme puhdistaneet datan ja saaneet dataframen nimelt√§ `new_pumpkins`, joka muistuttaa seuraavaa:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> Koodin dataa puhdistavista toimenpiteist√§ l√∂yd√§t tiedostosta [`notebook.ipynb`](notebook.ipynb). Olemme suorittaneet samat puhdistusvaiheet kuin edellisess√§ oppitunnissa, ja laskeneet `DayOfYear`-sarakkeen seuraavalla lausekkeella:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Nyt kun ymm√§rr√§t lineaarisen regression matematiikan taustalla, luokaamme regressiomalli n√§hd√§ksesi, voimmeko ennustaa, mik√§ kurpitsapakkaus tarjoaa parhaat kurpitsahinnat. Joku, joka ostaa kurpitsoja juhlapuutarhaa varten, haluaa t√§m√§n tiedon optimoidakseen kurpitsapakkaustensa ostot.

## Etsim√§ss√§ korrelaatiota

[![ML for beginners - Looking for Correlation: The Key to Linear Regression](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML for beginners - Looking for Correlation: The Key to Linear Regression")

> üé• Klikkaa yll√§ olevaa kuvaa n√§hd√§ksesi lyhyen videokatsauksen korrelaatiosta.

Edellisest√§ oppitunnista olet todenn√§k√∂isesti n√§hnyt, ett√§ eri kuukausien keskim√§√§r√§inen hinta n√§ytt√§√§ t√§lt√§:

<img alt="Keskiarvo hinta kuukauden mukaan" src="../../../../translated_images/fi/barchart.a833ea9194346d76.webp" width="50%"/>

T√§m√§ viittaa siihen, ett√§ jonkinlaista korrelaatiota pit√§isi olla, ja voimme yritt√§√§ opettaa lineaarisen regressiomallin ennustamaan suhdetta `Month` ja `Price` v√§lill√§, tai `DayOfYear` ja `Price` v√§lill√§. T√§ss√§ on hajontakaavio j√§lkimm√§isest√§ suhteesta:

<img alt="Hajontakaavio hinta vs. vuorokauden numero" src="../../../../translated_images/fi/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" /> 

Katsotaan onko korrelaatiota k√§ytt√§en `corr`-funktiota:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Korrelaation n√§ytt√§√§ olevan melko pieni, -0.15 kuukauden mukaan ja -0.17 vuorokauden numeron mukaan, mutta voi olla toinen t√§rke√§ suhde. N√§ytt√§√§ silt√§, ett√§ hintoja on eri ryhmi√§ eri kurpitsalajikkeille. Vahvistaaksemme t√§t√§ hypoteesia piirret√§√§n kukin kurpitsakategoria eri v√§rill√§. K√§ytt√§m√§ll√§ `ax` parametria `scatter`-funktiossa voimme piirt√§√§ kaikki pisteet samaan kuvaan:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Hajontakaavio hinta vs. vuorokauden numero v√§rikoodattu" src="../../../../translated_images/fi/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" /> 

Tutkimuksemme viittaa siihen, ett√§ lajikkeella on suurempi vaikutus kokonaishintaan kuin varsinaisella myyntip√§iv√§ll√§. N√§emme t√§m√§n pylv√§skaaviosta:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Pylv√§skaavio hinta vs. lajike" src="../../../../translated_images/fi/price-by-variety.744a2f9925d9bcb4.webp" width="50%" /> 

Keskityt√§√§n hetkeksi vain yhteen kurpitsalajikkeeseen, 'pie type', ja katsotaan mik√§ vaikutus p√§iv√§ll√§ on hintaan:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Hajontakaavio Hinta vs Vuorokauden Numero" src="../../../../translated_images/fi/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" /> 

Jos nyt lasketaan korrelaatio `Price` ja `DayOfYear` v√§lill√§ `corr`-funktiolla, saamme noin `-0.27`, mik√§ tarkoittaa, ett√§ ennustemallin opettaminen on j√§rkev√§√§.

> Ennen lineaarisen regressiomallin opettamista on t√§rke√§√§ varmistaa, ett√§ datamme on puhdasta. Lineaarinen regressio ei toimi hyvin puuttuvien arvojen kanssa, joten on j√§rkev√§√§ poistaa kaikki tyhj√§t solut:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Toinen l√§hestymistapa voisi olla t√§ytt√§√§ tyhj√§t arvot vastaavien sarakkeiden keskiarvoilla.

## Yksinkertainen lineaarinen regressio

[![ML for beginners - Linear and Polynomial Regression using Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML for beginners - Linear and Polynomial Regression using Scikit-learn")

> üé• Klikkaa yll√§ olevaa kuvaa n√§hd√§ksesi lyhyen videokatsauksen lineaarisesta ja polynomisesta regressiosta.

Lineaarisen regressiomallin opettamiseen k√§yt√§mme **Scikit-learn** kirjastoa.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Aloitamme erottamalla sy√∂tearvot (ominaisuudet) ja odotetun tuloksen (label) eri numpy-taulukoihin:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Huomaa, ett√§ jouduimme k√§ytt√§m√§√§n `reshape`-toimintoa sy√∂tteess√§, jotta LinearRegression kirjasto ymm√§rt√§√§ sen oikein. Lineaarinen regressio odottaa 2-ulotteista taulukkoa sy√∂tteen√§, jossa jokainen rivi on yksi ominaisuusvektori. Koska meill√§ on vain yksi sy√∂te, tarvitsemme N√ó1 muotoisen taulukon, jossa N on aineiston koko.

Seuraavaksi jaamme aineiston harjoitus- ja testidatoihin, jotta voimme validoida mallimme opettamisen j√§lkeen:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Viimein, mallin opettaminen tapahtuu kahdella rivill√§ koodia. M√§√§rittelemme `LinearRegression`-olion ja sovitamme sen dataamme metodilla `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

`LinearRegression`-objekti sovittamisen (`fit`) j√§lkeen sis√§lt√§√§ kaikki regressiokertoimet, joihin p√§√§see k√§siksi `.coef_`-ominaisuudella. Tapauksessamme on vain yksi kerroin, joka on noin `-0.017`. T√§m√§ tarkoittaa, ett√§ hinnat n√§ytt√§v√§t laskevan hieman ajan my√∂t√§, mutta ei liikaa, noin 2 sentti√§ p√§iv√§ss√§. Voimme my√∂s p√§√§st√§ k√§siksi regressioviivan leikkauspisteeseen Y-akselilla k√§ytt√§m√§ll√§ `lin_reg.intercept_` -arvoa ‚Äì se on tapauksessamme noin `21`, mik√§ osoittaa hinnan vuoden alussa.

N√§hd√§ksemme, kuinka tarkka mallimme on, voimme ennustaa hintoja testiaineistolla ja mitata, kuinka l√§hell√§ ennusteet ovat odotettuja arvoja. T√§m√§ voidaan tehd√§ k√§ytt√§en keskineli√∂virheen (MSE) metriikkaa, joka on kaikkien odotettujen ja ennustettujen arvojen neli√∂llisten erotusten keskiarvo.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

Virheemme n√§ytt√§√§ olevan noin 2 pistett√§, mik√§ on ~17%. Ei kovin hyv√§. Toinen mallin laadun mittari on **selitysaste** (coefficient of determination), joka saadaan seuraavasti:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Jos arvo on 0, se tarkoittaa, ett√§ malli ei ota sy√∂tteit√§ huomioon ja toimii *huonoimpana lineaarisena ennustajana*, joka on yksinkertaisesti tuloksen keskiarvo. Arvo 1 tarkoittaa, ett√§ voimme t√§ydellisesti ennustaa kaikki odotetut tulokset. Meid√§n tapauksessamme kerroin on noin 0.06, mik√§ on melko matala.

Voimme my√∂s piirt√§√§ testidatan yhdess√§ regressioviivan kanssa n√§hd√§ksesi paremmin, miten regressio toimii tapauksessamme:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Lineaarinen regressio" src="../../../../translated_images/fi/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## Polynomiregressio

Toinen lineaarisen regression muoto on polynomiregressio. Vaikka joskus muuttujien v√§lill√§ on lineaarinen suhde ‚Äì mit√§ suurempi kurpitsan tilavuus, sit√§ korkeampi hinta ‚Äì joskus n√§it√§ suhteita ei voi kuvata tasaisena tasona tai suorana viivana.

‚úÖ T√§ss√§ on [lis√§√§ esimerkkej√§](https://online.stat.psu.edu/stat501/lesson/9/9.8) aineistosta, johon voisi soveltaa polynomiregressiota

Katso uudelleen suhdetta P√§iv√§m√§√§r√§n ja Hinnan v√§lill√§. N√§ytt√§√§k√∂ t√§m√§ hajontakuvio silt√§, ett√§ sit√§ tulisi v√§ltt√§m√§tt√§ analysoida suoralla viivalla? Eik√∂ hinnat voi heilahdella? T√§ss√§ tapauksessa voit kokeilla polynomiregressiota.

‚úÖ Polynomit ovat matemaattisia lausekkeita, jotka voivat sis√§lt√§√§ yhden tai useamman muuttujan ja kertoimia

Polynomiregressio luo kaarevan k√§yr√§n sovittamaan paremmin ep√§lineaarista dataa. Jos mukaan otetaan toisen asteen `DayOfYear`-muuttuja sy√∂tt√∂tietoihin, pystymme sovittamaan dataamme paraabelin muotoisen k√§yr√§n, jolla on minimi tietyn pisteen kohdalla vuoden aikana.

Scikit-learn sis√§lt√§√§ hy√∂dyllisen [pipeline-rajapinnan](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) yhdist√§m√§√§n erilaiset data-analyysin vaiheet. **Pipeline** on ketju **estimaattoreita**. Tapauksessamme luomme pipeline:n, joka ensin lis√§√§ polynomiset ominaisuudet malliin ja sitten kouluttaa regression:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

`PolynomialFeatures(2)` tarkoittaa, ett√§ mukaan otetaan kaikki toisen asteen polynomit sy√∂tedatasta. Meid√§n tapauksessamme se tarkoittaa vain `DayOfYear`<sup>2</sup>, mutta jos meill√§ olisi kaksi sy√∂te muuttujaa X ja Y, mukaan tulisi X<sup>2</sup>, XY ja Y<sup>2</sup>. Voimme my√∂s k√§ytt√§√§ korkeampiasteisia polynomeja, jos haluamme.

Pipelinet√§ voi k√§ytt√§√§ samalla tavalla kuin alkuper√§ist√§ `LinearRegression`-objektia, eli voimme `fit`-metodilla sovittaa ja `predict`-metodilla ennustaa tuloksia. T√§ss√§ on kuvaaja, joka n√§ytt√§√§ testidatan ja approksimaatiok√§yr√§n:

<img alt="Polynomiregressio" src="../../../../translated_images/fi/poly-results.ee587348f0f1f60b.webp" width="50%" />

Polynomiregressiolla saamme hieman pienemm√§n MSE:n ja korkeamman selitysasteen, mutta ei merkitt√§v√§sti. Meid√§n t√§ytyy ottaa mukaan muitakin piirteit√§!

> Voit n√§hd√§, ett√§ minimihinnat kurpitsoille ovat jossain halloweenin tienoilla. Miten t√§t√§ voisi selitt√§√§?

üéÉ Onneksi olkoon, loit juuri mallin, joka auttaa ennustamaan kurpitsapiirakan hintoja. Voisit luultavasti toistaa saman prosessin kaikille kurpitsatyypeille, mutta se olisi ty√∂l√§st√§. Opitaan nyt, miten ottaa kurpitsan lajike huomioon mallissa!

## Kategoriset ominaisuudet

Ihannetilanteessa haluamme pysty√§ ennustamaan eri kurpitsalajikkeiden hintoja samalla mallilla. Kuitenkin `Variety`-sarake eroaa esimerkiksi `Month`-sarakkeesta, koska se sis√§lt√§√§ ei-numeerisia arvoja. T√§llaisia sarakkeita kutsutaan **kategorisiksi**.

[![ML aloittelijoille - Kategoriset ominaisuudet ja lineaarinen regressio](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML aloittelijoille - Kategoriset ominaisuudet ja lineaarinen regressio")

> üé• Klikkaa yll√§ olevaa kuvaa n√§hd√§ksesi lyhyen videokatsauksen kategoristen ominaisuuksien k√§yt√∂st√§.

T√§ss√§ n√§et, miten keskim√§√§r√§inen hinta riippuu lajikkeesta:

<img alt="Keskiarvohinta lajikkeittain" src="../../../../translated_images/fi/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

Ottaaksemme lajikkeen huomioon, meid√§n t√§ytyy ensin muuntaa se numeromuotoon eli **koodata** se. On olemassa muutamia tapoja tehd√§ t√§m√§:

* Yksinkertainen **numeroarvokoodaus** luo taulukon eri lajikkeista ja korvaa lajikenimen kyseisen taulukon indeksill√§. T√§m√§ ei ole paras idea lineaarisessa regressiossa, koska lineaariregressio ottaa indeksin numeerisen arvon eik√§ huomioi, ett√§ indeksin numeerinen arvo ei v√§ltt√§m√§tt√§ ole lineaarisesti yhteydess√§ hintaan. Toisin sanoen, lineaarisessa mallissa kerroin kerrotaan indeksiluvulla, mik√§ ei vastaa todellista hintasuhdetta.
* **One-hot-koodaus** korvaa `Variety`-sarakkeen nelj√§ll√§ eri sarakkeella, yhden kullekin lajikkeelle. Kukin sarake sis√§lt√§√§ `1`, jos rivi vastaa kyseist√§ lajiketta, ja `0` muuten. T√§m√§ tarkoittaa, ett√§ lineaariregressiossa on nelj√§ kerrointa, yksi jokaiselle kurpitsalajikkeelle, jotka kuvaavat kunkin lajikkeen "aloitushintaa" (tai pikemminkin "lis√§hintaa").

Alla oleva koodi n√§ytt√§√§, miten lajike voidaan one-hot-koodata:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Jotta voimme kouluttaa lineaariregression k√§ytt√§en one-hot-koodattua lajiketta sy√∂tteen√§, meid√§n pit√§√§ vain alustaa `X` ja `y` oikein:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Muu koodi on sama kuin aiemmin k√§ytt√§m√§mme lineaarisen regression koulutuksessa. Kun kokeilet, huomaat, ett√§ keskineli√∂virhe on suunnilleen sama, mutta selitysaste nousee huomattavasti (~77%). Tarkempia ennusteita saamme ottamalla huomioon lis√§√§ kategorisia piirteit√§ sek√§ numeerisia ominaisuuksia, kuten `Month` tai `DayOfYear`. Yhden suuren ominaisuustaulukon saamiseksi voimme k√§ytt√§√§ `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

T√§ss√§ otamme my√∂s huomioon `City`- ja `Package`-tyypin, mik√§ antaa meille MSE-arvoksi 2.84 (10%) ja selitysasteeksi 0.94!

## Kaiken yhdist√§minen

Parhaan mallin saamiseksi voimme k√§ytt√§√§ yhdistetty√§ (one-hot-koodattua kategorista + numeerista) dataa yll√§olevasta esimerkist√§ yhdess√§ polynomiregression kanssa. T√§ss√§ on koko koodi vaivattomuutta varten:

```python
# aseta harjoitusdata
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# tee opetus- ja testijako
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# m√§√§rit√§ ja kouluta putkisto
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# ennusta tulokset testidatalle
pred = pipeline.predict(X_test)

# laske MSE ja selitysaste
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

T√§m√§ antaa meille parhaan selitysasteen l√§hes 97 % ja MSE:n 2.23 (~8 % ennustevirhe).

| Malli | MSE | Selitysaste |
|-------|-----|-------------|
| `DayOfYear` lineaarinen | 2.77 (17.2%) | 0.07 |
| `DayOfYear` polynomi | 2.73 (17.0%) | 0.08 |
| `Variety` lineaarinen | 5.24 (19.7%) | 0.77 |
| Kaikki ominaisuudet lineaarinen | 2.84 (10.5%) | 0.94 |
| Kaikki ominaisuudet polynomi | 2.23 (8.25%) | 0.97 |

üèÜ Hienoa ty√∂t√§! Loit nelj√§ regressiomallia yhdess√§ oppitunnissa ja paransit mallin laatua 97 prosenttiin. Regressio-aiheen lopussa opit logistisesta regressiosta luokitteluja varten.

---
## üöÄHaaste

Testaa eri muuttujia t√§ss√§ muistikirjassa n√§hd√§ksesi, miten korrelaatio vastaa mallin tarkkuutta.

## [Luentoj√§lkeinen tietovisa](https://ff-quizzes.netlify.app/en/ml/)

## Kertaus & Itsen√§inen opiskelu

T√§ss√§ oppitunnissa opimme lineaarisesta regressiosta. On olemassa my√∂s muita t√§rkeit√§ regressiotyyppej√§. Lue lis√§√§ Stepwise-, Ridge-, Lasso- ja Elasticnet-menetelmist√§. Hyv√§ kurssi opinnoille on [Stanfordin tilastollisen oppimisen kurssi](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## Teht√§v√§

[Rakenna malli](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Vastuuvapauslauseke**:
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Pyrimme tarkkuuteen, mutta huomioithan, ett√§ automaattiset k√§√§nn√∂kset saattavat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§iskielell√§ on virallinen l√§hde. T√§rkeiss√§ asioissa suosittelemme ammattimaista ihmisk√§√§nn√∂st√§. Emme ota vastuuta t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§ aiheutuvista v√§√§rinymm√§rryksist√§ tai virheellisist√§ tulkinnoista.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->