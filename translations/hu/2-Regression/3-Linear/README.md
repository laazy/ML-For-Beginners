# Regresszi√≥s modell √©p√≠t√©se Scikit-learn haszn√°lat√°val: regresszi√≥ n√©gyf√©lek√©ppen

## Kezd≈ë megjegyz√©s

Line√°ris regresszi√≥t akkor haszn√°lunk, amikor egy **numerikus √©rt√©ket** akarunk megj√≥solni (p√©ld√°ul h√°z √°r√°t, h≈ëm√©rs√©kletet vagy √©rt√©kes√≠t√©st).
Ez √∫gy m≈±k√∂dik, hogy megkeresi azt a legjobb egyenest, amely legjobban reprezent√°lja a bemeneti jellemz≈ëk √©s a kimenet k√∂z√∂tti kapcsolatot.

Ebben a leck√©ben a fogalom meg√©rt√©s√©re koncentr√°lunk, miel≈ëtt tov√°bbi, fejlettebb regresszi√≥s technik√°kat vizsg√°ln√°nk.
![Line√°ris vs polinomi√°lis regresszi√≥ infografika](../../../../translated_images/hu/linear-polynomial.5523c7cb6576ccab.webp)
> Infografika k√©sz√≠t≈ëje: [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

> ### [Ez a lecke el√©rhet≈ë R nyelven is!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Bevezet√©s

Eddig megvizsg√°ltad, mi az a regresszi√≥, √©s mint√°kat n√©zt√©l meg a s√ºt≈ët√∂k √°rk√©pz√©si adatb√°zisb√≥l, amelyet az eg√©sz lecke sor√°n fogunk haszn√°lni. Meg is jelen√≠tetted azt a Matplotlib seg√≠ts√©g√©vel.

Most k√©szen √°llsz arra, hogy m√©lyebben bele√°sd magad a regresszi√≥ t√©m√°j√°ba a g√©pi tanul√°shoz. M√≠g a megjelen√≠t√©s lehet≈ëv√© teszi az adatok meg√©rt√©s√©t, a g√©pi tanul√°s val√≥di ereje a _modellek betan√≠t√°s√°ban_ rejlik. A modelleket t√∂rt√©neti adatokon tan√≠tjuk meg, hogy automatikusan megragadj√°k az adatok k√∂z√∂tti √∂sszef√ºgg√©seket, √©s lehet≈ëv√© tegy√©k, hogy √∫j adatokra el≈ërejelz√©seket k√©sz√≠ts√ºnk, melyekkel a modell m√©g nem tal√°lkozott.

Ebben a leck√©ben k√©t regresszi√≥t√≠pust fogsz megismerni: az _alapvet≈ë line√°ris regresszi√≥t_ √©s a _polinomi√°lis regresszi√≥t_, n√©h√°ny matematikai h√°tt√©rrel egy√ºtt. Ezek a modellek lehet≈ëv√© teszik, hogy el≈ëre jelezz√ºk a s√ºt≈ët√∂k √°rakat k√ºl√∂nb√∂z≈ë bemeneti adatok alapj√°n.

[![G√©pi tanul√°s kezd≈ëknek ‚Äì A line√°ris regresszi√≥ meg√©rt√©se](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "G√©pi tanul√°s kezd≈ëknek ‚Äì A line√°ris regresszi√≥ meg√©rt√©se")

> üé• Kattints a fenti k√©pre egy r√∂vid vide√≥s √°ttekint√©s√©rt a line√°ris regresszi√≥r√≥l.

> A tananyag sor√°n minim√°lis matematikai ismerettel sz√°molunk, √©s arra t√∂reksz√ºnk, hogy a m√°s ter√ºletekr≈ël √©rkez≈ë di√°kok sz√°m√°ra is √©rthet≈ë legyen, ez√©rt figyelj a megjegyz√©sekre, üßÆ kiemel√©sekre, √°br√°kra √©s egy√©b tanul√°si seg√©deszk√∂z√∂kre, amelyek seg√≠tik a meg√©rt√©st.

### El≈ëfelt√©tel

M√°r meg kell ismerned a s√ºt≈ët√∂k adatok szerkezet√©t, amelyeket megvizsg√°lunk. Ezek el≈ëre bet√∂ltve √©s megtiszt√≠tva szerepelnek enn√©l a leck√©n√©l az _notebook.ipynb_ f√°jlban. Ebben a f√°jlban a s√ºt≈ët√∂k √°r bushelenk√©nt jelenik meg egy √∫j adatkeretben. Gy≈ëz≈ëdj meg r√≥la, hogy futtatni tudod ezeket a jegyzetf√ºzeteket a Visual Studio Code kerneljeiben.

### El≈ëk√©sz√ºlet

Eml√©keztet≈ë√ºl: ezt az adatot az√©rt t√∂lt√∂d be, hogy k√©rd√©seket tehess fel vele kapcsolatban.

- Mikor a legjobb id≈ë s√ºt≈ët√∂k√∂t v√°s√°rolni?
- Milyen √°rat v√°rhatok mini s√ºt≈ët√∂k√∂k egy csomagj√°ra?
- √ârdemes-e f√©l bushel kos√°rban vagy ink√°bb 1 1/9 bushel dobozban venni?

N√©zz√ºk tov√°bb ezt az adatot.

Az el≈ëz≈ë leck√©ben l√©trehozt√°l egy Pandas adatkeretet, amelybe bet√∂lt√∂tted az eredeti adat√°llom√°ny egy r√©sz√©t √©s egys√©ges√≠tetted az √°rakat bushelenk√©nt. √çgy azonban csak kb. 400 adatpontot nyert√©l, √©s csak a ≈ëszi h√≥napokra vonatkoz√≥an.

N√©zd meg az ebben a leck√©ben el≈ëre bet√∂lt√∂tt adatot a jegyzetf√ºzet mell√©kletek√©nt. Az adat m√°r be van t√∂ltve, √©s egy kezdeti pontdiagram is k√©sz√ºlt a h√≥nap adatainak megjelen√≠t√©s√©re. Tal√°n siker√ºl a tiszt√≠t√°ssal √°rnyaltabb k√©pet kapni az adat term√©szet√©r≈ël.

## Egy line√°ris regresszi√≥s egyenes

Ahogy az 1. leck√©ben tanultad, a line√°ris regresszi√≥s feladat c√©lja egy olyan egyenes √°br√°zol√°sa, amely:

- **Variable kapcsolatok bemutat√°sa**. Megmutatja a v√°ltoz√≥k k√∂z√∂tti kapcsolatot
- **El≈ërejelz√©s k√©sz√≠t√©se**. Pontosan megj√≥solja, hogy az √∫j adatpont hol fog elhelyezkedni az egyeneshez k√©pest.

Tipikusan a **legkisebb n√©gyzetek regresszi√≥** alkalmaz√°sa r√©v√©n rajzolunk ilyen egyenest. A "legkisebb n√©gyzetek" kifejez√©s arra a folyamatra utal, amely sor√°n minimaliz√°ljuk a modell √∂sszes hib√°j√°t. Minden adatpontra megm√©rj√ºk a f√ºgg≈ëleges t√°vols√°got (rezidu√°lis), mely az adott pont √©s a regresszi√≥s egyenes k√∂z√∂tti t√°vols√°g.

Ezeket a t√°vols√°gokat n√©gyzetre emelj√ºk k√©t f≈ë okb√≥l:

1. **Ir√°nyn√°l fontosabb a nagys√°g:** Az a hib√°t, ha -5 vagy +5, egyform√°n kezelj√ºk. A n√©gyzetre emel√©ssel az √∂sszes √©rt√©k pozit√≠vv√° v√°lik.

2. **Elt√©r≈ë √©rt√©kek b√ºntet√©se:** A n√©gyzetre emel√©s nagyobb s√∫lyt ad a nagyobb hib√°knak, √≠gy az egyenes k√∂zelebb ker√ºl az elt√©r≈ë pontokhoz.

Mindezek ut√°n √∂sszeadjuk az √∂sszes n√©gyzetre emelt √©rt√©ket. A c√©l az, hogy megtal√°ljuk azt az egyenest, amelyn√©l ez az √∂sszeg a legkisebb (legkisebb lehets√©ges √©rt√©k) ‚Äì innen ered a "Legkisebb n√©gyzetek" n√©v.

> **üßÆ Mutasd a k√©pletet**
> 
> Ezt az egyenest, az √∫gynevezett _legjobb illeszked√©s≈± egyenest_, a [k√∂vetkez≈ë k√©plettel](https://en.wikipedia.org/wiki/Simple_linear_regression) lehet le√≠rni:
> 
> ```
> Y = a + bX
> ```
>
> `X` a ‚Äômagyar√°z√≥ v√°ltoz√≥‚Äô. `Y` a ‚Äôf√ºgg≈ë v√°ltoz√≥‚Äô. Az egyenes meredeks√©ge `b`, az `a` pedig az y-tengely metszete, vagyis az `Y` √©rt√©ke, amikor `X = 0`.
>
>![a meredeks√©g kisz√°m√≠t√°sa](../../../../translated_images/hu/slope.f3c9d5910ddbfcf9.webp)
>
> El≈ësz√∂r sz√°m√≠tsuk ki a `b` meredeks√©get. Infografika k√©sz√≠t≈ëje: [Jen Looper](https://twitter.com/jenlooper)
>
> M√°s sz√≥val, √©s utalva a s√ºt≈ët√∂k adataink eredeti k√©rd√©s√©re: "j√≥soljuk meg a s√ºt≈ët√∂k bushelenk√©nti √°r√°t h√≥napra lebontva", az `X` az √°rra, az `Y` pedig az √©rt√©kes√≠t√©s h√≥napj√°ra utalna.
>
>![egyenlet kit√∂lt√©se](../../../../translated_images/hu/calculation.a209813050a1ddb1.webp)
>
> Sz√°m√≠tsuk ki az `Y` √©rt√©k√©t. Ha k√∂r√ºlbel√ºl 4 doll√°rt fizetsz, bizony√°ra √°prilis van! Infografika k√©sz√≠t≈ëje: [Jen Looper](https://twitter.com/jenlooper)
>
> A k√©pletnek ki kell sz√°molnia az egyenes meredeks√©g√©t, amely az y-metszett≈ël, azaz att√≥l f√ºgg, hol helyezkedik el az `Y`, amikor `X = 0`.
>
> Megfigyelheted a sz√°m√≠t√°si m√≥dot a [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) weboldalon. L√°togass el erre [a legkisebb n√©gyzetek sz√°mol√≥g√©pre](https://www.mathsisfun.com/data/least-squares-calculator.html), hogy l√°sd, mik√©nt befoly√°solj√°k az √©rt√©kek az egyenest.

## Korrel√°ci√≥

Van m√©g egy fogalom, amit meg kell √©rteni, ez pedig a **korrel√°ci√≥s egy√ºtthat√≥** az adott X √©s Y v√°ltoz√≥k k√∂z√∂tt. Egy pontdiagram seg√≠ts√©g√©vel k√∂nnyen meg lehet jelen√≠teni ezt az egy√ºtthat√≥t. Ha az adatpontok sz√©pen egy egyenes ment√©n helyezkednek el, akkor magas a korrel√°ci√≥, ha pedig mindenfel√© sz√≥r√≥dnak az X √©s Y k√∂z√∂tt, akkor alacsony.

Egy j√≥ line√°ris regresszi√≥s modell lesz olyan, amely szoros (ink√°bb 1-hez, semmint 0-hoz k√∂zeli) korrel√°ci√≥s egy√ºtthat√≥val rendelkezik a legkisebb n√©gyzetek regresszi√≥s m√≥dszer√©vel egy regresszi√≥s vonal mellett.

‚úÖ Futtasd az ehhez a leck√©hez mell√©kelt jegyzetf√ºzetet, √©s n√©zd meg a H√≥nap-Ar√°ny pontdiagramot. A h√≥nap √©s √°ra k√∂z√∂tti kapcsolat a s√ºt≈ët√∂k √©rt√©kes√≠t√©s√©ben szerinted magas vagy alacsony korrel√°ci√≥j√∫ a pontdiagram vizu√°lis √©rtelmez√©se alapj√°n? V√°ltozik-e, ha finomabb m√©rt√©ket haszn√°lunk a `Month` helyett, p√©ld√°ul az *√©v napja* (az √©v els≈ë napja √≥ta eltelt napok sz√°ma) alapj√°n?

A lentebbi k√≥dban azt felt√©telezz√ºk, hogy megtiszt√≠tottuk az adatot, √©s l√©trej√∂tt egy `new_pumpkins` nev≈± adatkeret az al√°bbi m√≥don:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> Az adatok tiszt√≠t√°s√°ra szolg√°l√≥ k√≥d el√©rhet≈ë a [`notebook.ipynb`](notebook.ipynb) f√°jlban. Ugyanazokat a tiszt√≠t√°si l√©p√©seket hajtottuk v√©gre, mint az el≈ëz≈ë leck√©ben, √©s kisz√°moltuk a `DayOfYear` oszlopot a k√∂vetkez≈ë kifejez√©s haszn√°lat√°val:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Most, hogy √©rted a line√°ris regresszi√≥ m√∂g√∂tti matematik√°t, hozzunk l√©tre egy regresszi√≥s modellt, hogy megn√©zz√ºk, meg tudjuk-e j√≥solni, melyik s√ºt≈ët√∂k csomag √°raz√°sa lesz a legjobb. Valaki, aki √ºnnepi s√ºt≈ët√∂k d√≠szhez v√°s√°rol, ezt az inform√°ci√≥t haszn√°lhatja v√°s√°rl√°si d√∂nt√©sek optimaliz√°l√°s√°ra.

## Korrel√°ci√≥ keres√©se

[![G√©pi tanul√°s kezd≈ëknek - Korrel√°ci√≥ keres√©se: Az √∂sszef√ºgg√©s a line√°ris regresszi√≥ kulcsa](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "G√©pi tanul√°s kezd≈ëknek - Korrel√°ci√≥ keres√©se: Az √∂sszef√ºgg√©s a line√°ris regresszi√≥ kulcsa")

> üé• Kattints a fenti k√©pre egy r√∂vid vide√≥s √°ttekint√©s√©rt a korrel√°ci√≥r√≥l.

Az el≈ëz≈ë leck√©b≈ël val√≥sz√≠n≈±leg l√°ttad, hogy k√ºl√∂nb√∂z≈ë h√≥napok √°tlag√°ra √≠gy n√©z ki:

<img alt="√Åtlag√°r h√≥napokra bontva" src="../../../../translated_images/hu/barchart.a833ea9194346d76.webp" width="50%"/>

Ez arra utal, hogy van √∂sszef√ºgg√©s, √©s megpr√≥b√°lhatjuk a line√°ris regresszi√≥ modellt betan√≠tani a `Month` √©s az √°r, vagy a `DayOfYear` √©s az √°r kapcsolat√°ra. Az al√°bbi sz√≥r√°ster√ºlet √°br√°n ennek az ut√≥bbinak a viszonya l√°that√≥:

<img alt="Sz√≥r√°sdiagram az √°r √©s az √©v napja szerint" src="../../../../translated_images/hu/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" /> 

N√©zz√ºk meg, milyen korrel√°ci√≥ van az `corr` f√ºggv√©nnyel:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

√ögy t≈±nik, a korrel√°ci√≥ meglehet≈ësen kicsi, -0,15 a h√≥nap szerint √©s -0,17 az √©v napja szerint, de lehet, hogy egy m√°sik fontos √∂sszef√ºgg√©s is l√©tezik. √ögy t≈±nik, k√ºl√∂nb√∂z≈ë √°rkateg√≥ri√°k l√©teznek a s√ºt≈ët√∂k fajt√°k szerint. Ennek meger≈ës√≠t√©s√©re pr√≥b√°ljuk meg az egyes s√ºt≈ët√∂k kateg√≥ri√°kat k√ºl√∂nb√∂z≈ë sz√≠nnel megjelen√≠teni. Az `ax` param√©ter √°tad√°s√°val a `scatter` f√ºggv√©nynek az √∂sszes pont ugyanazon a grafikonon √°br√°zolhat√≥:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Sz√≥r√°sdiagram az √°r √©s az √©v napja sz√≠nk√≥dolt" src="../../../../translated_images/hu/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" /> 

Vizsg√°latunk arra utal, hogy a fajta hat√°sa nagyobb az √°r √∂ssz√©rt√©k√©re, mint az elad√°si id≈ëpont√©. Ezt egy oszlopdiagramon is megl√°thatjuk:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Oszlopdiagram az √°r √©s a fajta szerint" src="../../../../translated_images/hu/price-by-variety.744a2f9925d9bcb4.webp" width="50%" /> 

Most egyel≈ëre csak az egyik s√ºt≈ët√∂k fajt√°ra, a ‚Äôpie type‚Äô-ra koncentr√°ljunk, √©s n√©zz√ºk meg, milyen hat√°sa van az √©rt√©kes√≠t√©si d√°tumnak az √°rra:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Sz√≥r√°sdiagram az √°r √©s az √©v napja a pie t√≠pus√∫ s√ºt≈ët√∂kn√©l" src="../../../../translated_images/hu/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" /> 

Ha most kisz√°moljuk a korrel√°ci√≥t a `Price` √©s a `DayOfYear` k√∂z√∂tt az `corr` f√ºggv√©nnyel, az kb. `-0.27` lesz ‚Äì ami azt jelenti, hogy √©rdemes predikt√≠v modellt tan√≠tani.

> A line√°ris regresszi√≥s modell betan√≠t√°sa el≈ëtt fontos, hogy az adatunk tiszta legyen. A line√°ris regresszi√≥ nem m≈±k√∂dik j√≥l hi√°nyz√≥ √©rt√©kekkel, ez√©rt √©rdemes megszabadulni minden hi√°nyz√≥ adatt√≥l:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

M√°sik megk√∂zel√≠t√©s lehet a hi√°nyz√≥ √©rt√©kek kit√∂lt√©se az adott oszlop √°tlag√°val.

## Egyszer≈± line√°ris regresszi√≥

[![G√©pi tanul√°s kezd≈ëknek ‚Äì Line√°ris √©s polinomi√°lis regresszi√≥ Scikit-learn seg√≠ts√©g√©vel](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "G√©pi tanul√°s kezd≈ëknek ‚Äì Line√°ris √©s polinomi√°lis regresszi√≥ Scikit-learn seg√≠ts√©g√©vel")

> üé• Kattints a fenti k√©pre egy r√∂vid vide√≥s √°ttekint√©s√©rt a line√°ris √©s polinomi√°lis regresszi√≥r√≥l.

Line√°ris regresszi√≥s modell√ºnk betan√≠t√°s√°hoz a **Scikit-learn** k√∂nyvt√°rat fogjuk haszn√°lni.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Kezdj√ºk azzal, hogy a bemeneti √©rt√©keket (jellemz≈ëk) √©s a v√°rt kimenetet (c√≠mk√©t) k√ºl√∂n numpy t√∂mb√∂kbe rendezz√ºk:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Figyeld meg, hogy a bemeneti adatokat √°t kellett form√°znunk (`reshape`), hogy a Line√°ris Regresszi√≥ csomag helyesen √©rtelmezze ≈ëket. A Line√°ris Regresszi√≥ 2D t√∂mb√∂t v√°r bemenetk√©nt, ahol a t√∂mb minden sora egy vektor a bemeneti jellemz≈ëkr≈ël. Mivel nek√ºnk csak egy bemenet√ºnk van, egy N√ó1 alak√∫ t√∂mb√∂t kell adnunk, ahol N az adatk√©szlet m√©rete.

Ezut√°n az adatot sz√©t kell osztanunk tan√≠t√≥ √©s teszt halmazokra, hogy a modell betan√≠t√°sa ut√°n tesztelni tudjuk azt:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

V√©g√ºl a line√°ris regresszi√≥s modell t√©nyleges betan√≠t√°sa csak k√©t k√≥d sorb√≥l √°ll. L√©trehozzuk a `LinearRegression` objektumot, √©s betan√≠tjuk az adatainkra a `fit` met√≥dussal:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

A `LinearRegression` objektum a `fit`-el√©s ut√°n tartalmazza a regresszi√≥ √∂sszes egy√ºtthat√≥j√°t, amelyekhez a `.coef_` tulajdons√°gon kereszt√ºl f√©rhet√ºnk hozz√°. Eset√ºnkben csak egy egy√ºtthat√≥ van, amely k√∂r√ºlbel√ºl `-0.017` k√∂r√ºl v√°rhat√≥. Ez azt jelenti, hogy az √°rak id≈ëvel kiss√© cs√∂kkennek, de nem sokat, k√∂r√ºlbel√ºl 2 centet naponta. A regresszi√≥ Y tengellyel val√≥ metsz√©spontj√°t is el√©rhetj√ºk a `lin_reg.intercept_` seg√≠ts√©g√©vel ‚Äì ami n√°lunk k√∂r√ºlbel√ºl `21` lesz, jelezve az √©v eleji √°rat.

Hogy l√°ssuk, mennyire pontos a modell√ºnk, el≈ësz√∂r √°rakat j√≥solhatunk egy tesztadat√°llom√°nyra, majd m√©rhetj√ºk, mennyire k√∂zel vannak a j√≥slataink a v√°rt √©rt√©kekhez. Ezt elv√©gezhetj√ºk a n√©gyzetes hib√°k √°tlag√°val, azaz a mean square error (MSE) metrik√°val, amely a v√°rt √©s a j√≥solt √©rt√©kek k√∂z√∂tti n√©gyzetes k√ºl√∂nbs√©gek √°tlaga.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

A hib√°nk √∫gy t≈±nik, k√∂r√ºlbel√ºl 2 pont k√∂r√ºl van, ami kb. 17%. Nem t√∫l j√≥. A modell min≈ës√©g√©nek m√°sik mutat√≥ja az **elsz√°mol√°si egy√ºtthat√≥ (coefficient of determination)**, amely √≠gy hat√°rozhat√≥ meg:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Ha az √©rt√©k 0, az azt jelenti, hogy a modell nem veszi figyelembe a bemeneti adatokat, √©s a *legrosszabb line√°ris prediktork√©nt* m≈±k√∂dik, ami egyszer≈±en az eredm√©ny √°tlag√©rt√©ke. Ha az √©rt√©k 1, az azt jelenti, hogy t√∂k√©letesen tudjuk el≈ëre jelezni az √∂sszes v√°rt kimenetet. N√°lunk az egy√ºtthat√≥ k√∂r√ºlbel√ºl 0.06, ami el√©g alacsony.

A tesztadatokat a regresszi√≥s vonallal egy√ºtt is √°br√°zolhatjuk, hogy jobban l√°ssuk, hogyan m≈±k√∂dik a regresszi√≥ a mi eset√ºnkben:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linear regression" src="../../../../translated_images/hu/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## Polinomi√°lis regresszi√≥

A line√°ris regresszi√≥nak egy m√°sik t√≠pusa a polinomi√°lis regresszi√≥. B√°r n√©ha a v√°ltoz√≥k k√∂zt line√°ris kapcsolat van ‚Äì p√©ld√°ul min√©l nagyobb a t√∂k t√©rfogata, ann√°l magasabb az √°ra ‚Äì, el≈ëfordul, hogy ezek a kapcsolatok nem √°br√°zolhat√≥k egy s√≠kkal vagy egyenes vonallal.

‚úÖ Itt vannak [tov√°bbi p√©ld√°k](https://online.stat.psu.edu/stat501/lesson/9/9.8) olyan adatokra, amelyek eset√©ben polinomi√°lis regresszi√≥ alkalmazhat√≥.

N√©zz√ºk √∫jra az √∂sszef√ºgg√©st a D√°tum √©s az √År k√∂z√∂tt. Ez a pontfelh≈ë √∫gy t≈±nik, hogy felt√©tlen√ºl egyenes vonallal kellene elemezni? Nem ingadozhatnak az √°rak? Ilyen esetben polinomi√°lis regresszi√≥t pr√≥b√°lhatunk ki.

‚úÖ A polinomok matematikai kifejez√©sek, amelyek egy vagy t√∂bb v√°ltoz√≥b√≥l √©s egy√ºtthat√≥b√≥l √°llhatnak.

A polinomi√°lis regresszi√≥ egy g√∂rbe vonalat hoz l√©tre, hogy jobban illeszkedjen a nemline√°ris adatokra. N√°lunk, ha a bemeneti adathoz hozz√°adjuk a `DayOfYear` n√©gyzet√©t, akkor k√©pesek lesz√ºnk egy parabolikus g√∂rb√©vel illeszteni az adatokat, amelynek a minimuma az √©v egy bizonyos pontj√°n lesz.

A Scikit-learn tartalmaz egy hasznos [pipeline API-t](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline), amellyel t√∂bb adatfeldolgoz√°si l√©p√©st is √∂sszekapcsolhatunk. Egy **pipeline** egy **becsl≈ëk** l√°ncolata. N√°lunk egy olyan pipeline-t hozunk l√©tre, amely el≈ësz√∂r polinomi√°lis jellemz≈ëket ad a modellhez, majd megtan√≠tja a regresszi√≥t:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

A `PolynomialFeatures(2)` azt jelenti, hogy az √∂sszes m√°sodfok√∫ polinomot belefoglaljuk a bemeneti adatb√≥l. N√°lunk ez csak a `DayOfYear`<sup>2</sup>-t jelenti, de ha k√©t bemeneti v√°ltoz√≥nk, X √©s Y van, akkor hozz√°ad√≥dik az X<sup>2</sup>, XY √©s Y<sup>2</sup> is. Term√©szetesen magasabb fok√∫ polinomokat is haszn√°lhatunk.

A pipeline-okat ugyan√∫gy haszn√°lhatjuk, mint az eredeti `LinearRegression` objektumot, azaz illeszthetj√ºk (`fit`), majd j√≥solhatunk (`predict`). Itt l√°that√≥ egy grafikon a tesztadatokr√≥l √©s az illesztett g√∂rb√©r≈ël:

<img alt="Polynomial regression" src="../../../../translated_images/hu/poly-results.ee587348f0f1f60b.webp" width="50%" />

A polinomi√°lis regresszi√≥val kiss√© alacsonyabb MSE-t √©s magasabb determin√°ci√≥t √©rhet√ºnk el, de nem jelent≈ësen. M√°s jellemz≈ëket is figyelembe kell venn√ºnk!

> L√°that√≥, hogy a legkisebb t√∂k√°rak nagyj√°b√≥l Halloween k√∂rny√©k√©n vannak. Hogyan magyar√°zn√°d ezt?

üéÉ Gratul√°lok, most egy olyan modellt hozt√°l l√©tre, amely seg√≠t el≈ëre jelezni a s√ºt≈ët√∂k √°r√°t. Val√≥sz√≠n≈±leg ugyan√≠gy elj√°rhatsz az √∂sszes t√∂kfajt√°val, de ez f√°raszt√≥ lenne. Tanuljuk meg ink√°bb, hogyan vegy√ºk figyelembe a t√∂kfajt√°t a modell√ºnkben!

## Kategoriz√°lt jellemz≈ëk

Az ide√°lis vil√°gban ugyanazzal a modellel szeretn√©nk k√ºl√∂nb√∂z≈ë t√∂kfajt√°k √°r√°t el≈ërejelezni. Azonban a `Variety` oszlop elt√©r a `Month`-hoz hasonl√≥ oszlopokt√≥l, mert nem numerikus √©rt√©keket tartalmaz. Az ilyen oszlopokat **kateg√≥ri√°lis** oszlopoknak nevezz√ºk.

[![ML kezd≈ëknek - Kateg√≥ri√°s jellemz≈ëk el≈ërejelz√©se line√°ris regresszi√≥val](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML kezd≈ëknek - Kateg√≥ri√°s jellemz≈ëk el≈ërejelz√©se line√°ris regresszi√≥val")

> üé• Kattints a fenti k√©pre a kateg√≥ri√°s jellemz≈ëk haszn√°lat√°t bemutat√≥ r√∂vid vide√≥√©rt.

Itt l√°that√≥, hogyan f√ºgg az √°tlag√°r a fajt√°t√≥l:

<img alt="Average price by variety" src="../../../../translated_images/hu/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

Ahhoz, hogy figyelembe vegy√ºk a fajt√°t, el≈ësz√∂r numerikus form√°ba kell konvert√°lnunk, vagyis **k√≥dolnunk** kell. T√∂bbf√©lek√©ppen tehetj√ºk ezt meg:

* Az egyszer≈± **numerikus k√≥dol√°s** egy t√°bl√°zatot √©p√≠t a k√ºl√∂nb√∂z≈ë fajt√°kr√≥l, majd a fajta nev√©t a t√°bl√°zatbeli index√©re cser√©li. Ez nem a legjobb m√≥dszer a line√°ris regresszi√≥hoz, mert a regresszi√≥ a k√≥d numerikus √©rt√©k√©t veszi figyelembe √©s szorozza egy√ºtthat√≥val. N√°lunk az indexsz√°m √©s az √°r k√∂z√∂tti kapcsolat egy√©rtelm≈±en nem line√°ris, m√©g akkor sem, ha az indexeket valamilyen sorrendbe rendezz√ºk.
* A **one-hot k√≥dol√°s** a `Variety` oszlop helyett 4 k√ºl√∂n oszlopot k√©sz√≠t, egyet-egyet minden fajt√°ra. Minden oszlop 1-et tartalmaz, ha az adott sor az adott fajta, k√ºl√∂nben 0-t. Ez azt jelenti, hogy a line√°ris regresszi√≥ban n√©gy egy√ºtthat√≥ lesz, egy-egy minden t√∂kfajt√°ra, amelyek az adott fajta "kiindul√≥ √°ra" (vagy ink√°bb "tov√°bbi √°ra") felel≈ës.

Az al√°bbi k√≥d megmutatja, hogyan k√≥dolhatjuk one-hot m√≥dszerrel a fajt√°t:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

A line√°ris regresszi√≥ tan√≠t√°s√°hoz one-hot k√≥dolt fajt√°val csak helyesen kell inicializ√°lni az `X` √©s `y` adatokat:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

A tov√°bbi k√≥d ugyanaz, mint amit fent haszn√°ltunk a line√°ris regresszi√≥ tan√≠t√°s√°hoz. Ha kipr√≥b√°lod, azt l√°tod, hogy a n√©gyzetes hiba nagyj√°b√≥l ugyanaz marad, viszont j√≥val magasabb lesz az elsz√°mol√°si egy√ºtthat√≥ (~77%). M√©g pontosabb j√≥slatokhoz t√∂bb kateg√≥ri√°s jellemz≈ët is bevonhatunk, illetve numerikus jellemz≈ëket, p√©ld√°ul `Month` vagy `DayOfYear`. Az adatok egyetlen nagy t√∂mbb√© egyes√≠t√©s√©hez a `join` haszn√°lhat√≥:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Itt a `City`-t √©s a `Package` t√≠pust is figyelembe vessz√ºk, ami MSE=2.84 (10%) √©s determin√°ci√≥=0.94 eredm√©nyt ad!

## √ñsszeilleszt√©s

A legjobb modell elk√©sz√≠t√©s√©hez egyes√≠thetj√ºk a fent eml√≠tett (one-hot k√≥dolt kateg√≥ri√°s + numerikus) adatokat a polinomi√°lis regresszi√≥val. Al√°bb a teljes k√≥d a k√∂nnyebb haszn√°lathoz:

```python
# tr√©ning adatok el≈ëk√©sz√≠t√©se
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# tanul√≥-teszt adatfeloszt√°s v√©grehajt√°sa
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# pipeline be√°ll√≠t√°sa √©s betan√≠t√°sa
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# tesztadatokra t√∂rt√©n≈ë eredm√©nyj√≥sl√°s
pred = pipeline.predict(X_test)

# MSE √©s determin√°ci√≥ kisz√°m√≠t√°sa
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Ez megk√∂zel√≠t≈ëleg 97% determin√°ci√≥s egy√ºtthat√≥t √©s MSE=2.23 (~8% el≈ërejelz√©si hib√°t) eredm√©nyez.

| Modell | MSE | Determin√°ci√≥ |
|-------|-----|---------------|
| `DayOfYear` line√°ris | 2.77 (17.2%) | 0.07 |
| `DayOfYear` polinomi√°lis | 2.73 (17.0%) | 0.08 |
| `Variety` line√°ris | 5.24 (19.7%) | 0.77 |
| Minden jellemz≈ë line√°ris | 2.84 (10.5%) | 0.94 |
| Minden jellemz≈ë polinomi√°lis | 2.23 (8.25%) | 0.97 |

üèÜ Sz√©p munka! Ebben a leck√©ben n√©gy regresszi√≥s modellt hozt√°l l√©tre, √©s a modellmin≈ës√©get 97%-ra jav√≠tottad. Az utols√≥ fejezetben a logisztikus regresszi√≥r√≥l tanulsz majd a kateg√≥ri√°k meghat√°roz√°s√°hoz.

---
## üöÄKih√≠v√°s

Tesztelj k√ºl√∂nb√∂z≈ë v√°ltoz√≥kat ebben a jegyzetf√ºzetben, √©s figyeld meg, hogyan t√ºkr√∂z≈ëdik a korrel√°ci√≥ a modell pontoss√°g√°ban.

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

## √ñsszefoglal√°s √©s √∂n√°ll√≥ tanul√°s

Ebben a leck√©ben a line√°ris regresszi√≥val ismerkedt√ºnk meg. M√°s fontos regresszi√≥t√≠pusok is vannak. Olvass a Stepwise, Ridge, Lasso √©s Elasticnet technik√°kr√≥l. Egy aj√°nlott tanfolyam tov√°bbi ismeretek√©rt a [Stanford Statisztikai Tanul√°s kurzusa](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## Feladat

[Modell k√©sz√≠t√©se](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Nyilatkozat**:
Jelen dokumentumot az AI ford√≠t√≥ szolg√°ltat√°s, a [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel ford√≠tottuk. B√°r igyeksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az anyanyelv√©n tekintend≈ë hivatalos forr√°snak. Kritikus inform√°ci√≥k eset√©n professzion√°lis, emberi ford√≠t√°st javaslunk. Nem v√°llalunk felel≈ëss√©get a ford√≠t√°s haszn√°lat√°b√≥l ered≈ë f√©lre√©rt√©sek√©rt vagy f√©lre√©rtelmez√©sek√©rt.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->