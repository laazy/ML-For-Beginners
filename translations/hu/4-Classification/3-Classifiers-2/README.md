# Konyha oszt√°lyoz√≥k 2

Ebben a m√°sodik oszt√°lyoz√°si leck√©ben t√∂bbf√©le m√≥dot fogsz felfedezni a numerikus adatok oszt√°lyoz√°s√°ra. Megtanulod azt is, milyen k√∂vetkezm√©nyekkel j√°r, ha az egyik oszt√°lyoz√≥t v√°lasztod a m√°sik helyett.

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

### El≈ëfelt√©tel

Felt√©telezz√ºk, hogy elv√©gezted az el≈ëz≈ë leck√©ket, √©s van egy megtiszt√≠tott adat√°llom√°nyod a `data` mapp√°dban, amelynek neve _cleaned_cuisines.csv_, √©s amely ebben a 4 leck√©s mapp√°ban a gy√∂k√©rk√∂nyvt√°rban tal√°lhat√≥.

### El≈ëk√©sz√ºlet

Bet√∂lt√∂tt√ºk a _notebook.ipynb_ f√°jlodat a megtiszt√≠tott adatokkal, √©s sz√©tv√°lasztottuk ≈ëket X √©s y adatt√°bl√°kra, k√©szen az modell√©p√≠t√©si folyamathoz.

## Egy oszt√°lyoz√°si t√©rk√©p

Kor√°bban megtanultad a k√ºl√∂nf√©le opci√≥kat, amikor az adatokat oszt√°lyozod, a Microsoft csal√≥lapja alapj√°n. A Scikit-learn hasonl√≥, de m√©g r√©szletesebb csal√≥lapot k√≠n√°l, amely tov√°bb seg√≠thet lesz≈±k√≠teni az oszt√°lyoz√≥idat (m√°s sz√≥val becsl≈ëket):

![ML Map from Scikit-learn](../../../../translated_images/hu/map.e963a6a51349425a.webp)
> Tipp: [l√°togasd meg ezt a t√©rk√©pet online](https://scikit-learn.org/stable/tutorial/machine_learning_map/) √©s kattints a √∫tvonalon, hogy elolvasd a dokument√°ci√≥t.

### A terv

Ez a t√©rk√©p nagyon hasznos, amikor tiszt√°n √©rted az adataidat, mert v√©gig tudsz "s√©t√°lni" az √∫tvonalain a d√∂nt√©shez:

- T√∂bb mint 50 mint√°nk van
- Kateg√≥ri√°t akarunk el≈ëre jelezni
- C√≠mk√©zett adataink vannak
- Kevesebb mint 100 ezer mint√°nk van
- ‚ú® V√°laszthatunk egy Linear SVC-t
- Ha ez nem m≈±k√∂dik, mivel numerikus adataink vannak
    - Pr√≥b√°lkozhatunk egy ‚ú® KNeighbors oszt√°lyoz√≥val
      - Ha ez sem m≈±k√∂dik, pr√≥b√°ljuk az ‚ú® SVC-t √©s az ‚ú® Ensemble oszt√°lyoz√≥kat

Ez egy nagyon hasznos √∫tvonal, amit k√∂vetni lehet.

## Gyakorlat - oszd meg az adatokat

Ezt az utat k√∂vetve kezdj√ºk azzal, hogy import√°lunk n√©h√°ny k√∂nyvt√°rat haszn√°latra.

1. Import√°ld a sz√ºks√©ges k√∂nyvt√°rakat:

    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    import numpy as np
    ```

1. Oszd meg a tan√≠t√≥ √©s teszt adataidat:

    ```python
    X_train, X_test, y_train, y_test = train_test_split(cuisines_features_df, cuisines_label_df, test_size=0.3)
    ```

## Line√°ris SVC oszt√°lyoz√≥

A Support-Vector clustering (SVC) a Support-Vector g√©pek csal√°dj√°ba tartozik, amelyek g√©pi tanul√°si technik√°k (r√≥luk lentebb tanulhatsz). Ebben a m√≥dszerben v√°laszthatsz "kernelt", hogy meghat√°rozd, hogyan csoportos√≠tod a c√≠mk√©ket. A 'C' param√©ter a "regulariz√°ci√≥ra" utal, amely szab√°lyozza a param√©terek hat√°s√°t. A kernel lehet egyik [t√∂bb](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC); itt 'line√°ris'-ra √°ll√≠tottuk, hogy line√°ris SVC-t haszn√°ljunk. A val√≥sz√≠n≈±s√©get alap√©rtelmez√©s szerint 'hamis'-ra √°ll√≠tja; itt 'igaz'-ra √°ll√≠tjuk, hogy val√≥sz√≠n≈±s√©gi becsl√©seket kapjunk. A random √°llapotot '0'-ra √°ll√≠tjuk a v√©letlenszer≈± sorbarendez√©shez, hogy val√≥sz√≠n≈±s√©geket kapjunk.

### Gyakorlat - alkalmazz line√°ris SVC-t

Kezdj egy oszt√°lyoz√≥ t√∂mb l√©trehoz√°s√°val. Fokozatosan fogsz b≈ëv√≠teni ezen a t√∂mb√∂n, ahogy tesztel√ºnk.

1. Kezdd egy Linear SVC-vel:

    ```python
    C = 10
    # K√ºl√∂nb√∂z≈ë oszt√°lyoz√≥k l√©trehoz√°sa.
    classifiers = {
        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
    }
    ```

2. Tan√≠tsd meg a modelled a Line√°ris SVC-vel, √©s nyomtass ki egy jelent√©st:

    ```python
    n_classifiers = len(classifiers)
    
    for index, (name, classifier) in enumerate(classifiers.items()):
        classifier.fit(X_train, np.ravel(y_train))
    
        y_pred = classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
        print(classification_report(y_test,y_pred))
    ```

    Az eredm√©ny eg√©szen j√≥:

    ```output
    Accuracy (train) for Linear SVC: 78.6% 
                  precision    recall  f1-score   support
    
         chinese       0.71      0.67      0.69       242
          indian       0.88      0.86      0.87       234
        japanese       0.79      0.74      0.76       254
          korean       0.85      0.81      0.83       242
            thai       0.71      0.86      0.78       227
    
        accuracy                           0.79      1199
       macro avg       0.79      0.79      0.79      1199
    weighted avg       0.79      0.79      0.79      1199
    ```

## K-legk√∂zelebbi szomsz√©d oszt√°lyoz√≥

A K-Neighbors a "szomsz√©dok" csal√°dj√°ba tartozik a g√©pi tanul√°si m√≥dszereknek, amelyeket fel√ºgyelt √©s fel√ºgyelet n√©lk√ºli tanul√°sra is lehet haszn√°lni. Ebben a m√≥dszerben el≈ëre meghat√°rozott sz√°m√∫ pontot hozunk l√©tre, √©s az adatokat ezek k√∂r√© gy≈±jtj√ºk √∂ssze, hogy √°ltal√°nos√≠tott c√≠mk√©ket tudjunk el≈ëre jelezni.

### Gyakorlat - alkalmazd a K-Neighbors oszt√°lyoz√≥t

Az el≈ëz≈ë oszt√°lyoz√≥ j√≥l m≈±k√∂d√∂tt az adatokkal, de tal√°n jobb pontoss√°got √©rhet√ºnk el. Pr√≥b√°ld ki a K-Neighbors oszt√°lyoz√≥t.

1. Adj egy sort az oszt√°lyoz√≥ t√∂mbh√∂z (tegy√©l vessz≈ët a Linear SVC elem ut√°n):

    ```python
    'KNN classifier': KNeighborsClassifier(C),
    ```

    Az eredm√©ny kiss√© rosszabb:

    ```output
    Accuracy (train) for KNN classifier: 73.8% 
                  precision    recall  f1-score   support
    
         chinese       0.64      0.67      0.66       242
          indian       0.86      0.78      0.82       234
        japanese       0.66      0.83      0.74       254
          korean       0.94      0.58      0.72       242
            thai       0.71      0.82      0.76       227
    
        accuracy                           0.74      1199
       macro avg       0.76      0.74      0.74      1199
    weighted avg       0.76      0.74      0.74      1199
    ```

    ‚úÖ Ismerd meg a [K-Neighbors](https://scikit-learn.org/stable/modules/neighbors.html#neighbors) m√≥dszert

## Support Vector oszt√°lyoz√≥

A Support-Vector oszt√°lyoz√≥k a [Support-Vector g√©p](https://wikipedia.org/wiki/Support-vector_machine) csal√°dj√°ba tartoz√≥ g√©pi tanul√°si m√≥dszerek, amelyeket oszt√°lyoz√°si √©s regresszi√≥s feladatokra haszn√°lnak. Az SVM-ek "lek√©pezik a tan√≠t√≥ p√©ld√°kat t√©rbeli pontokk√°", hogy maximaliz√°lj√°k a k√©t kateg√≥ria k√∂z√∂tti t√°vols√°got. A k√©s≈ëbbi adatokat ebbe a t√©rbe k√©pezik le, hogy el≈ëre jelezhess√©k a kateg√≥ri√°jukat.

### Gyakorlat - alkalmazz Support Vector oszt√°lyoz√≥t

Pr√≥b√°ljunk egy kicsit jobb pontoss√°got egy Support Vector oszt√°lyoz√≥val.

1. Tegy√©l egy vessz≈ët a K-Neighbors elem ut√°n, majd add hozz√° ezt a sort:

    ```python
    'SVC': SVC(),
    ```

    Az eredm√©ny eg√©szen j√≥!

    ```output
    Accuracy (train) for SVC: 83.2% 
                  precision    recall  f1-score   support
    
         chinese       0.79      0.74      0.76       242
          indian       0.88      0.90      0.89       234
        japanese       0.87      0.81      0.84       254
          korean       0.91      0.82      0.86       242
            thai       0.74      0.90      0.81       227
    
        accuracy                           0.83      1199
       macro avg       0.84      0.83      0.83      1199
    weighted avg       0.84      0.83      0.83      1199
    ```

    ‚úÖ Ismerd meg a [Support-Vektorokat](https://scikit-learn.org/stable/modules/svm.html#svm)

## Ensemble oszt√°lyoz√≥k

K√∂vess√ºk az utat a legv√©gig, m√©g akkor is, ha az el≈ëz≈ë teszt nagyon j√≥ volt. Pr√≥b√°ljunk ki n√©h√°ny 'Ensemble' oszt√°lyoz√≥t, konkr√©tan Random Forest-et √©s AdaBoost-ot:

```python
  'RFST': RandomForestClassifier(n_estimators=100),
  'ADA': AdaBoostClassifier(n_estimators=100)
```

Az eredm√©ny nagyon j√≥, k√ºl√∂n√∂sen a Random Forest eset√©n:

```output
Accuracy (train) for RFST: 84.5% 
              precision    recall  f1-score   support

     chinese       0.80      0.77      0.78       242
      indian       0.89      0.92      0.90       234
    japanese       0.86      0.84      0.85       254
      korean       0.88      0.83      0.85       242
        thai       0.80      0.87      0.83       227

    accuracy                           0.84      1199
   macro avg       0.85      0.85      0.84      1199
weighted avg       0.85      0.84      0.84      1199

Accuracy (train) for ADA: 72.4% 
              precision    recall  f1-score   support

     chinese       0.64      0.49      0.56       242
      indian       0.91      0.83      0.87       234
    japanese       0.68      0.69      0.69       254
      korean       0.73      0.79      0.76       242
        thai       0.67      0.83      0.74       227

    accuracy                           0.72      1199
   macro avg       0.73      0.73      0.72      1199
weighted avg       0.73      0.72      0.72      1199
```

‚úÖ Ismerd meg az [Ensemble oszt√°lyoz√≥kat](https://scikit-learn.org/stable/modules/ensemble.html)

Ez a g√©pi tanul√°si m√≥dszer "t√∂bb alapbecsl≈ë el≈ërejelz√©s√©t kombin√°lja", hogy jav√≠tsa a modell min≈ës√©g√©t. A p√©ld√°nkban Random Trees-t √©s AdaBoost-ot haszn√°ltunk.

- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest), egy √°tlagol√≥ m√≥dszer, amely 'd√∂nt√©si f√°kat' √©p√≠t fel v√©letlenszer≈±s√©g be√©p√≠t√©s√©vel az t√∫lilleszked√©s elker√ºl√©se √©rdek√©ben. Az n_estimators param√©tert a f√°k sz√°m√°ra √°ll√≠tjuk.

- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) egy olyan oszt√°lyoz√≥t illeszt egy adat√°llom√°nyra, majd ugyanazt t√∂bbsz√∂r illeszti, f√≥kusz√°lva a helytelen√ºl oszt√°lyozott mint√°k s√∫lyaira, √©s a k√∂vetkez≈ë oszt√°lyoz√≥ jav√≠t√°s√°hoz √°ll√≠tja az illeszked√©st.

---

## üöÄKih√≠v√°s

Mindegyik technik√°nak nagy sz√°m√∫ param√©tere van, amelyeket m√≥dos√≠thatsz. Kutass ut√°na ezek alap√©rtelmezett param√©tereinek, √©s gondold √°t, milyen hat√°suk lehet ezek m√≥dos√≠t√°s√°nak a modell min≈ës√©g√©re.

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

## √Åttekint√©s & √∂n√°ll√≥ tanul√°s

Sok szakkifejez√©s tal√°lhat√≥ ezekben a leck√©kben, √≠gy sz√°nj egy percet arra, hogy √°ttekintsd [ezt a list√°t](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott) a hasznos terminol√≥gi√°kr√≥l!

## Feladat

[Param√©ter j√°t√©k](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Nyilatkozat**:
Ez a dokumentum az [Co-op Translator](https://github.com/Azure/co-op-translator) AI ford√≠t√≥ szolg√°ltat√°s√°val k√©sz√ºlt. B√°r az pontoss√°gra t√∂reksz√ºnk, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok tartalmazhatnak hib√°kat vagy pontatlans√°gokat. Az eredeti dokumentum az anyanyelv√©n tekintend≈ë hivatalos forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis, emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get az ebb≈ël ered≈ë f√©lre√©rt√©sek√©rt vagy f√©lre√©rtelmez√©sek√©rt.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->