<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ba0f6e1019351351c8ee4c92867b6a0b",
  "translation_date": "2025-08-29T21:15:20+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "ru"
}
-->
# Постскриптум: Отладка моделей машинного обучения с использованием компонентов панели ответственного ИИ

## [Тест перед лекцией](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## Введение

Машинное обучение оказывает влияние на нашу повседневную жизнь. Искусственный интеллект проникает в самые важные системы, которые затрагивают нас как личностей и как общество, включая здравоохранение, финансы, образование и трудоустройство. Например, системы и модели участвуют в ежедневных процессах принятия решений, таких как медицинская диагностика или выявление мошенничества. В результате, наряду с развитием ИИ и его ускоренным внедрением, растут общественные ожидания и ужесточается регулирование. Мы постоянно сталкиваемся с ситуациями, где системы ИИ не оправдывают ожиданий, создают новые вызовы, а правительства начинают регулировать решения на основе ИИ. Поэтому важно анализировать эти модели, чтобы обеспечить справедливые, надежные, инклюзивные, прозрачные и подотчетные результаты для всех.

В этом курсе мы рассмотрим практические инструменты, которые можно использовать для оценки наличия проблем с ответственным ИИ в модели. Традиционные методы отладки машинного обучения обычно основаны на количественных расчетах, таких как агрегированная точность или средняя ошибка. Представьте, что может произойти, если данные, которые вы используете для создания моделей, не включают определенные демографические группы, такие как раса, пол, политические взгляды, религия, или непропорционально представляют такие группы. А что, если вывод модели интерпретируется как благоприятствующий одной из демографических групп? Это может привести к чрезмерному или недостаточному представлению этих чувствительных групп, что вызовет проблемы справедливости, инклюзивности или надежности модели. Еще один фактор — модели машинного обучения часто воспринимаются как "черные ящики", что затрудняет понимание и объяснение причин их предсказаний. Все это — вызовы, с которыми сталкиваются специалисты по данным и разработчики ИИ, если у них нет подходящих инструментов для отладки и оценки справедливости или надежности модели.

В этом уроке вы узнаете, как отлаживать свои модели с помощью:

- **Анализа ошибок**: определение областей в распределении данных, где модель имеет высокий уровень ошибок.
- **Обзора модели**: проведение сравнительного анализа различных когорт данных для выявления различий в метриках производительности модели.
- **Анализа данных**: исследование областей, где может быть избыточное или недостаточное представление данных, что может склонить модель к предпочтению одной демографической группы перед другой.
- **Важности признаков**: понимание, какие признаки влияют на предсказания модели на глобальном или локальном уровне.

## Предварительные требования

В качестве предварительного требования, пожалуйста, ознакомьтесь с материалом [Инструменты ответственного ИИ для разработчиков](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![Gif об инструментах ответственного ИИ](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## Анализ ошибок

Традиционные метрики производительности моделей, используемые для измерения точности, в основном основаны на расчетах правильных и неправильных предсказаний. Например, определение того, что модель точна в 89% случаев с потерей ошибки 0.001, может считаться хорошим результатом. Однако ошибки часто распределены неравномерно в вашем исходном наборе данных. Вы можете получить показатель точности модели 89%, но обнаружить, что в некоторых областях данных модель ошибается в 42% случаев. Последствия таких паттернов ошибок для определенных групп данных могут привести к проблемам справедливости или надежности. Важно понимать, где модель работает хорошо, а где нет. Области данных с высоким количеством ошибок могут оказаться важными демографическими группами.

![Анализ и отладка ошибок модели](../../../../translated_images/ea-error-distribution.117452e1177c1dd84fab2369967a68bcde787c76c6ea7fdb92fcf15d1fce8206.ru.png)

Компонент "Анализ ошибок" на панели RAI показывает, как распределены ошибки модели по различным когортам с помощью визуализации в виде дерева. Это полезно для выявления признаков или областей, где уровень ошибок в вашем наборе данных высок. Видя, откуда исходят основные неточности модели, вы можете начать исследовать первопричину. Вы также можете создавать когорты данных для анализа. Эти когорты помогают в процессе отладки, чтобы определить, почему производительность модели хороша в одной когорте, но ошибочна в другой.

![Анализ ошибок](../../../../translated_images/ea-error-cohort.6886209ea5d438c4daa8bfbf5ce3a7042586364dd3eccda4a4e3d05623ac702a.ru.png)

Визуальные индикаторы на дереве помогают быстрее находить проблемные области. Например, чем темнее оттенок красного цвета у узла дерева, тем выше уровень ошибок.

Тепловая карта — еще одна функция визуализации, которую пользователи могут использовать для исследования уровня ошибок с использованием одной или двух характеристик, чтобы найти факторы, способствующие ошибкам модели в целом наборе данных или когорт.

![Тепловая карта анализа ошибок](../../../../translated_images/ea-heatmap.8d27185e28cee3830c85e1b2e9df9d2d5e5c8c940f41678efdb68753f2f7e56c.ru.png)

Используйте анализ ошибок, когда вам нужно:

* Глубже понять, как распределены ошибки модели по набору данных и по нескольким входным и характеристическим измерениям.
* Разбить агрегированные метрики производительности, чтобы автоматически обнаружить ошибочные когорты и определить целевые шаги по их устранению.

## Обзор модели

Оценка производительности модели машинного обучения требует целостного понимания ее поведения. Это можно достичь, рассматривая более одной метрики, такой как уровень ошибок, точность, полнота, точность или MAE (средняя абсолютная ошибка), чтобы выявить различия в метриках производительности. Одна метрика производительности может выглядеть отлично, но неточности могут быть выявлены в другой метрике. Кроме того, сравнение метрик для выявления различий в целом наборе данных или когорт помогает понять, где модель работает хорошо, а где нет. Это особенно важно для анализа производительности модели среди чувствительных и нечувствительных характеристик (например, раса пациента, пол или возраст), чтобы выявить потенциальную несправедливость модели. Например, обнаружение того, что модель более ошибочна в когорте с чувствительными характеристиками, может выявить потенциальную несправедливость модели.

Компонент "Обзор модели" на панели RAI помогает не только анализировать метрики производительности представления данных в когорте, но и дает пользователям возможность сравнивать поведение модели в разных когортах.

![Когорты набора данных — обзор модели на панели RAI](../../../../translated_images/model-overview-dataset-cohorts.dfa463fb527a35a0afc01b7b012fc87bf2cad756763f3652bbd810cac5d6cf33.ru.png)

Функция анализа на основе характеристик компонента позволяет пользователям сузить подгруппы данных в рамках определенной характеристики, чтобы выявить аномалии на более детальном уровне. Например, панель имеет встроенный интеллект для автоматической генерации когорт для выбранной пользователем характеристики (например, *"time_in_hospital < 3"* или *"time_in_hospital >= 7"*). Это позволяет пользователю изолировать определенную характеристику из более крупной группы данных, чтобы увидеть, является ли она ключевым фактором ошибочных результатов модели.

![Когорты характеристик — обзор модели на панели RAI](../../../../translated_images/model-overview-feature-cohorts.c5104d575ffd0c80b7ad8ede7703fab6166bfc6f9125dd395dcc4ace2f522f70.ru.png)

Компонент "Обзор модели" поддерживает два класса метрик различий:

**Различия в производительности модели**: Эти метрики рассчитывают различия в значениях выбранной метрики производительности между подгруппами данных. Вот несколько примеров:

* Различия в уровне точности
* Различия в уровне ошибок
* Различия в точности
* Различия в полноте
* Различия в средней абсолютной ошибке (MAE)

**Различия в уровне выбора**: Эта метрика включает разницу в уровне выбора (благоприятного предсказания) между подгруппами. Примером этого является различие в уровне одобрения кредитов. Уровень выбора означает долю точек данных в каждом классе, классифицированных как 1 (в бинарной классификации), или распределение значений предсказаний (в регрессии).

## Анализ данных

> "Если достаточно долго пытать данные, они признаются во всем" — Рональд Коуз

Это утверждение звучит экстремально, но правда в том, что данные могут быть манипулированы для подтверждения любого вывода. Такая манипуляция иногда происходит непреднамеренно. Как люди, мы все имеем предвзятость, и часто сложно осознанно понять, когда вы вводите предвзятость в данные. Обеспечение справедливости в ИИ и машинном обучении остается сложной задачей.

Данные — это огромная "слепая зона" для традиционных метрик производительности моделей. Вы можете иметь высокие показатели точности, но это не всегда отражает скрытую предвзятость данных, которая может быть в вашем наборе данных. Например, если в наборе данных сотрудников 27% женщин занимают руководящие должности в компании, а 73% мужчин находятся на том же уровне, модель ИИ для размещения вакансий, обученная на этих данных, может в основном ориентироваться на мужскую аудиторию для старших должностей. Этот дисбаланс в данных склоняет предсказания модели в пользу одного пола. Это выявляет проблему справедливости, где в модели ИИ присутствует гендерная предвзятость.

Компонент "Анализ данных" на панели RAI помогает выявлять области, где в наборе данных есть избыточное или недостаточное представление. Он помогает пользователям диагностировать первопричины ошибок и проблем справедливости, вызванных дисбалансом данных или отсутствием представления определенной группы данных. Это дает пользователям возможность визуализировать наборы данных на основе предсказанных и фактических результатов, групп ошибок и конкретных характеристик. Иногда обнаружение недостаточно представленной группы данных также может выявить, что модель плохо обучается, что приводит к высоким неточностям. Модель с предвзятостью данных — это не только проблема справедливости, но и показатель того, что модель не является инклюзивной или надежной.

![Компонент анализа данных на панели RAI](../../../../translated_images/dataanalysis-cover.8d6d0683a70a5c1e274e5a94b27a71137e3d0a3b707761d7170eb340dd07f11d.ru.png)

Используйте анализ данных, когда вам нужно:

* Исследовать статистику вашего набора данных, выбирая различные фильтры для разделения данных на разные измерения (также известные как когорты).
* Понять распределение вашего набора данных по различным когортам и группам характеристик.
* Определить, связаны ли ваши выводы о справедливости, анализе ошибок и причинности (полученные из других компонентов панели) с распределением вашего набора данных.
* Решить, в каких областях нужно собрать больше данных, чтобы устранить ошибки, вызванные проблемами представления, шумом меток, шумом характеристик, предвзятостью меток и другими факторами.

## Интерпретируемость модели

Модели машинного обучения часто воспринимаются как "черные ящики". Понять, какие ключевые характеристики данных влияют на предсказания модели, может быть сложно. Важно обеспечить прозрачность того, почему модель делает определенные предсказания. Например, если система ИИ предсказывает, что диабетический пациент рискует быть повторно госпитализированным в течение менее чем 30 дней, она должна предоставить данные, подтверждающие это предсказание. Наличие таких данных обеспечивает прозрачность, помогая врачам или больницам принимать обоснованные решения. Кроме того, возможность объяснить, почему модель сделала предсказание для конкретного пациента, обеспечивает соответствие требованиям здравоохранительных регуляций. Использование моделей машинного обучения в ситуациях, влияющих на жизнь людей, требует понимания и объяснения факторов, влияющих на поведение модели. Интерпретируемость и объяснимость модели помогают ответить на вопросы в таких сценариях, как:

* Отладка модели: Почему моя модель допустила эту ошибку? Как я могу улучшить модель?
* Сотрудничество человека и ИИ: Как я могу понять и доверять решениям модели?
* Соответствие нормативным требованиям: Соответствует ли моя модель юридическим требованиям?

Компонент "Важность признаков" на панели RAI помогает отлаживать и получать полное понимание того, как модель делает предсказания. Это также полезный инструмент для специалистов по машинному обучению и лиц, принимающих решения, чтобы объяснить и предоставить доказательства влияния признаков на поведение модели для соответствия нормативным требованиям. Пользователи могут исследовать как глобальные, так и локальные объяснения, чтобы проверить, какие признаки влияют на предсказания модели. Глобальные объяснения показывают основные признаки, которые повлияли на общее предсказание модели. Локальные объяснения отображают, какие признаки привели к предсказанию модели для конкретного случая. Возможность оценивать локальные объяснения также полезна при отладке или аудите конкретного случая, чтобы лучше понять и интерпретировать, почему модель сделала точное или неточное предсказание.

![Компонент важности признаков на панели RAI](../../../../translated_images/9-feature-importance.cd3193b4bba3fd4bccd415f566c2437fb3298c4824a3dabbcab15270d783606e.ru.png)

* Глобальные объяснения: Например, какие признаки влияют на общее поведение модели предсказания повторной госпитализации диабетиков?
* Локальные объяснения: Например, почему диабетический пациент старше 60 лет с предыдущими госпитализациями был предсказан как повторно госпитализированный или нет в течение 30 дней?

В процессе отладки производительности модели в разных когортам компонент "Важность признаков" показывает, какое влияние оказывает признак на когорты. Это помогает выявить аномалии при сравнении уровня влияния признака на ошибочные предсказания модели. Компонент "Важность признаков" может показать, какие значения признака положительно или отрицательно повлияли на результат модели. Например, если модель сделала неточное предсказание, компонент позволяет углубиться и определить, какие признаки или значения признаков привели к предсказанию. Этот уровень детализации помогает не только в отладке, но и обеспечивает прозрачность и подотчетность в ситуациях аудита. Наконец, компонент может помочь выявить проблемы справедливости. Например, если чувствительный признак, такой как этническая принадлежность или пол, оказывает значительное влияние на предсказания модели, это может быть признаком расовой или гендерной предвзятости в модели.

![Важность признаков](../../../../translated_images/9-features-influence.3ead3d3f68a84029f1e40d3eba82107445d3d3b6975d4682b23d8acc905da6d0.ru.png)

Используйте интерпретируемость, когда вам нужно:

* Определить, насколько надежны предсказания вашей системы ИИ, понимая, какие признаки наиболее важны для предсказаний.
* Подойти к отладке модели, сначала поняв ее и выявив, использует ли модель полезные признаки или ложные корреляции.
* Выявить потенциальные источники несправедливости, понимая, основывается ли модель на чувствительных признаках или на признаках, которые сильно с ними коррелируют.
* Завоевать доверие пользователей к решениям модели, создавая локальные объяснения для иллюстрации их результатов.
* Провести регуляторный аудит системы ИИ, чтобы проверить модели и контролировать влияние решений модели на людей.

## Заключение

Все компоненты панели RAI — это практические инструменты, которые помогают создавать модели машинного обучения, менее вредные и более надежные для общества. Они способствуют предотвращению угроз правам человека, дискриминации или исключению определенных групп из жизненных возможностей, а также рисков физического или психологического вреда. Эти инструменты также помогают завоевать доверие к решениям модели, создавая локальные объяснения для иллюстрации их результатов. Некоторые из потенциальных вредов можно классифицировать следующим образом:

- **Распределение**: если, например, один пол или этническая группа получают предпочтение перед другими.
- **Качество обслуживания**: если вы обучаете данные для одного конкретного сценария, но реальность гораздо сложнее, это приводит к низкому качеству обслуживания.
- **Стереотипизация**: ассоциация определенной группы с заранее заданными характеристиками.
- **Уничижение**: несправедливая критика и навешивание ярлыков.
- **Пере- или недопредставленность**. Идея заключается в том, что определенная группа людей не представлена в определенной профессии, и любая услуга или функция, которая продолжает это поддерживать, способствует нанесению вреда.

### Панель управления Azure RAI

[Панель управления Azure RAI](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) основана на инструментах с открытым исходным кодом, разработанных ведущими академическими учреждениями и организациями, включая Microsoft. Эти инструменты помогают специалистам по данным и разработчикам ИИ лучше понимать поведение моделей, выявлять и устранять нежелательные проблемы в моделях ИИ.

- Узнайте, как использовать различные компоненты, ознакомившись с [документацией панели управления RAI.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)

- Ознакомьтесь с [примерными ноутбуками панели управления RAI](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks) для отладки более ответственных сценариев ИИ в Azure Machine Learning.

---
## 🚀 Задача

Чтобы предотвратить возникновение статистических или данных предвзятостей, мы должны:

- обеспечить разнообразие опыта и точек зрения среди людей, работающих над системами
- инвестировать в наборы данных, которые отражают разнообразие нашего общества
- разрабатывать лучшие методы для выявления и исправления предвзятости, когда она возникает

Подумайте о реальных сценариях, где несправедливость очевидна при создании и использовании моделей. Что еще следует учитывать?

## [Тест после лекции](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## Обзор и самостоятельное изучение

На этом уроке вы узнали о некоторых практических инструментах для внедрения ответственного ИИ в машинное обучение.

Посмотрите этот семинар, чтобы углубиться в тему:

- Панель управления Responsible AI: универсальное решение для внедрения RAI на практике от Бесмиры Нуши и Мехрнуш Самеки

[![Панель управления Responsible AI: универсальное решение для внедрения RAI на практике](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "Панель управления Responsible AI: универсальное решение для внедрения RAI на практике")


> 🎥 Нажмите на изображение выше, чтобы посмотреть видео: Панель управления Responsible AI: универсальное решение для внедрения RAI на практике от Бесмиры Нуши и Мехрнуш Самеки

Обратитесь к следующим материалам, чтобы узнать больше об ответственном ИИ и о том, как создавать более надежные модели:

- Инструменты Microsoft для отладки моделей машинного обучения: [Ресурсы инструментов Responsible AI](https://aka.ms/rai-dashboard)

- Изучите набор инструментов Responsible AI: [Github](https://github.com/microsoft/responsible-ai-toolbox)

- Центр ресурсов Microsoft по ответственному ИИ: [Ресурсы по ответственному ИИ – Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Исследовательская группа Microsoft FATE: [FATE: Справедливость, ответственность, прозрачность и этика в ИИ - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## Задание

[Изучите панель управления RAI](assignment.md)

---

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.