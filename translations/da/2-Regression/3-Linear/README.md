# Byg en regressionsmodel ved hj√¶lp af Scikit-learn: regression p√• fire m√•der

## Begynderbem√¶rkning

Line√¶r regression bruges, n√•r vi vil forudsige en **numerisk v√¶rdi** (for eksempel huspris, temperatur eller salg).  
Den fungerer ved at finde en ret linje, der bedst repr√¶senterer forholdet mellem inputfunktioner og output.

I denne lektion fokuserer vi p√• at forst√• konceptet, f√∏r vi udforsker mere avancerede regressionsteknikker.  
![Line√¶r vs polynomiel regressions infografik](../../../../translated_images/da/linear-polynomial.5523c7cb6576ccab.webp)  
> Infografik af [Dasani Madipalli](https://twitter.com/dasani_decoded)  
## [Forventningsquiz](https://ff-quizzes.netlify.app/en/ml/)

> ### [Denne lektion findes p√• R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)  
### Introduktion

Indtil nu har du udforsket, hvad regression er med pr√∏ve-data indsamlet fra gr√¶skarpris-datas√¶ttet, som vi vil bruge igennem hele denne lektion. Du har ogs√• visualiseret det ved hj√¶lp af Matplotlib.

Nu er du klar til at dykke dybere ned i regression inden for ML. Mens visualisering g√∏r det muligt at forst√• data, kommer den egentlige kraft ved maskinl√¶ring fra _tr√¶ning af modeller_. Modeller tr√¶nes p√• historiske data for automatisk at fange dataafh√¶ngigheder, og de giver dig mulighed for at forudsige resultater for nye data, som modellen ikke har set f√∏r.

I denne lektion vil du l√¶re mere om to typer regression: _basal line√¶r regression_ og _polynomiel regression_, sammen med noget af den matematik, der ligger til grund for disse teknikker. Disse modeller vil give os mulighed for at forudsige gr√¶skarpriser afh√¶ngigt af forskellige inputdata.

[![ML for begyndere - Forst√• line√¶r regression](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML for beginners - Understanding Linear Regression")

> üé• Klik p√• billedet ovenfor for en kort videooversigt over line√¶r regression.

> I hele dette pensum antager vi minimal matematikkundskab og s√∏ger at g√∏re det tilg√¶ngeligt for studerende med baggrund fra andre felter, s√• hold √∏je med noter, üßÆ forklaringer, diagrammer og andre l√¶ringsv√¶rkt√∏jer til hj√¶lp med forst√•elsen.

### Foruds√¶tning

Du b√∏r nu v√¶re bekendt med strukturen i gr√¶skardataene, som vi unders√∏ger. Du kan finde det forudindl√¶st og forudrense i denne lektions _notebook.ipynb_-fil. I filen vises gr√¶skarprisen pr. bushel i en ny data frame. S√∏rg for, at du kan k√∏re disse notebooks i kerner i Visual Studio Code.

### Forberedelse

Som en p√•mindelse l√¶ser du disse data ind for at kunne stille sp√∏rgsm√•l til dem.

- Hvorn√•r er det bedste tidspunkt at k√∏be gr√¶skar?
- Hvilken pris kan jeg forvente for en kasse miniaturegr√¶skar?
- Skal jeg k√∏be dem i halvbushel-kurve eller i 1 1/9 bushel-kasser?

Lad os blive ved med at grave i disse data.

I den foreg√•ende lektion oprettede du en Pandas data frame og fyldte den med en del af det oprindelige datas√¶t, hvor priserne blev standardiseret pr. bushel. Ved at g√∏re det kunne du dog kun samle omkring 400 datapunkter og kun for efter√•rsm√•nederne.

Tag et kig p√• de data, vi forudindl√¶ste i denne lektions ledsagende notebook. Dataene er forudindl√¶st, og et indledende spredningsdiagram er tegnet for at vise m√•nedsdata. M√•ske kan vi f√• lidt mere indsigt i dataenes natur ved at rense det yderligere.

## En line√¶r regressionslinje

Som du l√¶rte i Lektion 1, er m√•let med en line√¶r regressions√∏velse at kunne plotte en linje for at:

- **Vise variabelrelationer**. Vise forholdet mellem variable  
- **Foretage forudsigelser**. Foretage pr√¶cise forudsigelser om, hvor et nyt datapunkt vil falde i forhold til den linje.

Det er typisk for **Minste Kvadraters Regression** at tegne denne type linje. Udtrykket "Minste Kvadrater" henviser til processen med at minimere den samlede fejl i vores model. For hvert datapunkt m√•ler vi den lodrette afstand (kaldet residual) mellem det faktiske punkt og vores regressionslinje.

Vi kvadrerer disse afstande af to hoved√•rsager:

1. **St√∏rrelse frem for retning:** Vi vil behandle en fejl p√• -5 p√• samme m√•de som en fejl p√• +5. Kvadrering g√∏r alle v√¶rdier positive.

2. **Straffe for afvigere:** Kvadrering giver st√∏rre v√¶gt til st√∏rre fejl og tvinger linjen til at holde sig t√¶ttere p√• punkter, som er langt v√¶k.

Derefter l√¶gger vi alle disse kvadrerede v√¶rdier sammen. Vores m√•l er at finde den specifikke linje, hvor dette endelige summen er mindst (mindste mulige v√¶rdi)‚Äîderaf navnet "Minste Kvadrater".

> **üßÆ Vis mig matematikken**  
>  
> Denne linje, kaldet _den bedste tilpassede linje_, kan udtrykkes ved [en ligning](https://en.wikipedia.org/wiki/Simple_linear_regression):  
>  
> ```
> Y = a + bX
> ```
>  
> `X` er den 'forklarende variabel'. `Y` er den 'afh√¶ngige variabel'. H√¶ldningen af linjen er `b` og `a` er sk√¶ringspunktet med y-aksen, hvilket henviser til v√¶rdien af `Y` n√•r `X = 0`.  
>  
>![beregn h√¶ldningen](../../../../translated_images/da/slope.f3c9d5910ddbfcf9.webp)  
>  
> F√∏rst beregnes h√¶ldningen `b`. Infografik af [Jen Looper](https://twitter.com/jenlooper)  
>  
> Med andre ord, og med henvisning til vores gr√¶skardatas oprindelige sp√∏rgsm√•l: "forudsig prisen p√• et gr√¶skar pr. bushel efter m√•ned", ville `X` referere til prisen og `Y` henvise til salgs-m√•neden.  
>  
>![fuldf√∏r ligningen](../../../../translated_images/da/calculation.a209813050a1ddb1.webp)  
>  
> Beregn v√¶rdien af Y. Hvis du betaler omkring 4 $, m√• det v√¶re april! Infografik af [Jen Looper](https://twitter.com/jenlooper)  
>  
> Matematikken, der beregner linjen, skal demonstrere h√¶ldningen p√• linjen, som ogs√• afh√¶nger af sk√¶ringspunktet, eller hvor `Y` er placeret, n√•r `X = 0`.  
>  
> Du kan se beregningsmetoden for disse v√¶rdier p√• [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) websitet. Bes√∏g ogs√• [denne mindst-kvadrater-lommeregner](https://www.mathsisfun.com/data/least-squares-calculator.html) for at se, hvordan v√¶rdiernes tal p√•virker linjen.

## Korrelation

Et begreb mere at forst√• er **Korrelationskoefficienten** mellem givne X- og Y-variable. Ved hj√¶lp af et spredningsdiagram kan du hurtigt visualisere denne koefficient. En graf med datapunkter spredt i en p√¶n linje har h√∏j korrelation, mens en graf med datapunkter spredt overalt mellem X og Y har lav korrelation.

En god line√¶r regressionsmodel vil v√¶re en, der har en h√∏j (t√¶ttere p√• 1 end 0) korrelationskoefficient ved brug af Minste Kvadraters-regressionsmetoden med en regressionslinje.

‚úÖ K√∏r notebook'en til denne lektion og se p√• spredningsdiagrammet for m√•ned og pris. Virker dataene, der forbinder m√•ned til pris for gr√¶skarsalg, til at have h√∏j eller lav korrelation baseret p√• din visuelle fortolkning af spredningsdiagrammet? √Ündres det, hvis du bruger en mere finmasket m√•ling i stedet for `Month`, f.eks. *dag p√• √•ret* (dvs. antal dage siden √•rets start)?

I koden nedenfor antager vi, at vi har renset dataene og opn√•et en data frame kaldet `new_pumpkins`, som ligner f√∏lgende:

ID | M√•ned | DagP√•√Öret | Sort | By | Pakke | Lav Pris | H√∏j Pris | Pris  
---|-------|-----------|-------|----|--------|----------|----------|------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel-kasser | 15.0 | 15.0 | 13.636364  
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel-kasser | 18.0 | 18.0 | 16.363636  
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel-kasser | 18.0 | 18.0 | 16.363636  
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel-kasser | 17.0 | 17.0 | 15.454545  
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel-kasser | 15.0 | 15.0 | 13.636364  

> Koden til at rense dataene findes i [`notebook.ipynb`](notebook.ipynb). Vi har udf√∏rt de samme rensningsskridt som i den foreg√•ende lektion og har beregnet `DayOfYear` kolonnen ved hj√¶lp af f√∏lgende udtryk:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```
  
Nu hvor du har en forst√•else for matematikken bag line√¶r regression, lad os oprette en regressionsmodel for at se, om vi kan forudsige, hvilken pakke med gr√¶skar der vil have de bedste priser. En person, der k√∏ber gr√¶skar til en ferie-gr√¶skarmark, kunne have brug for denne information for at optimere deres k√∏b af gr√¶skarpakker til markedet.

## S√∏ger efter korrelation

[![ML for begyndere - S√∏ger efter korrelation: n√∏glen til line√¶r regression](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML for beginners - Looking for Correlation: The Key to Linear Regression")

> üé• Klik p√• billedet ovenfor for en kort videooversigt over korrelation.

Fra den foreg√•ende lektion har du sandsynligvis set, at gennemsnitsprisen for forskellige m√•neder ser s√•dan ud:

<img alt="Gennemsnitspris efter m√•ned" src="../../../../translated_images/da/barchart.a833ea9194346d76.webp" width="50%"/>

Dette tyder p√•, at der burde v√¶re en vis korrelation, og vi kan pr√∏ve at tr√¶ne en line√¶r regressionsmodel til at forudsige forholdet mellem `Month` og `Price`, eller mellem `DayOfYear` og `Price`. Her er spredningsdiagrammet, der viser sidstn√¶vnte forhold:

<img alt="Spredningsdiagram over pris vs. dag p√• √•ret" src="../../../../translated_images/da/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" /> 

Lad os se, om der er en korrelation ved hj√¶lp af `corr` funktionen:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```
  
Det ser ud til, at korrelationen er ret lille, -0.15 for `Month` og -0.17 for `DayOfMonth`, men der kunne v√¶re en anden vigtig sammenh√¶ng. Det ser ud til, at der er forskellige grupper af priser svarende til forskellige gr√¶skarsorter. For at bekr√¶fte denne hypotese, lad os plotte hver gr√¶skarkategori med en anden farve. Ved at sende en `ax` parameter til `scatter` plotting-funktionen kan vi plotte alle punkter p√• samme graf:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```
  
<img alt="Spredningsdiagram over pris vs. dag p√• √•ret" src="../../../../translated_images/da/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" /> 

Vores unders√∏gelse antyder, at sorten har st√∏rre effekt p√• den samlede pris end den faktiske salgsdato. Dette kan vi se med et s√∏jlediagram:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```
  
<img alt="S√∏jlediagram over pris vs. sort" src="../../../../translated_images/da/price-by-variety.744a2f9925d9bcb4.webp" width="50%" /> 

Lad os indtil videre fokusere p√• √©n gr√¶skartype, 'pie type', og unders√∏ge, hvilken effekt datoen har p√• prisen:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Spredningsdiagram over pris vs. dag p√• √•ret" src="../../../../translated_images/da/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" /> 

Hvis vi nu beregner korrelationen mellem `Price` og `DayOfYear` ved hj√¶lp af `corr` funktionen, f√•r vi noget i retning af `-0.27` - hvilket betyder, at det giver mening at tr√¶ne en forudsigelsesmodel.

> F√∏r vi tr√¶ner en line√¶r regressionsmodel, er det vigtigt at sikre, at vores data er rensede. Line√¶r regression fungerer ikke godt med manglende v√¶rdier, s√• det giver mening at fjerne alle tomme celler:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```
  
En anden tilgang ville v√¶re at udfylde de tomme v√¶rdier med gennemsnitsv√¶rdier fra den tilsvarende kolonne.

## Simpel line√¶r regression

[![ML for begyndere - Line√¶r og polynomiel regression ved brug af Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML for beginners - Linear and Polynomial Regression using Scikit-learn")

> üé• Klik p√• billedet ovenfor for en kort videooversigt over line√¶r og polynomiel regression.

For at tr√¶ne vores line√¶re regressionsmodel vil vi bruge **Scikit-learn** biblioteket.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```
  
Vi starter med at adskille inputv√¶rdier (features) og den forventede output (label) i separate numpy-arrays:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```
  
> Bem√¶rk, at vi var n√∏dt til at udf√∏re `reshape` p√• inputdataene, for at Linear Regression-pakken kunne forst√• dem korrekt. Line√¶r Regression forventer et 2D-array som input, hvor hver r√¶kke af arrayet svarer til en vektor af inputfunktioner. I vores tilf√¶lde, da vi kun har √©n input, har vi brug for et array med form N&times;1, hvor N er datam√¶ngdens st√∏rrelse.

Derefter skal vi splitte dataene op i tr√¶nings- og testdatas√¶t, s√• vi kan validere vores model efter tr√¶ning:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```
  
Endelig tager tr√¶ningen af den faktiske line√¶re regressionsmodel kun to kodelinjer. Vi definerer et `LinearRegression`-objekt og fitter det til vores data ved hj√¶lp af `fit`-metoden:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

`LinearRegression`-objektet efter `fit`-ning indeholder alle koefficienterne for regressionen, som kan tilg√•s ved hj√¶lp af `.coef_`-egenskaben. I vores tilf√¶lde er der kun √©n koefficient, som burde v√¶re omkring `-0.017`. Det betyder, at priserne tilsyneladende falder lidt over tid, men ikke for meget, omkring 2 cent per dag. Vi kan ogs√• tilg√• sk√¶ringspunktet for regressionen med Y-aksen ved hj√¶lp af `lin_reg.intercept_` - det vil v√¶re omkring `21` i vores tilf√¶lde, hvilket angiver prisen i begyndelsen af √•ret.

For at se hvor n√∏jagtig vores model er, kan vi forudsige priser p√• et testdatas√¶t, og derefter m√•le hvor t√¶t vores forudsigelser er p√• de forventede v√¶rdier. Dette kan g√∏res ved hj√¶lp af mean square error (MSE) m√•lemetrik, som er gennemsnittet af alle kvadrerede forskelle mellem forventede og forudsagte v√¶rdier.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

Vores fejl ser ud til at v√¶re omkring 2 point, hvilket er ~17%. Ikke s√• godt. En anden indikator for modellens kvalitet er **determinationskoefficienten**, som kan opn√•s p√• denne m√•de:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
 Hvis v√¶rdien er 0, betyder det, at modellen ikke tager inputdata i betragtning, og fungerer som den *v√¶rste line√¶re forudsigelse*, som simpelthen er gennemsnitsv√¶rdien af resultatet. V√¶rdien 1 betyder, at vi perfekt kan forudsige alle forventede output. I vores tilf√¶lde er koefficienten omkring 0,06, hvilket er ganske lavt.

Vi kan ogs√• plotte testdata sammen med regressionslinjen for bedre at se, hvordan regressionen fungerer i vores tilf√¶lde:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linear regression" src="../../../../translated_images/da/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## Polynomiel Regression

En anden type line√¶r regression er polynomiel regression. Selvom der nogle gange er et line√¶rt forhold mellem variabler - jo st√∏rre gr√¶skar i volumen, desto h√∏jere pris - kan disse forhold til tider ikke plottes som et plan eller en lige linje.

‚úÖ Her er [nogle flere eksempler](https://online.stat.psu.edu/stat501/lesson/9/9.8) p√• data, som kunne bruge polynomiel regression

Tag endnu et kig p√• forholdet mellem Dato og Pris. Ser dette scatterplot ud som om det n√∏dvendigvis skal analyseres med en lige linje? Kan priser ikke svinge? I dette tilf√¶lde kan du pr√∏ve polynomiel regression.

‚úÖ Polynomier er matematiske udtryk, som kan best√• af en eller flere variable og koefficienter

Polynomiel regression skaber en buet linje for bedre at tilpasse ikke-line√¶re data. I vores tilf√¶lde, hvis vi medtager et kvadreret `DayOfYear`-variabel i inputdata, b√∏r vi kunne tilpasse vores data med en parabolsk kurve, som vil have et minimum p√• et bestemt tidspunkt i l√∏bet af √•ret.

Scikit-learn inkluderer en nyttig [pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) til at kombinere forskellige trin i databehandlingen sammen. En **pipeline** er en k√¶de af **estimators**. I vores tilf√¶lde vil vi skabe en pipeline, der f√∏rst tilf√∏jer polynomielle features til vores model, og derefter tr√¶ner regressionen:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

At bruge `PolynomialFeatures(2)` betyder, at vi vil inkludere alle andengradspolynomier fra inputdata. I vores tilf√¶lde vil det bare betyde `DayOfYear`<sup>2</sup>, men givet to inputvariabler X og Y, vil dette tilf√∏je X<sup>2</sup>, XY og Y<sup>2</sup>. Vi kan ogs√• bruge polynomier med h√∏jere grad, hvis vi √∏nsker.

Pipelines kan bruges p√• samme m√•de som det oprindelige `LinearRegression`-objekt, dvs. vi kan `fit` pipelinen og derefter bruge `predict` til at f√• forudsigelsesresultater. Her er grafen, der viser testdata og tilpasningskurven:

<img alt="Polynomial regression" src="../../../../translated_images/da/poly-results.ee587348f0f1f60b.webp" width="50%" />

Med polynomiel regression kan vi opn√• lidt lavere MSE og h√∏jere determination, men ikke signifikant. Vi skal tage andre features i betragtning!

> Du kan se, at de laveste gr√¶skarpriser observeres et sted omkring Halloween. Hvordan kan du forklare dette?

üéÉ Tillykke, du har netop skabt en model, der kan hj√¶lpe med at forudsige prisen p√• t√¶rtegr√¶skar. Du kan sikkert gentage den samme procedure for alle gr√¶skartyper, men det ville v√¶re kedeligt. Lad os nu l√¶re, hvordan vi tager gr√¶skarvariant i betragtning i vores model!

## Kategoriske Features

I en ideel verden vil vi kunne forudsige priser for forskellige gr√¶skarvarianter ved hj√¶lp af den samme model. Men `Variety`-kolonnen er lidt anderledes end kolonner som `Month`, fordi den indeholder ikke-numeriske v√¶rdier. S√•danne kolonner kaldes **kategoriske**.

[![ML for beginners - Categorical Feature Predictions with Linear Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML for beginners - Categorical Feature Predictions with Linear Regression")

> üé• Klik p√• billedet ovenfor for en kort videooversigt over brugen af kategoriske features.

Her kan du se, hvordan gennemsnitsprisen afh√¶nger af variant:

<img alt="Average price by variety" src="../../../../translated_images/da/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

For at tage variant i betragtning, skal vi f√∏rst konvertere den til numerisk form, eller **encode** den. Der er flere m√•der, vi kan g√∏re det p√•:

* Simpel **numerisk encoding** bygger en tabel over forskellige varianter og erstatter derefter variantnavnet med et indeks i tabellen. Dette er ikke den bedste ide for line√¶r regression, fordi line√¶r regression tager den faktiske numeriske v√¶rdi af indekset og l√¶gger det til resultatet, multipliceret med en koefficient. I vores tilf√¶lde er forholdet mellem indeksnummer og pris tydeligt ikke-line√¶rt, selv hvis vi s√∏rger for, at indeksene er ordnet p√• en bestemt m√•de.
* **One-hot encoding** vil erstatte `Variety`-kolonnen med 4 forskellige kolonner, en for hver variant. Hver kolonne vil indeholde `1`, hvis den tilsvarende r√¶kke er af den givne variant, og `0` ellers. Det betyder, at der vil v√¶re fire koefficienter i line√¶r regression, √©n for hver gr√¶skarvariant, ansvarlig for "startpris" (eller snarere "ekstra pris") for netop denne variant.

Koden nedenfor viser, hvordan vi kan one-hot encode en variant:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

For at tr√¶ne line√¶r regression ved brug af one-hot encoded variant som input, skal vi blot initialisere `X` og `y` data korrekt:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Resten af koden er den samme, som vi brugte ovenfor til at tr√¶ne line√¶r regression. Hvis du pr√∏ver det, vil du se, at mean squared error er omtrent det samme, men vi f√•r en meget h√∏jere determinationskoefficient (~77%). For at f√• endnu mere pr√¶cise forudsigelser kan vi tage flere kategoriske features i betragtning, s√•vel som numeriske features, s√•som `Month` eller `DayOfYear`. For at f√• et stort array af features kan vi bruge `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Her tager vi ogs√• hensyn til `City` og `Package`-type, hvilket giver os MSE 2,84 (10%) og determination 0,94!

## At samle det hele

For at lave den bedste model kan vi bruge kombinerede (one-hot encoded kategoriske + numeriske) data fra ovenst√•ende eksempel sammen med polynomiel regression. Her er den komplette kode for din bekvemmelighed:

```python
# ops√¶t tr√¶ningsdata
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# lav tr√¶nings- og testopdeling
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# ops√¶t og tr√¶n pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# forudsig resultater for testdata
pred = pipeline.predict(X_test)

# beregn MSE og bestemmelse
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Dette skulle give os den bedste determinationskoefficient p√• n√¶sten 97%, og MSE=2,23 (~8% fejl i forudsigelsen).

| Model | MSE | Determination |
|-------|-----|---------------|
| `DayOfYear` Line√¶r | 2,77 (17,2%) | 0,07 |
| `DayOfYear` Polynomiel | 2,73 (17,0%) | 0,08 |
| `Variety` Line√¶r | 5,24 (19,7%) | 0,77 |
| Alle features Line√¶r | 2,84 (10,5%) | 0,94 |
| Alle features Polynomiel | 2,23 (8,25%) | 0,97 |

üèÜ Godt klaret! Du har skabt fire regressionsmodeller i √©n lektion og forbedret modelkvaliteten til 97%. I den sidste sektion om regression vil du l√¶re om logistisk regression til bestemmelse af kategorier.

---
## üöÄUdfordring

Test flere forskellige variable i denne notesbog for at se, hvordan korrelation svarer til modeln√∏jagtighed.

## [Quiz efter lektionen](https://ff-quizzes.netlify.app/en/ml/)

## Gennemgang & Selvstudie

I denne lektion l√¶rte vi om line√¶r regression. Der findes andre vigtige typer regression. L√¶s om Stepwise, Ridge, Lasso og Elasticnet teknikker. Et godt kursus at studere for at l√¶re mere er [Stanford Statistical Learning course](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## Opgave

[Byg en model](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfraskrivelse**:
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, bedes du v√¶re opm√¶rksom p√•, at automatiske overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det oprindelige dokument p√• dets modersm√•l b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os intet ansvar for eventuelle misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->