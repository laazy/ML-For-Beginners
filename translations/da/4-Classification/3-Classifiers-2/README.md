# Cuisine classifiers 2

I denne anden klassificeringslektion vil du udforske flere m√•der at klassificere numeriske data p√•. Du vil ogs√• l√¶re om konsekvenserne ved at v√¶lge den ene klassifikator frem for den anden.

## [For-forel√¶sning quiz](https://ff-quizzes.netlify.app/en/ml/)

### Foruds√¶tning

Vi antager, at du har gennemf√∏rt de tidligere lektioner og har et renset datas√¶t i din `data`-mappe kaldet _cleaned_cuisines.csv_ i roden af denne 4-lektions mappe.

### Forberedelse

Vi har indl√¶st din _notebook.ipynb_-fil med det rensede datas√¶t og delt det op i X og y dataframes, klar til modelbyggerprocessen.

## Et klassificeringskort

Tidligere l√¶rte du om de forskellige muligheder, du har ved klassificering af data ved hj√¶lp af Microsofts snydeark. Scikit-learn tilbyder et lignende, men mere detaljeret snydeark, der yderligere kan hj√¶lpe med at indsn√¶vre dine estimeringsmetoder (et andet ord for klassifikatorer):

![ML Map from Scikit-learn](../../../../translated_images/da/map.e963a6a51349425a.webp)
> Tip: [bes√∏g dette kort online](https://scikit-learn.org/stable/tutorial/machine_learning_map/) og klik dig igennem stien for at l√¶se dokumentationen.

### Planen

Dette kort er meget nyttigt, n√•r du har et klart overblik over dine data, da du kan 'g√•' langs dets stier til en beslutning:

- Vi har >50 pr√∏ver
- Vi √∏nsker at forudsige en kategori
- Vi har m√¶rket data
- Vi har f√¶rre end 100K pr√∏ver
- ‚ú® Vi kan v√¶lge en Linear SVC
- Hvis det ikke virker, da vi har numeriske data
    - Kan vi pr√∏ve en ‚ú® KNeighbors Classifier 
      - Hvis det stadig ikke virker, pr√∏v ‚ú® SVC og ‚ú® Ensemble Classifiers

Dette er en meget nyttig rute at f√∏lge.

## √òvelse - del dataene

F√∏lgende denne sti b√∏r vi starte med at importere nogle biblioteker til brug.

1. Import√©r de n√∏dvendige biblioteker:

    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    import numpy as np
    ```

1. Del dine tr√¶nings- og testdata:

    ```python
    X_train, X_test, y_train, y_test = train_test_split(cuisines_features_df, cuisines_label_df, test_size=0.3)
    ```

## Linear SVC-klassifikator

Support-Vector clustering (SVC) er en del af Support-Vector maskiner-familien af ML-teknikker (l√¶r mere om disse nedenfor). Med denne metode kan du v√¶lge en ‚Äòkernel‚Äô for at bestemme, hvordan etiketterne skal grupperes. Parameteren ‚ÄòC‚Äô refererer til ‚Äòregularisering‚Äô, som regulerer parametrenes indflydelse. Kernelen kan v√¶re en af [flere](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC); her s√¶tter vi den til ‚Äòlinear‚Äô for at sikre, at vi udnytter linear SVC. Probability er som standard sat til ‚Äòfalse‚Äô; her s√¶tter vi den til ‚Äòtrue‚Äô for at indsamle sandsynlighedssk√∏n. Vi s√¶tter random state til ‚Äò0‚Äô for at blande dataene og f√• sandsynligheder.

### √òvelse - anvend en linear SVC

Start med at oprette et array af klassifikatorer. Du vil gradvist tilf√∏je til dette array, mens vi tester.

1. Start med en Linear SVC:

    ```python
    C = 10
    # Opret forskellige klassifikatorer.
    classifiers = {
        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
    }
    ```

2. Tr√¶n din model med Linear SVC og udskriv en rapport:

    ```python
    n_classifiers = len(classifiers)
    
    for index, (name, classifier) in enumerate(classifiers.items()):
        classifier.fit(X_train, np.ravel(y_train))
    
        y_pred = classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
        print(classification_report(y_test,y_pred))
    ```

    Resultatet er ret godt:

    ```output
    Accuracy (train) for Linear SVC: 78.6% 
                  precision    recall  f1-score   support
    
         chinese       0.71      0.67      0.69       242
          indian       0.88      0.86      0.87       234
        japanese       0.79      0.74      0.76       254
          korean       0.85      0.81      0.83       242
            thai       0.71      0.86      0.78       227
    
        accuracy                           0.79      1199
       macro avg       0.79      0.79      0.79      1199
    weighted avg       0.79      0.79      0.79      1199
    ```

## K-Neighbors klassifikator

K-Neighbors h√∏rer til i "neighbors"-familien af ML-metoder, som kan bruges til b√•de overv√•get og ikke-overv√•get l√¶ring. I denne metode oprettes et foruddefineret antal punkter, og data samles omkring disse punkter, s√• generaliserede etiketter kan forudsiges for dataene.

### √òvelse - anvend K-Neighbors klassifikatoren

Den forrige klassifikator var god og fungerede godt med dataene, men m√•ske kan vi f√• bedre n√∏jagtighed. Pr√∏v en K-Neighbors klassifikator.

1. Tilf√∏j en linje til dit klassifikator-array (tilf√∏j et komma efter Linear SVC-objektet):

    ```python
    'KNN classifier': KNeighborsClassifier(C),
    ```

    Resultatet er en smule d√•rligere:

    ```output
    Accuracy (train) for KNN classifier: 73.8% 
                  precision    recall  f1-score   support
    
         chinese       0.64      0.67      0.66       242
          indian       0.86      0.78      0.82       234
        japanese       0.66      0.83      0.74       254
          korean       0.94      0.58      0.72       242
            thai       0.71      0.82      0.76       227
    
        accuracy                           0.74      1199
       macro avg       0.76      0.74      0.74      1199
    weighted avg       0.76      0.74      0.74      1199
    ```

    ‚úÖ L√¶s om [K-Neighbors](https://scikit-learn.org/stable/modules/neighbors.html#neighbors)

## Support Vector Classifier

Support-Vector klassifikatorer er en del af [Support-Vector Machine](https://wikipedia.org/wiki/Support-vector_machine) familien af ML-metoder, der bruges til klassificering og regressionsopgaver. SVM'er "afbilder tr√¶ningseksempler til punkter i rummet" for at maksimere afstanden mellem to kategorier. Efterf√∏lgende data afbildes i dette rum, s√• deres kategori kan forudsiges.

### √òvelse - anvend en Support Vector Classifier

Lad os pr√∏ve p√• en lidt bedre n√∏jagtighed med en Support Vector Classifier.

1. Tilf√∏j et komma efter K-Neighbors-elementet, og tilf√∏j derefter denne linje:

    ```python
    'SVC': SVC(),
    ```

    Resultatet er ret godt!

    ```output
    Accuracy (train) for SVC: 83.2% 
                  precision    recall  f1-score   support
    
         chinese       0.79      0.74      0.76       242
          indian       0.88      0.90      0.89       234
        japanese       0.87      0.81      0.84       254
          korean       0.91      0.82      0.86       242
            thai       0.74      0.90      0.81       227
    
        accuracy                           0.83      1199
       macro avg       0.84      0.83      0.83      1199
    weighted avg       0.84      0.83      0.83      1199
    ```

    ‚úÖ L√¶s om [Support-Vectors](https://scikit-learn.org/stable/modules/svm.html#svm)

## Ensemble klassifikatorer

Lad os f√∏lge stien helt til enden, selvom den tidligere test var ganske god. Lad os pr√∏ve nogle 'Ensemble Classifiers', specifikt Random Forest og AdaBoost:

```python
  'RFST': RandomForestClassifier(n_estimators=100),
  'ADA': AdaBoostClassifier(n_estimators=100)
```

Resultatet er meget godt, is√¶r for Random Forest:

```output
Accuracy (train) for RFST: 84.5% 
              precision    recall  f1-score   support

     chinese       0.80      0.77      0.78       242
      indian       0.89      0.92      0.90       234
    japanese       0.86      0.84      0.85       254
      korean       0.88      0.83      0.85       242
        thai       0.80      0.87      0.83       227

    accuracy                           0.84      1199
   macro avg       0.85      0.85      0.84      1199
weighted avg       0.85      0.84      0.84      1199

Accuracy (train) for ADA: 72.4% 
              precision    recall  f1-score   support

     chinese       0.64      0.49      0.56       242
      indian       0.91      0.83      0.87       234
    japanese       0.68      0.69      0.69       254
      korean       0.73      0.79      0.76       242
        thai       0.67      0.83      0.74       227

    accuracy                           0.72      1199
   macro avg       0.73      0.73      0.72      1199
weighted avg       0.73      0.72      0.72      1199
```

‚úÖ L√¶s om [Ensemble Classifiers](https://scikit-learn.org/stable/modules/ensemble.html)

Denne metode inden for Maskinl√¶ring "kombinerer forudsigelserne fra flere basismetoder" for at forbedre modellens kvalitet. I vores eksempel brugte vi Random Trees og AdaBoost.

- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest), en gennemsnitsmetode, bygger en 'skov' af 'beslutningstr√¶er' fyldt med tilf√¶ldighed for at undg√• overtilpasning. Parameteren n_estimators s√¶ttes til antallet af tr√¶er.

- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) tilpasser en klassifikator til et datas√¶t og tilpasser derefter kopier af den klassifikator til det samme datas√¶t. Den fokuserer p√• v√¶gtene af forkert klassificerede elementer og justerer tilpasningen for den n√¶ste klassifikator for at rette op.

---

## üöÄUdfordring

Hver af disse teknikker har et stort antal parametre, som du kan justere. Unders√∏g standardparametrene for hver enkelt, og overvej, hvad justering af disse parametre ville betyde for modellens kvalitet.

## [Efter-forel√¶sning quiz](https://ff-quizzes.netlify.app/en/ml/)

## Gennemgang & Selvstudie

Der er meget jargon i disse lektioner, s√• tag et √∏jeblik til at gennemg√• [denne liste](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott) med nyttige termer!

## Opgave

[Parameter leg](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfraskrivelse**:
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, skal du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det oprindelige dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os ikke ansvar for misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->