# Izgradite regresijski model koristeÄ‡i Scikit-learn: regresija na Äetiri naÄina

## BiljeÅ¡ka za poÄetnike

Linearna regresija se koristi kada Å¾elimo predvidjeti **numeriÄku vrijednost** (na primjer, cijenu kuÄ‡e, temperaturu ili prodaju).
Radi tako da pronalazi pravu liniju koja najbolje predstavlja odnos izmeÄ‘u ulaznih znaÄajki i izlaza.

U ovom dijelu fokusiramo se na razumijevanje koncepta prije nego Å¡to istraÅ¾imo naprednije tehnike regresije.
![Infografika linearne nasuprot polinomnoj regresiji](../../../../translated_images/hr/linear-polynomial.5523c7cb6576ccab.webp)
> Infografika od [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Kviz prije predavanja](https://ff-quizzes.netlify.app/en/ml/)

> ### [Ovaj je lekciju dostupan i u R-u!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Uvod 

Do sada ste istraÅ¾ivali Å¡to je regresija koristeÄ‡i uzorak podataka prikupljenih iz skupa podataka o cijenama bundeva kojeg koristimo kroz cijelu ovu lekciju. TakoÄ‘er ste ga vizualizirali koristeÄ‡i Matplotlib.

Sada ste spremni dublje zaroniti u regresiju za ML. Dok vizualizacija omoguÄ‡uje razumijevanje podataka, prava moÄ‡ strojnog uÄenja dolazi iz _treniranja modela_. Modeli se treniraju na povijesnim podacima kako bi automatski uhvatili ovisnosti podataka, i omoguÄ‡uju vam da predviÄ‘ate ishode za nove podatke koje model ranije nije vidio.

U ovoj lekciji nauÄit Ä‡ete viÅ¡e o dvije vrste regresije: _osnovna linearna regresija_ i _polinomna regresija_, zajedno s dijelom matematike koja stoji iza ovih tehnika. Ti modeli Ä‡e nam omoguÄ‡iti predviÄ‘anje cijene bundeva ovisno o razliÄitim ulaznim podacima.

[![ML za poÄetnike - Razumijevanje linearne regresije](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML for beginners - Understanding Linear Regression")

> ğŸ¥ Kliknite na slikovnu poveznicu za kratki video pregled linearne regresije.

> Kroz ovaj kurikulum pretpostavljamo minimalno matematiÄko znanje i nastojimo ga uÄiniti pristupaÄnim studentima iz drugih podruÄja, pa pripazite na napomene, ğŸ§® objaÅ¡njenja, dijagrame i druge alate za uÄenje koji pomaÅ¾u razumijevanju.

### Preduvjeti

Do sada biste trebali biti upoznati sa strukturom podataka o bundevama koje prouÄavamo. MoÅ¾ete ih pronaÄ‡i predinstalirane i predobrijeÄ‘ene u _notebook.ipynb_ datoteci ove lekcije. U datoteci se prikazuje cijena bundeve po bushelu u novom okviru podataka. Provjerite da moÅ¾ete pokrenuti ove biljeÅ¾nice u Visual Studio Code kernelima.

### Priprema

Kao podsjetnik, uÄitavate ove podatke kako biste ih mogli ispitivati.

- Kada je najbolje vrijeme za kupnju bundeva? 
- Koju cijenu mogu oÄekivati za paket minijaturnih bundeva?
- Trebam li ih kupiti u koÅ¡arama od pola bushela ili u kutiji od 1 1/9 bushela?
Nastavimo s ispitivanjem ovih podataka.

U prethodnoj ste lekciji kreirali Pandas okvir podataka i ispunili ga dijelom izvornog skupa podataka, standardizirajuÄ‡i cijene po bushelu. MeÄ‘utim, uÄinili ste to samo za oko 400 podataka i samo za jesenske mjesece.

Pogledajte podatke koje smo predinstalirali u biljeÅ¾nici ove lekcije. Podaci su uÄitani, a prikazan je poÄetni scatterplot koji prikazuje podatke mjeseci. MoÅ¾emo li moÅ¾da dobiti malo viÅ¡e detalja o prirodi podataka ÄiÅ¡Ä‡enjem?

## Linija linearne regresije

Kao Å¡to ste nauÄili u Lekciji 1, cilj linearne regresije je biti u stanju nacrtati liniju koja:

- **Prikazuje odnose meÄ‘u varijablama**. Prikazuje odnos izmeÄ‘u varijabli
- **Daje predviÄ‘anja**. ToÄno predviÄ‘a gdje bi novi podatak pao u odnosu na tu liniju.

TipiÄno je za **metodu najmanjih kvadrata** crtanje takve linije. Pojam "najmanjih kvadrata" odnosi se na proces minimiziranja ukupne greÅ¡ke u naÅ¡em modelu. Za svaku toÄku mjerimo vertikalnu udaljenost (nazvanu residuum) izmeÄ‘u stvarne toÄke i naÅ¡e regresijske linije.

Te udaljenosti kvadriramo zbog dva glavna razloga:

1. **Magnitude nad smjerom:** Å½elimo tretirati greÅ¡ku -5 isto kao i greÅ¡ku +5. Kvadriranje sve vrijednosti Äini pozitivnima.

2. **Kaznjavanje odmetnika:** Kvadriranje daje veÄ‡u teÅ¾inu veÄ‡im greÅ¡kama, prisiljavajuÄ‡i liniju da ostane bliÅ¾e udaljenim toÄkama.

Zatim zbrojimo sve te kvadrirane vrijednosti. NaÅ¡ cilj je pronaÄ‡i toÄnu liniju gdje je taj konaÄni zbroj najmanji (najmanja moguÄ‡a vrijednost)â€”otuda naziv "najmanjih kvadrata."

> **ğŸ§® PokaÅ¾i mi matematiku** 
> 
> Ta linija, nazvana _linija najboljeg pristajanja_, moÅ¾e se izraziti [jednadÅ¾bom](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` je 'objaÅ¡njavajuÄ‡a varijabla'. `Y` je 'ovisna varijabla'. Nagib linije je `b`, a `a` je odsjeÄak na y-osi, Å¡to se odnosi na vrijednost `Y` kada je `X = 0`. 
>
>![izraÄunaj nagib](../../../../translated_images/hr/slope.f3c9d5910ddbfcf9.webp)
>
> Prvo, izraÄunajte nagib `b`. Infografika od [Jen Looper](https://twitter.com/jenlooper)
>
> Drugim rijeÄima, i referirajuÄ‡i se na naÅ¡e izvornog pitanje o bundevama: "predvidjeti cijenu bundeve po bushelu prema mjesecu", `X` bi se odnosilo na cijenu, a `Y` na mjesec prodaje.
>
>![dovrÅ¡i jednadÅ¾bu](../../../../translated_images/hr/calculation.a209813050a1ddb1.webp)
>
> IzraÄunajte vrijednost Y. Ako plaÄ‡ate oko 4 dolara, mora biti travanj! Infografika od [Jen Looper](https://twitter.com/jenlooper)
>
> Matematika koja izraÄunava liniju mora pokazati nagib linije, koji takoÄ‘er ovisi o odsjeÄku ili gdje se `Y` nalazi kada je `X = 0`.
>
> Metodu izraÄuna ovih vrijednosti moÅ¾ete promatrati na web stranici [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). TakoÄ‘er posjetite [ovaj kalkulator najmanjih kvadrata](https://www.mathsisfun.com/data/least-squares-calculator.html) da vidite kako vrijednosti brojeva utjeÄu na liniju.

## Korelacija

JoÅ¡ jedan pojam koji treba razumjeti jest **koeficijent korelacije** izmeÄ‘u zadanih X i Y varijabli. KoristeÄ‡i scatterplot, brzo moÅ¾ete vizualizirati ovaj koeficijent. Grafikon s toÄkama rasporeÄ‘enim uredno u liniji ima visoku korelaciju, dok grafikon s toÄkama raÅ¡trkanima posvuda izmeÄ‘u X i Y ima nisku korelaciju.

Dobar model linearne regresije bit Ä‡e onaj koji ima visok (bliÅ¾i 1 nego 0) koeficijent korelacije koristeÄ‡i metodu najmanjih kvadrata s regresijskom linijom.

âœ… Pokrenite biljeÅ¾nicu uz ovu lekciju i pogledajte scatterplot Mjesec prema Cijeni. Ima li podataka koji povezuju mjesec s cijenom prodaje bundeva visoku ili nisku korelaciju, prema vaÅ¡oj vizualnoj interpretaciji scatterplota? Mijenja li se to ako koristite finiju mjeru umjesto `Month`, npr. *dan u godini* (broj dana od poÄetka godine)?

U sljedeÄ‡em kodu pretpostavit Ä‡emo da smo oÄistili podatke i dobili okvir podataka nazvan `new_pumpkins`, sliÄan sljedeÄ‡em:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> Kod za ÄiÅ¡Ä‡enje podataka nalazi se u [`notebook.ipynb`](notebook.ipynb). Izveli smo iste korake ÄiÅ¡Ä‡enja kao u prethodnoj lekciji te izraÄunali stupac `DayOfYear` koristeÄ‡i sljedeÄ‡i izraz:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Sada kad imate razumijevanje matematike iza linearne regresije, kreirajmo regresijski model da vidimo moÅ¾emo li predvidjeti koji paket bundeva Ä‡e imati najbolje cijene. Netko tko kupuje bundeve za blagdanski prikaz bundeva moÅ¾e htjeti ovu informaciju kako bi optimizirao kupnju paketa bundeva za prikaz.

## TraÅ¾enje korelacije

[![ML za poÄetnike - TraÅ¾enje korelacije: KljuÄ za linearnu regresiju](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML for beginners - Looking for Correlation: The Key to Linear Regression")

> ğŸ¥ Kliknite na slikovnu poveznicu za kratki video pregled korelacije.

Iz prethodne lekcije vjerojatno ste vidjeli da prosjeÄna cijena za razliÄite mjesece izgleda ovako:

<img alt="ProsjeÄna cijena po mjesecu" src="../../../../translated_images/hr/barchart.a833ea9194346d76.webp" width="50%"/>

To sugerira da bi trebala postojati neka korelacija, i moÅ¾emo pokuÅ¡ati trenirati linearni regresijski model da predvidi odnos izmeÄ‘u `Month` i `Price`, ili izmeÄ‘u `DayOfYear` i `Price`. Evo scatterplota koji prikazuje ovaj zadnji odnos:

<img alt="Scatter plot cijena u odnosu na dan u godini" src="../../../../translated_images/hr/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" /> 

Pogledajmo ima li korelacije koristeÄ‡i funkciju `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Izgleda da je korelacija priliÄno mala, -0.15 prema `Month` i -0.17 prema `DayOfMonth`, ali moglo bi postojati joÅ¡ neki vaÅ¾an odnos. ÄŒini se da postoje razliÄiti klasteri cijena koji odgovaraju razliÄitim vrstama bundeva. Da bismo potvrdili ovu hipotezu, nacrtajmo svaku kategoriju bundeva razliÄitom bojom. ProsljeÄ‘ivanjem parametra `ax` funkciji `scatter` moÅ¾emo nacrtati sve toÄke na istom grafikonu:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Scatter plot cijena u odnosu na dan u godini s bojama" src="../../../../translated_images/hr/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" /> 

NaÅ¡a istraga sugerira da sorta ima veÄ‡i utjecaj na ukupnu cijenu nego stvarni datum prodaje. To moÅ¾emo vidjeti na stupÄastom grafikonu:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="StupÄasti grafikon cijene po vrstama" src="../../../../translated_images/hr/price-by-variety.744a2f9925d9bcb4.webp" width="50%" /> 

UsredotoÄimo se na trenutak samo na jednu vrstu bundeve, 'pie type', i vidimo kakav utjecaj datum ima na cijenu:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Scatter plot cijena u odnosu na dan u godini za pie vrste bundeva" src="../../../../translated_images/hr/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" /> 

Ako sad izraÄunamo korelaciju izmeÄ‘u `Price` i `DayOfYear` koristeÄ‡i funkciju `corr`, dobit Ä‡emo neÅ¡to oko `-0.27` - Å¡to znaÄi da ima smisla trenirati prediktivni model.

> Prije treniranja modela linearne regresije vaÅ¾no je osigurati da su naÅ¡i podaci Äisti. Linearna regresija ne funkcionira dobro s nedostajuÄ‡im vrijednostima, stoga ima smisla ukloniti sve prazne Ä‡elije:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Drugi pristup bio bi popuniti prazne vrijednosti prosjeÄnim vrijednostima iz odgovarajuÄ‡eg stupca.

## Jednostavna linearna regresija

[![ML za poÄetnike - Linearna i polinomna regresija koristeÄ‡i Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML for beginners - Linear and Polynomial Regression using Scikit-learn")

> ğŸ¥ Kliknite na slikovnu poveznicu za kratki video pregled linearne i polinomne regresije.

Za treniranje naÅ¡eg modela linearne regresije koristit Ä‡emo **Scikit-learn** biblioteku.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

ZapoÄinjemo razdvajanjem ulaznih vrijednosti (znaÄajki) i oÄekivanog izlaza (oznake) u zasebne numpy nizove:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Primijetite da smo morali napraviti `reshape` ulaznih podataka kako bi Linear Regression paket ispravno razumio podatke. Linearna regresija oÄekuje 2D niz kao ulaz, gdje svaki redak niza odgovara vektoru ulaznih znaÄajki. U naÅ¡em sluÄaju, buduÄ‡i da imamo samo jedan ulaz, potrebna nam je matrica dimenzija N&times;1, gdje je N veliÄina skupa podataka.

Zatim trebamo podijeliti podatke na trening i test skupove, kako bismo mogli validirati naÅ¡ model nakon treniranja:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Na kraju, treniranje stvarnog modela linearne regresije traje samo dvije linije koda. Definiramo objekt `LinearRegression` i prilagoÄ‘avamo ga naÅ¡im podacima koriÅ¡tenjem metode `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Objekt `LinearRegression` nakon `fit`-iranja sadrÅ¾i sve koeficijente regresije, kojima se moÅ¾e pristupiti pomoÄ‡u `.coef_` svojstva. U naÅ¡em sluÄaju postoji samo jedan koeficijent, koji bi trebao biti oko `-0.017`. To znaÄi da cijene izgledaju kao da malo padaju s vremenom, ali ne previÅ¡e, oko 2 centa dnevno. TakoÄ‘er moÅ¾emo pristupiti sjeciÅ¡tu regresije s Y-osi pomoÄ‡u `lin_reg.intercept_` - ono Ä‡e biti oko `21` u naÅ¡em sluÄaju, Å¡to oznaÄava cijenu na poÄetku godine.

Da bismo vidjeli koliko je naÅ¡ model toÄan, moÅ¾emo predvidjeti cijene na testnom skupu podataka, a zatim izmjeriti koliko su naÅ¡a predviÄ‘anja bliska oÄekivanim vrijednostima. To se moÅ¾e uÄiniti pomoÄ‡u metrike srednje kvadratne pogreÅ¡ke (MSE), Å¡to je srednja vrijednost svih kvadratnih razlika izmeÄ‘u oÄekivane i predviÄ‘ene vrijednosti.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

NaÅ¡a pogreÅ¡ka izgleda da je oko 2 boda, Å¡to je ~17%. Nije baÅ¡ dobro. JoÅ¡ jedan pokazatelj kvalitete modela je **koeficijent determinacije**, koji se moÅ¾e dobiti ovako:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Ako je vrijednost 0, to znaÄi da model ne uzima u obzir ulazne podatke i djeluje kao *najgori linearni prediktor*, Å¡to je jednostavno srednja vrijednost rezultata. Vrijednost 1 znaÄi da moÅ¾emo savrÅ¡eno predvidjeti sve oÄekivane izlaze. U naÅ¡em sluÄaju, koeficijent je oko 0.06, Å¡to je priliÄno nisko.

TakoÄ‘er moÅ¾emo prikazati testne podatke zajedno s regresijskom linijom kako bismo bolje vidjeli kako regresija radi u naÅ¡em sluÄaju:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linear regression" src="../../../../translated_images/hr/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## Polinomska regresija

Druga vrsta linearne regresije je polinomska regresija. Dok ponekad postoji linearna veza izmeÄ‘u varijabli - Å¡to je bundeva veÄ‡a po volumenu, to je viÅ¡a cijena - ponekad ove veze ne mogu biti prikazane kao ravnina ili ravna linija.

âœ… Evo [joÅ¡ nekoliko primjera](https://online.stat.psu.edu/stat501/lesson/9/9.8) podataka koji bi mogli koristiti polinomsku regresiju

Pogledajte ponovno odnos izmeÄ‘u Datuma i Cijene. Izgleda li ovaj scatterplot kao da ga nuÅ¾no treba analizirati ravnom linijom? Ne mogu li cijene varirati? U tom sluÄaju moÅ¾ete pokuÅ¡ati polinomsku regresiju.

âœ… Polinomi su matematiÄki izrazi koji mogu sadrÅ¾avati jednu ili viÅ¡e varijabli i koeficijenata.

Polinomska regresija stvara zakrivljenu liniju koja bolje pristaje nelinearnim podacima. U naÅ¡em sluÄaju, ako u ulazne podatke ukljuÄimo kvadratnu varijablu `DayOfYear`, trebali bismo moÄ‡i modelirati podatke parabolom koja Ä‡e imati minimum u nekoj toÄki tijekom godine.

Scikit-learn ukljuÄuje korisno [pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) za kombiniranje razliÄitih koraka obrade podataka zajedno. **Pipeline** je lanac **procjenitelja**. U naÅ¡em sluÄaju, stvorit Ä‡emo pipeline koji prvo dodaje polinomske znaÄajke naÅ¡em modelu, a zatim trenira regresiju:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

KoriÅ¡tenje `PolynomialFeatures(2)` znaÄi da Ä‡emo ukljuÄiti sve polinome drugog stupnja iz ulaznih podataka. U naÅ¡em sluÄaju to Ä‡e jednostavno znaÄiti `DayOfYear`<sup>2</sup>, ali s dvije ulazne varijable X i Y, to Ä‡e dodati X<sup>2</sup>, XY i Y<sup>2</sup>. TakoÄ‘er moÅ¾emo koristiti polinome viÅ¡eg stupnja ako Å¾elimo.

Pipelinove se mogu koristiti na isti naÄin kao izvorni objekt `LinearRegression`, tj. moÅ¾emo `fit`-ati pipeline, a zatim koristiti `predict` za dobivanje predviÄ‘anja. Evo grafikona koji prikazuje testne podatke i aproksimacijsku krivulju:

<img alt="Polynomial regression" src="../../../../translated_images/hr/poly-results.ee587348f0f1f60b.webp" width="50%" />

KoriÅ¡tenjem polinomske regresije moÅ¾emo dobiti neÅ¡to niÅ¾u MSE i viÅ¡i koeficijent determinacije, ali ne znaÄajno. Moramo uzeti u obzir i druge znaÄajke!

> MoÅ¾ete vidjeti da su minimalne cijene bundeva opaÅ¾ene negdje oko NoÄ‡i vjeÅ¡tica. Kako to moÅ¾ete objasniti?

ğŸƒ ÄŒestitamo, upravo ste stvorili model koji moÅ¾e pomoÄ‡i u predviÄ‘anju cijene pita od bundeva. Vjerojatno moÅ¾ete ponoviti isti postupak za sve vrste bundeva, ali to bi bilo zamorno. NauÄimo sada kako u model ukljuÄiti vrstu bundeve!

## Kategorijske znaÄajke

U idealnom svijetu Å¾elimo moÄ‡i predvidjeti cijene za razliÄite sorte bundeva koristeÄ‡i isti model. MeÄ‘utim, stupac `Variety` se razlikuje od stupaca poput `Month`, jer sadrÅ¾i nenumeriÄke vrijednosti. Takvi se stupci nazivaju **kategorijskim**.

[![ML za poÄetnike - PredviÄ‘anja kategorijskih znaÄajki linearnom regresijom](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML za poÄetnike - PredviÄ‘anja kategorijskih znaÄajki linearnom regresijom")

> ğŸ¥ Kliknite gornju sliku za kratak video pregled koriÅ¡tenja kategorijskih znaÄajki.

Ovdje moÅ¾ete vidjeti kako prosjeÄna cijena ovisi o sorti:

<img alt="Average price by variety" src="../../../../translated_images/hr/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

Da bismo uzeli sortu u obzir, prvo je moramo pretvoriti u numeriÄki oblik, ili je **enkodirati**. Postoji nekoliko naÄina kako to moÅ¾emo uÄiniti:

* Jednostavna **numeriÄka enkodiranja** Ä‡e napraviti tablicu razliÄitih sorti, a zatim zamijeniti ime sorte indeksom u toj tablici. To nije najbolja ideja za linearnu regresiju, jer linearna regresija uzima stvarnu numeriÄku vrijednost indeksa i dodaje je rezultatu mnoÅ¾eÄ‡i s nekim koeficijentom. U naÅ¡em sluÄaju, veza izmeÄ‘u broja indeksa i cijene je jasno nelinearna, Äak i ako osiguramo da su indeksi ureÄ‘eni na neki specifiÄan naÄin.
* **One-hot enkodiranje** zamijenit Ä‡e stupac `Variety` s 4 razliÄita stupca, po jedan za svaku sortu. Svaki stupac Ä‡e sadrÅ¾avati `1` ako je odgovarajuÄ‡i redak odreÄ‘ene sorte, a `0` inaÄe. To znaÄi da Ä‡e postojati Äetiri koeficijenta u linearnoj regresiji, po jedan za svaku sortu bundeve, koji Ä‡e biti odgovorni za "poÄetnu cijenu" (ili bolje reÄeno "dodatnu cijenu") za tu odreÄ‘enu sortu.

Donji kÃ´d pokazuje kako moÅ¾emo one-hot enkodirati sortu:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Da bismo trenirali linearnu regresiju koristeÄ‡i one-hot enkodiranu sortu kao ulaz, samo trebamo pravilno inicijalizirati podatke `X` i `y`:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Ostatak koda je isti kao Å¡to smo gore koristili za treniranje Linearne Regresije. Ako to probate, vidjet Ä‡ete da je srednja kvadratna pogreÅ¡ka pribliÅ¾no ista, ali dobivamo puno veÄ‡i koeficijent determinacije (~77%). Da bismo dobili joÅ¡ toÄnija predviÄ‘anja, moÅ¾emo uzeti u obzir joÅ¡ kategorijskih znaÄajki, kao i numeriÄkih znaÄajki, poput `Month` ili `DayOfYear`. Da bismo dobili jedan veliki niz znaÄajki, moÅ¾emo koristiti `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Ovdje takoÄ‘er uzimamo u obzir `City` i tip `Package`, Å¡to nam daje MSE 2.84 (10%), i determinaciju 0.94!

## Sve spojeno

Da bismo napravili najbolji model, moÅ¾emo koristiti kombinirane (one-hot enkodirane kategorijske + numeriÄke) podatke iz prethodnog primjera zajedno s polinomskom regresijom. Evo kompletnog koda radi vaÅ¡e udobnosti:

```python
# postavi podatke za trening
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# napravi podjelu podataka na trening i test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# postavi i treniraj pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predvidi rezultate za testne podatke
pred = pipeline.predict(X_test)

# izraÄunaj MSE i koeficijent determinacije
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

To bi nam trebalo dati najbolji koeficijent determinacije od gotovo 97%, i MSE=2.23 (~8% pogreÅ¡ka predviÄ‘anja).

| Model | MSE | Determinacija |
|-------|-----|--------------|
| Linearni `DayOfYear` | 2.77 (17.2%) | 0.07 |
| Polinomski `DayOfYear` | 2.73 (17.0%) | 0.08 |
| Linearna `Variety` | 5.24 (19.7%) | 0.77 |
| Sve znaÄajke Linearno | 2.84 (10.5%) | 0.94 |
| Sve znaÄajke Polinomski | 2.23 (8.25%) | 0.97 |

ğŸ† Svaka Äast! Stvorili ste Äetiri modela regresije u jednoj lekciji i poboljÅ¡ali kvalitetu modela na 97%. U zavrÅ¡nom dijelu o regresiji nauÄit Ä‡ete o logistiÄkoj regresiji za odreÄ‘ivanje kategorija.

---
## ğŸš€Izazov

Testirajte nekoliko razliÄitih varijabli u ovom biljeÅ¾niku da vidite kako korelacija odgovara toÄnosti modela.

## [Kviz nakon predavanja](https://ff-quizzes.netlify.app/en/ml/)

## Pregled i samostalan rad

U ovoj lekciji smo nauÄili o linearnoj regresiji. Postoje i druge vaÅ¾ne vrste regresije. ProÄitajte o tehnikama Stepwise, Ridge, Lasso i Elasticnet. Dobar teÄaj za daljnje prouÄavanje je [Stanford Statistical Learning course](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## Zadatak

[Izradi model](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Odricanje od odgovornosti**:
Ovaj je dokument preveden pomoÄ‡u AI usluge prevoÄ‘enja [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati greÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kritiÄne informacije preporuÄa se profesionalni prijevod ljudskog prevoditelja. Ne snosimo odgovornost za bilo kakva nesporazumevanja ili pogreÅ¡ne interpretacije nastale koriÅ¡tenjem ovog prijevoda.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->