# Ehita regressioonimudel, kasutades Scikit-learn'i: regressioon neli erinevat viisi

## Algaja m√§rkus

Lineaarset regressiooni kasutatakse, kui tahame prognoosida **numbrilist v√§√§rtust** (n√§iteks maja hind, temperatuur v√µi m√º√ºk).
See t√∂√∂tab leidmisega sirgjoont, mis k√µige paremini esindab seost sisendfunktsioonide ja v√§ljundi vahel.

Selles √µppet√ºkis keskendume m√µiste m√µistmisele enne keerukamate regressioonitehnikate uurimist.
![Lineaarne ja pol√ºnoomne regressioon infograafik](../../../../translated_images/et/linear-polynomial.5523c7cb6576ccab.webp)
> Infograafik autorilt [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Eelnev loengu visuaaltest](https://ff-quizzes.netlify.app/en/ml/)

> ### [See √µppet√ºkk on saadaval ka R keeles!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Sissejuhatus

Senini oled uurinud, mis on regressioon, kasutades n√§idisandmeid k√µrvitsahinnast, mida kasutame kogu √µppet√ºki v√§ltel. Oled seda ka visualiseerinud Matplotlib'i abil.

N√º√ºd oled valmis s√ºvenema regressioonisse masin√µppes. Kuigi visualiseerimine aitab andmeid m√µista, tuleb masin√µppe t√µeline j√µud _mudelite koolitamisest_. Mudelid koolitatakse ajalooliste andmete p√µhjal, et automaatselt j√§√§dvustada andmete s√µltuvused, v√µimaldades ennustada tulemusi uutele andmetele, mida mudel varem n√§inud ei ole.

Selles √µppet√ºkis √µpid tundma kahte regressioonit√º√ºpi: _lihtsat lineaarset regressiooni_ ja _pol√ºnoomset regressiooni_, koos m√µningate matemaatiliste aluste selgitustega. Need mudelid v√µimaldavad meil ennustada k√µrvitsahindu erinevate sisendandmete p√µhjal.

[![Masin√µpe algajatele - Lineaarse regressiooni m√µistmine](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "Masin√µpe algajatele - Lineaarse regressiooni m√µistmine")

> üé• Kl√µpsa √ºlalolevale pildile, et vaadata l√ºhikest √ºlevaadet lineaarse regressiooni kohta.

> Selle √µppekava jooksul eeldame v√§hest matemaatikateadmiste hulka ja p√º√ºame muuta selle teiste valdkondade √ºli√µpilastele arusaadavaks, nii et j√§lgi m√§rkusi, üßÆ selgitusi, diagramme ja teisi √µppematerjale, mis h√µlbustavad m√µistmist.

### Eelteadmised

Sul peaks n√º√ºd olema tuttav k√µrvitsaandmete struktuur, mida me uurime. Need on eelnevalt laetud ja puhastatud antud √µppet√ºki _notebook.ipynb_ failis. Failis kuvatakse k√µrvitsahind busheli kohta uues andmeraamis. Veendu, et suudad need m√§rkmikud Visual Studio Code'is tuumades jooksutada.

### Ettevalmistus

Meenutuseks: sa laadid neid andmeid, et esitada neile k√ºsimusi.

- Millal on parim aeg k√µrvitsaid osta?
- Millist hinda v√µib oodata minik√µrvitsate paki eest?
- Kas peaksin ostma neid poolbusheli korvides v√µi 1 1/9 busheli karbis?
J√§tkame andmete uurimist.

Eelmisel √µppetunnil l√µid Pandase andmeraami ja t√§itsid selle osa algsest andmestikust, standardiseerides hinnad busheli alusel. Selle tegemisega kogusid siiski ainult umbes 400 andmepunkti ja ainult s√ºgisekuude kohta.

Vaata andmeid, mida me eelnevalt laadsime selle √µppet√ºki kaasasolevasse m√§rkmikku. Andmed on eelnevalt laetud ja tehtud algne hajuvusdiagramm, mis n√§itab kuup√µhiseid andmeid. V√µib-olla saame andmete olemuse kohta veidi detailsemalt teada, tehes veel puhastust.

## Lineaarse regressioonijoon

Nagu √µppet√ºkis 1 √µppisid, on lineaarse regressiooni eesm√§rk joonistada joon, mis:

- **N√§itab muutujate seoseid**. N√§itab seost muutujate vahel
- **Teeb ennustusi**. Teeb t√§pseid prognoose, kuhu uus andmepunkt joone suhtes langeb.
 
Tavaliselt joonistatakse sellist joont meetodi **V√§himate ruutude regressioon** abil. M√µiste "V√§himate ruutude" viitab meie mudeli koguviga minimeerimise protsessile. Iga andmepunkti puhul m√µ√µdame vertikaalkauguse (nimetatakse j√§√§kveaks) tegeliku punkti ja regressioonijoone vahel.

Me ruudutame need kaugused kahe peamise p√µhjuse t√µttu:

1. **Suurema t√§htsus kui suuna puhul:** Tahame, et -5 vea suurus oleks sama kui +5 viga. Ruudutamine teeb k√µik v√§√§rtused positiivseks.

2. **Ebatavaliste v√§√§rtuste karistamine:** Ruudutamine annab suurematele vigadele suurema kaalu, sundides joont olema l√§hemal kaugel olevatele punktidele.

Seej√§rel liidame need ruudutatud v√§√§rtused kokku. Meie eesm√§rk on leida see konkreetne joon, millel see summa on minimaalne (v√µimalikult v√§ike v√§√§rtus) ‚Äî seega nimi "V√§himate ruutude".

> **üßÆ N√§ita mulle matemaatikat**  
>  
> Seda joont, mida nimetatakse _parima sobivusega joon_, saab v√§ljendada [valemi abil](https://en.wikipedia.org/wiki/Simple_linear_regression):  
>  
> ```
> Y = a + bX
> ```
>
> `X` on 'selgitav muutuja'. `Y` on 's√µltuv muutuja'. Joonte kalle on `b` ja `a` on y-l√µikepunkt, mis t√§histab `Y` v√§√§rtust, kui `X = 0`.
>
>![kallet arvutada](../../../../translated_images/et/slope.f3c9d5910ddbfcf9.webp)
>
> Esiteks arvuta kalle `b`. Infograafik autorilt [Jen Looper](https://twitter.com/jenlooper)
>
> Teisis√µnu, viidates meie k√µrvitsaandmete algsele k√ºsimusele: "prognoosi k√µrvitsa hind busheli kohta kuu j√§rgi", viitab `X` hinnale ja `Y` m√º√ºgikuule.
>
>![valemi l√µpetamine](../../../../translated_images/et/calculation.a209813050a1ddb1.webp)
>
> Arvuta `Y` v√§√§rtus. Kui maksad umbes 4 dollarit, peab see olema aprill! Infograafik autorilt [Jen Looper](https://twitter.com/jenlooper)
>
> Matemaatika, mis arvutab joone, peab demonstreerima joone kalde, mis s√µltub ka l√µikepunktist, ehk kus `Y` asub, kui `X = 0`.
>
> Saad vaadata nende v√§√§rtuste arvutusmeetodit veebisaidil [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). Samuti k√ºlasta [kasutades v√§himate ruutude kalkulaatorit](https://www.mathsisfun.com/data/least-squares-calculator.html), et n√§ha, kuidas numbrite v√§√§rtused m√µjutavad joont.

## Korrelatsioon

Veel √ºks m√µiste, mida m√µista, on **Korrelatsioonikordaja** antud X ja Y muutujate vahel. Hajuvusdiagrammiga saab seda kordajat kiiresti visualiseerida. Kui andmepunktid paiknevad korrap√§rases reas, on k√µrge korrelatsioon; kui punktid on hajutatud k√µikjale X ja Y vahel, on korrelatsioon madal.

Hea lineaarne regressioonimudel omab k√µrget (l√§hemal 1-le kui 0-le) korrelatsioonikordajat, kasutades V√§himate ruutude regressiooni meetodit koos regressioonijoonega.

‚úÖ K√§ivita selle √µppet√ºki m√§rkmik ja vaata kuup√µhist hinna hajuvusdiagrammi. Kas andmed, mis seovad kuup√§eva ja k√µrvitsate hinna, n√§ivad omavat suurt v√µi v√§ikest korrelatsiooni vastavalt sinu visuaalsele t√µlgendusele hajuvusdiagrammil? Kas see muutub, kui kasutad peenemat m√µ√µdikut kui `Kuu`, nt *aasta p√§ev* (st p√§evade arv aasta algusest)?

J√§rgmises koodis eeldame, et andmed on puhastatud ja meil on andmeraamistik nimega `new_pumpkins`, mis on sarnane j√§rgmisele:

ID | Kuu | AastaP√§ev | T√º√ºp | Linn | Pakk | Madal hind | K√µrge hind | Hind
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | K√ïRVITSA TIPP | BALTIMORE | 1 1/9 busheli kastid | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | K√ïRVITSA TIPP | BALTIMORE | 1 1/9 busheli kastid | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | K√ïRVITSA TIPP | BALTIMORE | 1 1/9 busheli kastid | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | K√ïRVITSA TIPP | BALTIMORE | 1 1/9 busheli kastid | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | K√ïRVITSA TIPP | BALTIMORE | 1 1/9 busheli kastid | 15.0 | 15.0 | 13.636364

> Andmete puhastamise kood on saadaval failis [`notebook.ipynb`](notebook.ipynb). Oleme teinud samad puhastamise sammud nagu eelnevas √µppet√ºkis ning arvutanud `AastaP√§ev` veeru j√§rgmiselt: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

N√º√ºd, kui sul on arusaam lineaarse regressiooni matemaatikast, loome regressioonimudeli, et n√§ha, kas v√µime ennustada, milline k√µrvitsapakend annab parimad hinnad. Keegi, kes ostab k√µrvitsaid p√ºha-k√µrvitsapeenrale, v√µib vajada seda infot, et osta parima hinnaga k√µrvitsapakette.

## Korrelatsiooni otsimine

[![Masin√µpe algajatele - korrelatsiooni otsimine: lineaarse regressiooni v√µti](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "Masin√µpe algajatele - korrelatsiooni otsimine: lineaarse regressiooni v√µti")

> üé• Kl√µpsa √ºlalolevale pildile, et vaadata l√ºhikest √ºlevaadet korrelatsioonist.

Eelmisest √µppet√ºkist oled ilmselt n√§inud, et kuu keskmine hind n√§eb v√§lja selline:

<img alt="Keskmine hind kuu kaupa" src="../../../../translated_images/et/barchart.a833ea9194346d76.webp" width="50%"/>

See viitab v√µimalusele, et korrelatsioon v√µiks eksisteerida, ja v√µime proovida treenida lineaarset regressioonimudelit, mis ennustab seost `Kuu` ja `Hind` vahel v√µi `AastaP√§ev` ja `Hind` vahel. Siin on hajuvusdiagramm, mis n√§itab viimast seost:

<img alt="Hajuvusdiagramm hinna ja aasta p√§eva vahel" src="../../../../translated_images/et/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" /> 

Vaatame, kas korrelatsiooni on, kasutades funktsiooni `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Tundub, et korrelatsioon on suhteliselt v√§ike, -0.15 `Kuu` j√§rgi ja -0.17 `AastaP√§eva` j√§rgi, kuid v√µib olla m√µni teine t√§htis seos. N√§ib, et hinnad jagunevad erinevatesse gruppidesse vastavalt k√µrvitsat√º√ºbile. Selle kinnitamiseks joonistame iga k√µrvitsaliigi erinevas v√§rvitoonis. Andmepunktide samaaegseks joonistamiseks peame `scatter` joonistamismetoodil kasutama parameetrit `ax`:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Hajuvusdiagramm hinna ja aasta p√§eva vahel v√§rviliselt" src="../../../../translated_images/et/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" /> 

Meie uurimine viitab, et liigiga on suurem m√µju √ºldisele hinnale kui m√º√ºgikuup√§eval. Seda n√§eme ka tulpdiagrammilt:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Tulpdiagramm hindade kohta liigiti" src="../../../../translated_images/et/price-by-variety.744a2f9925d9bcb4.webp" width="50%" /> 

Keskendume hetkel ainult √ºhele k√µrvitsaliigile, 'pirukaliigile', ja vaatame, kuidas hind s√µltub kuup√§evast:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Hajuvusdiagramm hinna ja aasta p√§eva vahel 'pirukaliigile'" src="../../../../translated_images/et/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" /> 

Kui n√º√ºd arvutada korrelatsioon `Price` ja `DayOfYear` vahel funktsiooniga `corr`, saame ligikaudu `-0.27` - mis t√§hendab, et prognoosimudeli koolitamine on m√µistlik.

> Enne lineaarse regressioonimudeli koolitamist on oluline veenduda, et andmed on puhtad. Lineaarne regressioon ei t√∂√∂ta h√§sti puuduvate v√§√§rtustega, seega on m√µistlik t√ºhjad lahtrid eemaldada:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Teine v√µimalus oleks need t√ºhjad v√§√§rtused asendada vastava veeru keskmisega.

## Lihtne lineaarne regressioon

[![Masin√µpe algajatele - lineaarne ja pol√ºnoomne regressioon Scikit-learniga](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "Masin√µpe algajatele - lineaarne ja pol√ºnoomne regressioon Scikit-learniga")

> üé• Kl√µpsa √ºlalolevale pildile, et vaadata l√ºhikest √ºlevaadet lineaarse ja pol√ºnoomse regressiooni kohta.

Me koolitame oma lineaarse regressioonimudeli, kasutades **Scikit-learn'i** teeki.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Alustame sisendv√§√§rtuste (tunnuste) ja oodatud v√§ljundi (sildi) eraldamisest eraldi numpy massiivideks:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> M√§rka, et pidime sisendandmeid t√∂√∂tlema `reshape` abil, et LinearRepression pakett saaks neid √µigesti m√µista. Lineaarne regressioon eeldab 2D massiivi sisendina, kus iga rea vastab sisendfunktsioonide vektorile. Meie puhul, kui meil on ainult √ºks sisend, vajame massiivi kuju N &times; 1, kus N on andmestiku suurus.

Seej√§rel peame andmed jagama koolitus- ja testandmeteks, et saaksime mudelit p√§rast koolitust valideerida:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

L√µpuks v√µtab lineaarse regressioonimudeli koolitus vaid kaks koodirida. Defineerime `LinearRegression` objekti ja sobitame selle meie andmetega meetodiga `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

`LinearRegression` objekt sisaldab p√§rast `fit`-imist regressiooni k√µiki koefitsiente, millele p√§√§seb ligi `.coef_` omaduse kaudu. Meie puhul on vaid √ºks koefitsient, mis peaks olema umbes `-0.017`. See t√§hendab, et hinnad paistavad aja jooksul veidi langemas, aga mitte liiga palju, umbes 2 senti p√§evas. Samuti saame regressiooni l√µikepunkti Y-teljel k√§tte `lin_reg.intercept_` abil - see on meie puhul umbes `21`, mis n√§itab hinna v√§√§rtust aasta alguses.

Selleks, et n√§ha, kui t√§pne meie mudel on, saame prognoosida hindu testandmestikul ja seej√§rel m√µ√µta, kui l√§hedal on meie prognoosid oodatud v√§√§rtustele. Seda saab teha ruutkeskmise vea (MSE) m√µ√µdikuga, mis on k√µigi ruutude keskmine erinevus oodatud ja prognoositud v√§√§rtuste vahel.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

Meie viga paistab olevat umbes 2 punkti, mis on umbes 17%. Mitte kuigi hea. Teine mudeli kvaliteedi n√§itaja on **determinisatsioonikordaja**, mida saab saada j√§rgmise koodiga:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Kui v√§√§rtus on 0, t√§hendab see, et mudel ei v√µta sisendandmeid arvesse ja toimib nagu *k√µige halvem lineaarne prognoosija*, mis on lihtsalt tulemuse keskmine v√§√§rtus. V√§√§rtus 1 t√§hendab, et suudame k√µiki oodatud v√§ljundeid t√§iuslikult prognoosida. Meie puhul on kordaja umbes 0.06, mis on √ºsna madal.

Testandmeid v√µime koos regressioonijoonisega joonistada, et paremini n√§ha, kuidas regressioon meie puhul t√∂√∂tab:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Lineaarne regressioon" src="../../../../translated_images/et/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## Pol√ºnoomne regressioon

Teine lineaarse regressiooni t√º√ºp on pol√ºnoomne regressioon. Kuigi m√µnikord on muutujate vahel lineaarne seos - n√§iteks suurem k√µrvits mahult t√§hendab k√µrgemat hinda -, siis m√µnikord neid seoseid ei saa joonistada tasandina ega sirgjoonena.

‚úÖ Siin on [veel m√µned n√§ited](https://online.stat.psu.edu/stat501/lesson/9/9.8) andmetest, mille puhul v√µiks kasutada pol√ºnoomset regressiooni

V√µta veelkord pilk peale seosele kuup√§eva ja hinna vahel. Kas see hajuvusdiagramm tundub kindlasti nii, et seda peaks tingimata anal√º√ºsima sirgjoonega? Kas hinnad ei k√µigu? Sellisel juhul v√µid proovida pol√ºnoomset regressiooni.

‚úÖ Pol√ºnoomid on matemaatilised avaldised, mis v√µivad koosneda √ºhest v√µi mitmest muutujast ja koefitsiendist

Pol√ºnoomne regressioon loob k√µverjoone, mis paremini sobib mittelineaarsete andmetega. Meie puhul, kui lisame sisendandmetesse ruutfunktsiooni `DayOfYear` muutujast, peaksime suutma sobitada andmeid paraboolkuvaga, millel on aasta jooksul mingi miinimumtipp.

Scikit-learn sisaldab kasulikku [pipeline API-d](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline), et kombineerida erinevad andmet√∂√∂tluse sammud. **Pipelines** on **estimatsioonide** kett. Meie puhul loome v√§√§rtusahelas esimese sammuna pol√ºnoomsed tunnused ja seej√§rel treenime regressiooni:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

`PolynomialFeatures(2)` kasutamine t√§hendab, et kaasame k√µik teise astme pol√ºnoomid sisendandmetest. Meie puhul t√§hendab see ainult `DayOfYear`<sup>2</sup>, aga kui on kaks sisendmuutujat X ja Y, lisab see X<sup>2</sup>, XY ja Y<sup>2</sup>. Saame kasutada ka k√µrgema astme pol√ºnoome, kui soovime.

Pipeline'idega saab t√∂√∂d teha samamoodi nagu algse `LinearRegression` objektiga, s.t. saame `fit` v√§√§rtusahela ja seej√§rel `predict` meetodit kasutades saada prognoosid. J√§rgneval graafikul on testandmed ja ligikaudne k√µver:

<img alt="Pol√ºnoomne regressioon" src="../../../../translated_images/et/poly-results.ee587348f0f1f60b.webp" width="50%" />

Pol√ºnoomse regressiooni kasutamisel saame kergelt madalama MSE ja k√µrgema determinatsiooni, kuid mitte m√§rkimisv√§√§rselt. Peame arvesse v√µtma ka teisi tunnuseid!

> N√§ed, et k√µrvitsate hinnad on minimaalsed kusagil √µuduspeo (Halloween) ajal. Kuidas seda seletada?

üéÉ Palju √µnne, sa just l√µid mudeli, mis aitab prognoosida kookk√µrvitsate hinda. Saame t√µen√§oliselt sama protseduuri korrata k√µigi k√µrvitsaliikide jaoks, aga see oleks t√º√ºtu. √ïpime n√º√ºd, kuidas mudelis arvestada k√µrvitsa sorti!

## Kategoorilised tunnused

Ideaalis tahame suuta prognoosida erinevate k√µrvitsaliikide hindu sama mudeli abil. Kuid `Variety` (sort) veerg erineb sellistest veergudest nagu `Month` (kuu), sest see sisaldab mitte-arvulisi v√§√§rtusi. Selliseid veerge nimetatakse **kategoorilisteks**.

[![ML algajatele - kategooriliste tunnuste prognoosimine lineaarse regressiooniga](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML algajatele - kategooriliste tunnuste prognoosimine lineaarse regressiooniga")

> üé• Kl√µpsa √ºlaloleval pildil, et n√§ha l√ºhikest videot kategooriliste tunnuste kasutamisest.

Siin n√§ed, kuidas keskmine hind s√µltub sordist:

<img alt="Keskmine hind sortide kaupa" src="../../../../translated_images/et/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

Sordi arvesse v√µtmiseks peame esmalt selle numbriliseks muutma ehk **kodeerima**. Selleks on mitmeid v√µimalusi:

* Lihtne **numbriline kodeerimine** loob nimekirja erinevatest sortidest ja asendab seej√§rel sordinime selle nimekirja indeksi vastu. See pole lineaarse regressiooni jaoks parim idee, sest lineaarne regressioon kasutab indeksi tegelikku numbrilist v√§√§rtust ja lisab seda tulemusele teatud koefitsiendiga. Meie puhul on seos indeksi numbri ja hinna vahel selgelt mittelineaarne, isegi kui indekseid j√§rjestada kindlal viisil.
* **One-hot kodeerimine** asendab `Variety` veeru nelja eraldi veeruga, √ºhe iga sordi jaoks. Igas veerus on v√§√§rtuseks `1`, kui vastav rida on antud sorti, ja `0` muul juhul. See t√§hendab, et lineaarse regressiooni jaoks on neli koefitsienti, √ºks iga k√µrvitsaliigi jaoks, mis vastutab antud sordi "algusehinna" (v√µi pigem "t√§iendava hinna") eest.

Allj√§rgnev kood n√§itab, kuidas sorti one-hot kodeerida:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | SEGASED P√ÑRANDLIIGID | KOOKK√ïRVITS
----|-----------|-----------|---------------------|------------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Et kasutada lineaarset regressiooni one-hot kodeeritud sortide p√µhjal, peame lihtsalt √µigesti algatama andmed `X` ja `y`:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

√úlej√§√§nud kood on sama, mida kasutati eelnevalt lineaarse regressiooni treenimiseks. Kui proovida, n√§eme, et ruutkeskmine viga on umbes sama, kuid determinatsioonikordaja on palju k√µrgem (~77%). Veelgi t√§psemate prognooside saamiseks v√µime arvesse v√µtta rohkem kategoorilisi tunnuseid ning ka numbrilisi tunnuseid nagu `Month` v√µi `DayOfYear`. K√µigi tunnuste √ºhtseks massiiviks saamiseks v√µime kasutada `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Siin v√µtame arvesse ka `City` ja `Package` t√º√ºpi, mis annab meile MSE v√§√§rtuse 2.84 (10%) ja determinatsiooni 0.94!

## K√µik kokku

Parima mudeli koostamiseks v√µime kasutada √ºlaltoodud n√§ite kombineeritud (one-hot kodeeritud kategoorilised + numbrilised) andmed koos pol√ºnoomse regressiooniga. Siin on mugavaks kasutamiseks t√§ielik kood:

```python
# seadista treeningandmed
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# tee koolitus- ja testandmete jagamine
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# seadista ja treeni andmet√∂√∂tluskanal
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# ennusta tulemused testandmete jaoks
pred = pipeline.predict(X_test)

# arvuta keskmine ruutviga ja m√§√§ramisaste
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

See peaks andma parima determinatsioonikordaja, mis on peaaegu 97%, ning MSE=2.23 (~8% ennustuse viga).

| Mudel | MSE | Determinatsioon |
|-------|-----|-----------------|
| `DayOfYear` lineaarne | 2.77 (17.2%) | 0.07 |
| `DayOfYear` pol√ºnoomne | 2.73 (17.0%) | 0.08 |
| `Variety` lineaarne | 5.24 (19.7%) | 0.77 |
| K√µik tunnused lineaarne | 2.84 (10.5%) | 0.94 |
| K√µik tunnused pol√ºnoomne | 2.23 (8.25%) | 0.97 |

üèÜ V√§ga h√§sti! Sa l√µid nelja mudelit √ºhes √µppet√ºkis ja parandasid mudeli kvaliteedi 97% peale. Regressiooni viimases osas √µpid logistilisest regressioonist, mida kasutatakse kategooriate m√§√§ramiseks.

---
## üöÄV√§ljakutse

Proovi selles m√§rkmes mitu erinevat muutujat, et n√§ha, kuidas korrelatsioon vastab mudeli t√§psusele.

## [Loengu j√§rgse test](https://ff-quizzes.netlify.app/en/ml/)

## Kordamine & iseseisev √µppimine

Selles √µppet√ºkis √µppisime lineaarset regressiooni. On ka teisi olulisi regressioonit√º√ºpe. Loe Stepwise-, Ridge-, Lasso- ja Elasticnet-meetodite kohta. Heaks √µppeallikaks on [Stanfordi statistilise √µppe kursus](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Kodut√∂√∂

[Ehita mudel](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Vastutusest loobumine**:
See dokument on t√µlgitud kasutades tehisintellekti t√µlkimisteenust [Co-op Translator](https://github.com/Azure/co-op-translator). Kuigi p√º√ºame tagada t√§psust, palun arvestage, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algset dokumenti selle emakeeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitatakse kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlkega seotud arusaamatuste v√µi valesti m√µistmiste eest.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->