# Vytvorte regresnÃ½ model pomocou Scikit-learn: Å¡tyri spÃ´soby regresie

## PoznÃ¡mka pre zaÄiatoÄnÃ­kov

LineÃ¡rna regresia sa pouÅ¾Ã­va, keÄ chceme predpovedaÅ¥ **ÄÃ­selnÃº hodnotu** (naprÃ­klad cenu domu, teplotu alebo predaj).
Funguje tak, Å¾e nÃ¡jde priamku, ktorÃ¡ najlepÅ¡ie reprezentuje vzÅ¥ah medzi vstupnÃ½mi premennÃ½mi a vÃ½stupom.

V tejto lekcii sa zameriavame na pochopenie konceptu predtÃ½m, neÅ¾ preskÃºmame pokroÄilejÅ¡ie regresnÃ© techniky.
![LineÃ¡rna vs polynomiÃ¡lna regresia infografika](../../../../translated_images/sk/linear-polynomial.5523c7cb6576ccab.webp)
> Infografika od [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [KvÃ­z pred prednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ml/)

> ### [TÃ¡to lekcia je dostupnÃ¡ aj v R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Ãšvod

Doteraz ste preskÃºmali, Äo je regresia, pomocou vzorovÃ½ch Ãºdajov zo sÃºboru Ãºdajov o cene tekvÃ­c, ktorÃ½ budeme pouÅ¾Ã­vaÅ¥ poÄas celej tejto lekcie. TieÅ¾ ste ich vizualizovali pomocou Matplotlib.

Teraz ste pripravenÃ­ ponoriÅ¥ sa hlbÅ¡ie do regresie pre ML. ZatiaÄ¾ Äo vizualizÃ¡cia umoÅ¾Åˆuje lepÅ¡ie pochopiÅ¥ Ãºdaje, skutoÄnÃ¡ sila strojovÃ©ho uÄenia pochÃ¡dza z _trÃ©novania modelov_. Modely sa trÃ©nujÃº na historickÃ½ch dÃ¡tach, aby automaticky zachytili zÃ¡vislosti v dÃ¡tach, a umoÅ¾ÅˆujÃº vÃ¡m predpovedaÅ¥ vÃ½sledky pre novÃ© dÃ¡ta, ktorÃ© model predtÃ½m nevidel.

V tejto lekcii sa dozviete viac o dvoch typoch regresie: _zÃ¡kladnej lineÃ¡rnej regresii_ a _polynomiÃ¡lnej regresii_ spolu s niektorou z matematiky, ktorÃ¡ stojÃ­ za tÃ½mito technikami. Tieto modely nÃ¡m umoÅ¾nia predpovedaÅ¥ ceny tekvÃ­c v zÃ¡vislosti od rÃ´znych vstupnÃ½ch Ãºdajov.

[![ML pre zaÄiatoÄnÃ­kov - Pochopenie lineÃ¡rnej regresie](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML pre zaÄiatoÄnÃ­kov - Pochopenie lineÃ¡rnej regresie")

> ğŸ¥ Kliknite na obrÃ¡zok vyÅ¡Å¡ie pre krÃ¡tke video o lineÃ¡rnej regresii.

> PoÄas celÃ©ho kurikula predpokladÃ¡me minimÃ¡lne znalosti matematiky a snaÅ¾Ã­me sa ich sprÃ­stupniÅ¥ Å¡tudentom z inÃ½ch odborov, tak sledujte poznÃ¡mky, ğŸ§® upozornenia, diagramy a ÄalÅ¡ie vÃ½uÄbovÃ© pomÃ´cky na lepÅ¡ie pochopenie.

### Predpoklady

Teraz by ste uÅ¾ mali byÅ¥ oboznÃ¡menÃ­ so Å¡truktÃºrou Ãºdajov o tekviciach, ktorÃ© skÃºmame. NÃ¡jdete ich prednaÄÃ­tanÃ© a predvyÄistenÃ© v sÃºbore _notebook.ipynb_ tejto lekcie. V sÃºbore je cena tekvÃ­c uvedenÃ¡ za koÅ¡Ã­k v novom dÃ¡tovom rÃ¡mci. Uistite sa, Å¾e viete spustiÅ¥ tieto notebooky v kerneloch vo Visual Studio Code.

### PrÃ­prava

Ako pripomienku, naÄÃ­tavate tieto Ãºdaje, aby ste na nich mohli klÃ¡sÅ¥ otÃ¡zky.

- Kedy je najlepÅ¡Ã­ Äas na nÃ¡kup tekvÃ­c? 
- AkÃº cenu mÃ´Å¾em oÄakÃ¡vaÅ¥ za balenie mini tekvÃ­c?
- MÃ¡m ich kÃºpiÅ¥ v polkoÅ¡Ã­koch alebo v krabici 1 1/9 koÅ¡Ã­ka?
PoÄme sa Äalej ponoriÅ¥ do tohto dÃ¡tovÃ©ho sÃºboru.

V predchÃ¡dzajÃºcej lekcii ste vytvorili dÃ¡tovÃ½ rÃ¡mec Pandas a naplnili ho ÄasÅ¥ou pÃ´vodnÃ©ho datasetu, Å¡tandardizujÃºc ceny podÄ¾a koÅ¡Ã­ka. TÃ½mto ste vÅ¡ak zÃ­skali iba asi 400 dÃ¡tovÃ½ch bodov a len pre jesennÃ© mesiace.

Pozrite si Ãºdaje, ktorÃ© sme prednaÄÃ­tali v sprievodnom notebooku tejto lekcie. Ãšdaje sÃº predpripravenÃ© a prvÃ½ rozptÃ½lenÃ½ graf ukazuje mesiac predaja. MoÅ¾no pÃ´jdeme Äalej a vyÄistÃ­me dÃ¡ta podrobnejÅ¡ie.

## LineÃ¡rna regresnÃ¡ Äiara

Ako ste sa nauÄili v Lekcii 1, cieÄ¾om lineÃ¡rnej regresie je nakresliÅ¥ Äiaru, ktorÃ¡:

- **Ukazuje vzÅ¥ah medzi premennÃ½mi**. UkÃ¡Å¾e vzÅ¥ah medzi premennÃ½mi
- **PredpovedÃ¡**. UmoÅ¾nÃ­ presne predpovedaÅ¥, kde by novÃ½ dÃ¡tovÃ½ bod leÅ¾al vzhÄ¾adom na tÃºto Äiaru.

TypickÃ© na **regresii metÃ³dou najmenÅ¡Ã­ch Å¡tvorcov** je kreslenie takÃ©hoto druhu Äiary. TermÃ­n "najmenÅ¡ie Å¡tvorce" oznaÄuje proces minimalizÃ¡cie celkovej chyby v naÅ¡om modeli. Pre kaÅ¾dÃ½ dÃ¡tovÃ½ bod meriame vertikÃ¡lnu vzdialenosÅ¥ (nazÃ½vanÃº rezÃ­duum) medzi skutoÄnÃ½m bodom a regresnou Äiarou.

Tieto vzdialenosti umocÅˆujeme na druhÃº pre dva hlavnÃ© dÃ´vody:

1. **VeÄ¾kosÅ¥ pred smerom:** Chceme, aby chyba -5 bola rovnocennÃ¡ chybe +5. Umocnenie na druhÃº zmenÃ­ vÅ¡etky hodnoty na kladnÃ©.

2. **Trestenie odÄ¾ahlÃ½ch hodnÃ´t:** Umocnenie na druhÃº dÃ¡va vÃ¤ÄÅ¡iu vÃ¡hu vÃ¤ÄÅ¡Ã­m chybÃ¡m, nÃºti Äiaru zostaÅ¥ bliÅ¾Å¡ie k bodom, ktorÃ© sÃº vzdialenÃ©.

Tieto umocnenÃ© hodnoty potom sÄÃ­tame. NaÅ¡Ã­m cieÄ¾om je nÃ¡jsÅ¥ Äiaru, kde vÃ½slednÃ½ sÃºÄet bude Äo najmenÅ¡Ã­ (najmenÅ¡ia moÅ¾nÃ¡ hodnota) â€” odtiaÄ¾ nÃ¡zov "najmenÅ¡ie Å¡tvorce".

> **ğŸ§® UkÃ¡Å¾ mi matematiku** 
> 
> TÃ¡to Äiara, nazÃ½vanÃ¡ _Äiara najlepÅ¡ieho prispÃ´sobenia_, mÃ´Å¾e byÅ¥ vyjadrenÃ¡ [rovnicou](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` je 'vysvetÄ¾ujÃºca premennÃ¡'. `Y` je 'zÃ¡vislÃ¡ premennÃ¡'. Sklon Äiary je `b` a `a` je y-prieseÄnÃ­k, ktorÃ½ predstavuje hodnotu `Y` pre `X = 0`. 
>
>![vypoÄÃ­tajte sklon](../../../../translated_images/sk/slope.f3c9d5910ddbfcf9.webp)
>
> Najprv vypoÄÃ­tajte sklon `b`. Infografika od [Jen Looper](https://twitter.com/jenlooper)
>
> InÃ½mi slovami a odkazujÃºc na pÃ´vodnÃº otÃ¡zku naÅ¡ich Ãºdajov o tekviciach: "predpovedaÅ¥ cenu tekvice za koÅ¡Ã­k podÄ¾a mesiaca", `X` by predstavoval cenu a `Y` by oznaÄoval mesiac predaja.
>
>![dokonÄite rovnicu](../../../../translated_images/sk/calculation.a209813050a1ddb1.webp)
>
> VypoÄÃ­tajte hodnotu Y. Ak platÃ­te okolo 4 dolÃ¡rov, musÃ­ to byÅ¥ aprÃ­l! Infografika od [Jen Looper](https://twitter.com/jenlooper)
>
> Matematika, ktorÃ¡ poÄÃ­ta Äiaru, musÃ­ ukÃ¡zaÅ¥ sklon Äiary, ktorÃ½ zÃ¡visÃ­ aj od prieseÄnÃ­ka, teda kde sa `Y` nachÃ¡dza, keÄ `X = 0`.
>
> MetÃ³du vÃ½poÄtu tÃ½chto hodnÃ´t mÃ´Å¾ete vidieÅ¥ na webovej strÃ¡nke [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). NavÅ¡tÃ­vte tieÅ¾ [tento Least-squares kalkulÃ¡tor](https://www.mathsisfun.com/data/least-squares-calculator.html), aby ste videli, ako hodnoty ÄÃ­sel ovplyvÅˆujÃº Äiaru.

## KorelÃ¡cia

EÅ¡te jeden termÃ­n, ktorÃ½ je dobrÃ© pochopiÅ¥, je **KorelaÄnÃ½ koeficient** medzi danÃ½mi premennÃ½mi X a Y. Pomocou rozptÃ½lenÃ©ho grafu mÃ´Å¾ete rÃ½chlo vizualizovaÅ¥ tento koeficient. Graf, kde sÃº body rozptÃ½lenÃ© pozdÄºÅ¾ Äistej Äiary, mÃ¡ vysokÃº korelÃ¡ciu, zatiaÄ¾ Äo graf, kde sÃº body rozptÃ½lenÃ© vÅ¡ade medzi X a Y, mÃ¡ nÃ­zku korelÃ¡ciu.

DobrÃ½ lineÃ¡rny regresnÃ½ model bude takÃ½, ktorÃ½ mÃ¡ vysokÃ½ (bliÅ¾Å¡ie k 1 neÅ¾ k 0) KorelaÄnÃ½ koeficient pouÅ¾itÃ­m metÃ³dy najmenÅ¡Ã­ch Å¡tvorcov s regresnou Äiarou.

âœ… Spustite notebook sprevÃ¡dzajÃºci tÃºto lekciu a pozrite sa na rozptÃ½lenÃ½ graf Mesiac k Cene. ZdÃ¡ sa vÃ¡m, Å¾e dÃ¡ta spÃ¡jajÃºce Mesiac s Cenou predaja tekvÃ­c majÃº vysokÃº alebo nÃ­zku korelÃ¡ciu podÄ¾a vaÅ¡ej vizuÃ¡lnej interpretÃ¡cie rozptÃ½lenÃ©ho grafu? ZmenÃ­ sa to, ak namiesto `Month` pouÅ¾ijete detailnejÅ¡ie meranie, naprÃ­klad *deÅˆ v roku* (t.j. poÄet dnÃ­ od zaÄiatku roka)?

NiÅ¾Å¡ie v kÃ³de predpokladÃ¡me, Å¾e sme Ãºdaje vyÄistili a zÃ­skali dÃ¡tovÃ½ rÃ¡mec nazvanÃ½ `new_pumpkins`, podobnÃ½ nasledovnÃ©mu:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> KÃ³d na vyÄistenie dÃ¡t je dostupnÃ½ v [`notebook.ipynb`](notebook.ipynb). Vykonali sme rovnakÃ© Äistiace kroky ako v predchÃ¡dzajÃºcej lekcii a vypoÄÃ­tali stÄºpec `DayOfYear` pomocou nasledujÃºceho vÃ½razu:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

KeÄÅ¾e uÅ¾ rozumiete matematike za lineÃ¡rnou regresiou, vytvorme regresnÃ½ model, aby sme zistili, Äi vieme predpovedaÅ¥, ktorÃ© balenie tekvÃ­c bude maÅ¥ najlepÅ¡iu cenu. Niekto, kto kupuje tekvice na jesennÃº vÃ½zdobu, by moÅ¾no chcel tieto informÃ¡cie, aby mohol optimalizovaÅ¥ nÃ¡kup balenÃ­ tekvÃ­c pre svoj patch.

## HÄ¾adanie korelÃ¡cie

[![ML pre zaÄiatoÄnÃ­kov - HÄ¾adanie korelÃ¡cie: kÄ¾ÃºÄ k lineÃ¡rnej regresii](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML pre zaÄiatoÄnÃ­kov - HÄ¾adanie korelÃ¡cie: kÄ¾ÃºÄ k lineÃ¡rnej regresii")

> ğŸ¥ Kliknite na obrÃ¡zok vyÅ¡Å¡ie pre krÃ¡tke video o korelÃ¡cii.

Z predchÃ¡dzajÃºcej lekcie ste pravdepodobne videli, Å¾e priemernÃ¡ cena podÄ¾a mesiacov vyzerÃ¡ takto:

<img alt="PriemernÃ¡ cena podÄ¾a mesiaca" src="../../../../translated_images/sk/barchart.a833ea9194346d76.webp" width="50%"/>

To naznaÄuje, Å¾e nejakÃ¡ korelÃ¡cia tam bude, a mÃ´Å¾eme skÃºsiÅ¥ natrÃ©novaÅ¥ lineÃ¡rny regresnÃ½ model na predpovedanie vzÅ¥ahu medzi `Month` a `Price`, alebo medzi `DayOfYear` a `Price`. Tu je rozptÃ½lenÃ½ graf, ktorÃ½ ukazuje druhÃ½ vzÅ¥ah:

<img alt="RozptÃ½lenÃ½ graf Cena vs. DeÅˆ v roku" src="../../../../translated_images/sk/scatter-dayofyear.bc171c189c9fd553.webp" width="50%" /> 

SkÃºsme zistiÅ¥ korelÃ¡ciu pomocou funkcie `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

ZdÃ¡ sa, Å¾e korelÃ¡cia je pomerne malÃ¡, -0.15 podÄ¾a `Month` a -0.17 podÄ¾a `DayOfMonth`, ale mÃ´Å¾e tu byÅ¥ inÃ½ dÃ´leÅ¾itÃ½ vzÅ¥ah. VyzerÃ¡ to, Å¾e existujÃº rÃ´zne skupiny cien zodpovedajÃºce rÃ´znym odrodÃ¡m tekvÃ­c. Aby sme tÃºto hypotÃ©zu potvrdili, nakreslime kaÅ¾dÃº kategÃ³riu tekvÃ­c inou farbou. Pre odovzdanie parametra `ax` funkcii `scatter` mÃ´Å¾eme vykresliÅ¥ vÅ¡etky body do rovnakÃ©ho grafu:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="RozptÃ½lenÃ½ graf Cena vs. DeÅˆ v roku s farebnÃ½m rozlÃ­Å¡enÃ­m" src="../../../../translated_images/sk/scatter-dayofyear-color.65790faefbb9d54f.webp" width="50%" /> 

NaÅ¡e vyÅ¡etrovanie naznaÄuje, Å¾e odroda mÃ¡ vÃ¤ÄÅ¡Ã­ vplyv na celkovÃº cenu neÅ¾ samotnÃ½ dÃ¡tum predaja. VidÃ­me to aj na stÄºpcovom grafe:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="StÄºpcovÃ½ graf ceny podÄ¾a odrody" src="../../../../translated_images/sk/price-by-variety.744a2f9925d9bcb4.webp" width="50%" /> 

Zamerajme sa teraz na jednu odrodu tekvÃ­c, 'pie type', a pozrime sa, akÃ½ vplyv mÃ¡ dÃ¡tum na cenu:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="RozptÃ½lenÃ½ graf Cena vs. DeÅˆ v roku pre pie type" src="../../../../translated_images/sk/pie-pumpkins-scatter.d14f9804a53f927e.webp" width="50%" /> 

Ak teraz vypoÄÃ­tame korelÃ¡ciu medzi `Price` a `DayOfYear` pomocou funkcie `corr`, zÃ­skame pribliÅ¾ne `-0.27` â€” Äo znamenÃ¡, Å¾e natrÃ©novanie prediktÃ­vneho modelu mÃ¡ zmysel.

> Pred trÃ©novanÃ­m lineÃ¡rneho regresnÃ©ho modelu je dÃ´leÅ¾itÃ© zabezpeÄiÅ¥, Å¾e naÅ¡e dÃ¡ta sÃº ÄistÃ©. LineÃ¡rna regresia nefunguje dobre s chÃ½bajÃºcimi hodnotami, preto je rozumnÃ© zbaviÅ¥ sa vÅ¡etkÃ½ch prÃ¡zdnych buniek:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

InÃ½ prÃ­stup by bol vyplniÅ¥ tieto prÃ¡zdne hodnoty priemernÃ½mi hodnotami prÃ­sluÅ¡nÃ©ho stÄºpca.

## JednoduchÃ¡ lineÃ¡rna regresia

[![ML pre zaÄiatoÄnÃ­kov - LineÃ¡rna a polynomiÃ¡lna regresia pomocou Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML pre zaÄiatoÄnÃ­kov - LineÃ¡rna a polynomiÃ¡lna regresia pomocou Scikit-learn")

> ğŸ¥ Kliknite na obrÃ¡zok vyÅ¡Å¡ie pre krÃ¡tke video o lineÃ¡rnej a polynomiÃ¡lnej regresii.

Na trÃ©novanie nÃ¡Å¡ho lineÃ¡rneho regresnÃ©ho modelu pouÅ¾ijeme kniÅ¾nicu **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

ZaÄneme tÃ½m, Å¾e oddelÃ­me vstupnÃ© hodnoty (vlastnosti) a oÄakÃ¡vanÃ½ vÃ½stup (Å¡tÃ­tok) do samostatnÃ½ch numpy polÃ­:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> VÅ¡imnite si, Å¾e sme museli vykonaÅ¥ `reshape` na vstupnÃ½ch dÃ¡tach, aby ich balÃ­Äek Linear Regression sprÃ¡vne pochopil. LineÃ¡rna regresia oÄakÃ¡va vstup v tvare 2D poÄ¾a, kde kaÅ¾dÃ½ riadok poÄ¾a zodpovedÃ¡ vektoru vstupnÃ½ch vlastnostÃ­. V naÅ¡om prÃ­pade, keÄÅ¾e mÃ¡me iba jeden vstup, potrebujeme pole tvaru N&times;1, kde N je veÄ¾kosÅ¥ datasetu.

Potom musÃ­me rozdeliÅ¥ Ãºdaje na trÃ©ningovÃ© a testovacie datasety, aby sme mohli po trÃ©novanÃ­ modelu overiÅ¥ jeho vÃ½kon:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Nakoniec samotnÃ© trÃ©novanie lineÃ¡rneho regresnÃ©ho modelu zaberie len dva riadky kÃ³du. Definujeme objekt `LinearRegression` a prispÃ´sobÃ­me ho naÅ¡im dÃ¡tam pomocou metÃ³dy `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Objekt `LinearRegression` po natrÃ©novanÃ­ obsahuje vÅ¡etky koeficienty regresie, ku ktorÃ½m sa dÃ¡ pristÃºpiÅ¥ pomocou vlastnosti `.coef_`. V naÅ¡om prÃ­pade je len jeden koeficient, ktorÃ½ by mal byÅ¥ okolo `-0.017`. To znamenÃ¡, Å¾e ceny sa zdajÃº s Äasom mierne zniÅ¾ovaÅ¥, ale nie prÃ­liÅ¡, pribliÅ¾ne o 2 centy za deÅˆ. MÃ´Å¾eme tieÅ¾ pristÃºpiÅ¥ k prieseÄnÃ­ku regresie s osou Y pomocou `lin_reg.intercept_` â€“ v naÅ¡om prÃ­pade to bude okolo `21`, Äo znaÄÃ­ cenu na zaÄiatku roka.

Aby sme videli, akÃ¡ je presnosÅ¥ nÃ¡Å¡ho modelu, mÃ´Å¾eme predikovaÅ¥ ceny na testovacej mnoÅ¾ine dÃ¡t a potom zmeraÅ¥, ako sÃº naÅ¡e predpovede blÃ­zke oÄakÃ¡vanÃ½m hodnotÃ¡m. To sa dÃ¡ urobiÅ¥ pomocou metriky strednej Å¡tvorcovej chyby (MSE), Äo je priemer vÅ¡etkÃ½ch Å¡tvorcovÃ½ch rozdielov medzi oÄakÃ¡vanou a predikovanou hodnotou.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

NaÅ¡a chyba sa javÃ­ okolo 2 bodov, Äo je pribliÅ¾ne 17%. Nie je to prÃ­liÅ¡ dobrÃ©. ÄalÅ¡Ã­m ukazovateÄ¾om kvality modelu je **koeficient determinÃ¡cie**, ktorÃ½ mÃ´Å¾eme zÃ­skaÅ¥ takto:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Ak je hodnota 0, znamenÃ¡ to, Å¾e model neberie do Ãºvahy vstupnÃ© dÃ¡ta a sprÃ¡va sa ako *najhorÅ¡Ã­ lineÃ¡rny prediktor*, ktorÃ½ je jednoducho priemernou hodnotou vÃ½sledku. Hodnota 1 znamenÃ¡, Å¾e dokÃ¡Å¾eme dokonale predpovedaÅ¥ vÅ¡etky oÄakÃ¡vanÃ© vÃ½stupy. V naÅ¡om prÃ­pade je koeficient okolo 0.06, Äo je dosÅ¥ nÃ­zke.

MÃ´Å¾eme tieÅ¾ vykresliÅ¥ testovacie dÃ¡ta spolu s regresnou Äiarou, aby sme lepÅ¡ie videli, ako regresia funguje v naÅ¡om prÃ­pade:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linear regression" src="../../../../translated_images/sk/linear-results.f7c3552c85b0ed1c.webp" width="50%" />

## PolynomickÃ¡ regresia

ÄalÅ¡Ã­m typom lineÃ¡rnej regresie je polynomickÃ¡ regresia. KÃ½m niekedy existuje lineÃ¡rny vzÅ¥ah medzi premennÃ½mi â€“ ÄÃ­m vÃ¤ÄÅ¡ia je tekvica objemom, tÃ½m vyÅ¡Å¡ia je cena â€“ niekedy sa tieto vzÅ¥ahy nedajÃº zobraziÅ¥ ako rovina alebo priamka.

âœ… Tu je [niekoÄ¾ko ÄalÅ¡Ã­ch prÃ­kladov](https://online.stat.psu.edu/stat501/lesson/9/9.8) dÃ¡t, pre ktorÃ© by bolo vhodnÃ© pouÅ¾iÅ¥ polynomickÃº regresiu

Pozrite sa eÅ¡te raz na vzÅ¥ah medzi dÃ¡tumom a cenou. ZdÃ¡ sa vÃ¡m, Å¾e by mal byÅ¥ nevyhnutne analyzovanÃ½ priamkou? NemÃ´Å¾u ceny kolÃ­saÅ¥? V tomto prÃ­pade mÃ´Å¾ete skÃºsiÅ¥ polynomickÃº regresiu.

âœ… PolynomickÃ© vÃ½razy sÃº matematickÃ© vÃ½razy, ktorÃ© mÃ´Å¾u obsahovaÅ¥ jednu alebo viac premennÃ½ch a koeficientov

PolynomickÃ¡ regresia vytvÃ¡ra zakrivenÃº Äiaru, aby lepÅ¡ie vyhovela nelineÃ¡rnym dÃ¡tam. V naÅ¡om prÃ­pade, ak do vstupnÃ½ch dÃ¡t zahrnieme druhÃº mocninu premennej `DayOfYear`, mali by sme byÅ¥ schopnÃ­ prispÃ´sobiÅ¥ dÃ¡ta parabolickou krivkou, ktorÃ¡ bude maÅ¥ minimum v urÄitom bode v priebehu roka.

Scikit-learn obsahuje uÅ¾itoÄnÃ© [pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) na kombinovanie rÃ´znych krokov spracovania dÃ¡t dokopy. **Pipeline** je reÅ¥azec **estimatorov**. V naÅ¡om prÃ­pade vytvorÃ­me pipeline, ktorÃ¡ najprv pridÃ¡ polynomickÃ© prvky do nÃ¡Å¡ho modelu a potom trÃ©nuje regresiu:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

PouÅ¾itie `PolynomialFeatures(2)` znamenÃ¡, Å¾e zahrnieme vÅ¡etky polynÃ³my druhÃ©ho stupÅˆa z vstupnÃ½ch dÃ¡t. V naÅ¡om prÃ­pade to bude iba `DayOfYear`<sup>2</sup>, ale pri dvoch vstupnÃ½ch premennÃ½ch X a Y sa pridajÃº X<sup>2</sup>, XY a Y<sup>2</sup>. MÃ´Å¾eme tieÅ¾ pouÅ¾iÅ¥ polynÃ³my vyÅ¡Å¡Ã­ch stupÅˆov, ak chceme.

Pipeline moÅ¾no pouÅ¾Ã­vaÅ¥ rovnako ako pÃ´vodnÃ½ objekt `LinearRegression`, teda mÃ´Å¾eme pipeline natrÃ©novaÅ¥ pomocou `fit` a potom pouÅ¾iÅ¥ `predict` na zÃ­skanie vÃ½sledkov predikcie. Tu je graf zobrazujÃºci testovacie dÃ¡ta a aproximaÄnÃº krivku:

<img alt="Polynomial regression" src="../../../../translated_images/sk/poly-results.ee587348f0f1f60b.webp" width="50%" />

PouÅ¾itÃ­m polynomickej regresie mÃ´Å¾eme dosiahnuÅ¥ mierne niÅ¾Å¡iu MSE a vyÅ¡Å¡Ã­ koeficient determinÃ¡cie, ale nie vÃ½razne. MusÃ­me zohÄ¾adniÅ¥ ÄalÅ¡ie vlastnosti!

> VidÃ­te, Å¾e minimÃ¡lne ceny tekvÃ­c sa prejavujÃº niekde okolo Halloweenu. Ako by ste to vysvetlili?

ğŸƒ Gratulujeme, prÃ¡ve ste vytvorili model, ktorÃ½ mÃ´Å¾e pomÃ´cÅ¥ predpovedaÅ¥ cenu tekvÃ­c na kolÃ¡Äe. Pravdepodobne mÃ´Å¾ete rovnakÃ½ postup zopakovaÅ¥ pre vÅ¡etky druhy tekvÃ­c, ale to by bolo zdÄºhavÃ©. NauÄÃ­me sa teraz, ako zohÄ¾adniÅ¥ odrodu tekvice v naÅ¡om modeli!

## KategorickÃ© vlastnosti

V ideÃ¡lnom svete chceme byÅ¥ schopnÃ­ predpovedaÅ¥ ceny pre rÃ´zne odrody tekvÃ­c pomocou toho istÃ©ho modelu. AvÅ¡ak stÄºpec `Variety` je trochu inÃ½ ako stÄºpce ako `Month`, pretoÅ¾e obsahuje neÄÃ­selnÃ© hodnoty. TakÃ©to stÄºpce sa nazÃ½vajÃº **kategorickÃ©**.

[![ML pre zaÄiatoÄnÃ­kov â€“ predikcie kategÃ³riÃ­ pomocou lineÃ¡rnej regresie](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML pre zaÄiatoÄnÃ­kov â€“ predikcie kategÃ³riÃ­ pomocou lineÃ¡rnej regresie")

> ğŸ¥ Kliknite na obrÃ¡zok vyÅ¡Å¡ie pre krÃ¡tky videoprÃ­klad pouÅ¾itia kategorickÃ½ch vlastnostÃ­.

Tu vidÃ­te, ako priemernÃ¡ cena zÃ¡visÃ­ na odrode:

<img alt="Average price by variety" src="../../../../translated_images/sk/price-by-variety.744a2f9925d9bcb4.webp" width="50%" />

Aby sme zohÄ¾adnili odrodu, musÃ­me ju najskÃ´r premeniÅ¥ na ÄÃ­selnÃº formu, teda **zakÃ³dovaÅ¥** ju. Existuje niekoÄ¾ko spÃ´sobov, ako to urobiÅ¥:

* JednoduchÃ© **ÄÃ­selnÃ© kÃ³dovanie** vytvorÃ­ tabuÄ¾ku rÃ´znych odrÃ´d a potom nahradÃ­ nÃ¡zov odrody indexom z tejto tabuÄ¾ky. To nie je najlepÅ¡ia voÄ¾ba pre lineÃ¡rnu regresiu, pretoÅ¾e lineÃ¡rna regresia vezme skutoÄnÃº ÄÃ­slenÃº hodnotu indexu a vynÃ¡sobÃ­ ju koeficientom, ÄÃ­m ju pridÃ¡ k vÃ½sledku. V naÅ¡om prÃ­pade je vzÅ¥ah medzi ÄÃ­slom indexu a cenou zjavne nelineÃ¡rny, aj keÄ zabezpeÄÃ­me, Å¾e indexy budÃº usporiadanÃ© urÄitÃ½m spÃ´sobom.
* **One-hot encoding** nahradÃ­ stÄºpec `Variety` Å¡tyrmi rÃ´znymi stÄºpcami, po jednom pre kaÅ¾dÃº odrodu. KaÅ¾dÃ½ stÄºpec bude obsahovaÅ¥ `1`, ak prÃ­sluÅ¡nÃ½ riadok je danej odrody, a `0` inak. To znamenÃ¡, Å¾e v lineÃ¡rnej regresii budÃº Å¡tyri koeficienty, jeden pre kaÅ¾dÃº odrodu tekvÃ­c, zodpovedajÃºce â€poÄiatoÄnej ceneâ€œ (alebo skÃ´r â€dodatoÄnej ceneâ€œ) pre tÃºto konkrÃ©tnu odrodu.

NasledujÃºci kÃ³d ukazuje, ako mÃ´Å¾eme one-hot kÃ³dovaÅ¥ odrodu:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Na trÃ©novanie lineÃ¡rnej regresie so vstupom ako one-hot kÃ³dovanÃ¡ odroda staÄÃ­ sprÃ¡vne inicializovaÅ¥ dÃ¡ta `X` a `y`:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

ZvyÅ¡ok kÃ³du je rovnakÃ½ ako sme pouÅ¾Ã­vali vyÅ¡Å¡ie na trÃ©novanie lineÃ¡rnej regresie. Ak to vyskÃºÅ¡ate, uvidÃ­te, Å¾e strednÃ¡ Å¡tvorcovÃ¡ chyba je pribliÅ¾ne rovnakÃ¡, ale zÃ­skame oveÄ¾a vyÅ¡Å¡Ã­ koeficient determinÃ¡cie (~77%). Pre eÅ¡te presnejÅ¡ie predikcie mÃ´Å¾eme zohÄ¾adniÅ¥ ÄalÅ¡ie kategorickÃ© vlastnosti, ako aj ÄÃ­selnÃ© vlastnosti, naprÃ­klad `Month` alebo `DayOfYear`. Na zÃ­skanie jednÃ©ho veÄ¾kÃ©ho poÄ¾a vlastnostÃ­ mÃ´Å¾eme pouÅ¾iÅ¥ `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Tu tieÅ¾ zohÄ¾adÅˆujeme `City` a typ `Package`, Äo nÃ¡m dÃ¡va MSE 2.84 (10%) a determinÃ¡ciu 0.94!

## Spojme to vÅ¡etko dokopy

Na vytvorenie najlepÅ¡ieho modelu mÃ´Å¾eme pouÅ¾iÅ¥ kombinovanÃ© (one-hot kÃ³dovanÃ© kategorickÃ© + ÄÃ­selnÃ©) dÃ¡ta z vyÅ¡Å¡ie uvedenÃ©ho prÃ­kladu spolu s polynomickou regresiou. Tu je kompletnÃ½ kÃ³d pre vaÅ¡u pohodlnosÅ¥:

```python
# nastaviÅ¥ trÃ©ningovÃ© dÃ¡ta
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# vykonaÅ¥ rozdelenie na trÃ©novaciu a testovaciu mnoÅ¾inu
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# nastaviÅ¥ a trÃ©novaÅ¥ pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predpovedaÅ¥ vÃ½sledky pre testovacie dÃ¡ta
pred = pipeline.predict(X_test)

# vypoÄÃ­taÅ¥ MSE a koeficient urÄenia
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

To by nÃ¡m malo daÅ¥ najlepÅ¡Ã­ koeficient determinÃ¡cie takmer 97% a MSE=2.23 (~8% chyba predikcie).

| Model | MSE | Koeficient determinÃ¡cie |
|-------|-----|-------------------------|
| LineÃ¡rna regresia s `DayOfYear` | 2.77 (17,2%) | 0.07 |
| PolynomickÃ¡ regresia s `DayOfYear` | 2.73 (17,0%) | 0.08 |
| LineÃ¡rna regresia s `Variety` | 5.24 (19,7%) | 0.77 |
| LineÃ¡rna regresia so vÅ¡etkÃ½mi vlastnosÅ¥ami | 2.84 (10,5%) | 0.94 |
| PolynomickÃ¡ regresia so vÅ¡etkÃ½mi vlastnosÅ¥ami | 2.23 (8,25%) | 0.97 |

ğŸ† VÃ½borne! V tejto lekcii ste vytvorili Å¡tyri regresnÃ© modely a zlepÅ¡ili kvalitu modelu na 97%. V zÃ¡vereÄnej sekcii o regresii sa nauÄÃ­te o logistickej regresii na urÄenie kategÃ³riÃ­.

---
## ğŸš€VÃ½zva

Otestujte niekoÄ¾ko rÃ´znych premennÃ½ch v tomto zÃ¡pisnÃ­ku a zistite, ako korelÃ¡cia sÃºvisÃ­ s presnosÅ¥ou modelu.

## [KvÃ­z po prednÃ¡Å¡ke](https://ff-quizzes.netlify.app/en/ml/)

## PrehÄ¾ad a samostatnÃ© Å¡tÃºdium

V tejto lekcii sme sa nauÄili o lineÃ¡rnej regresii. ExistujÃº aj inÃ© dÃ´leÅ¾itÃ© typy regresie. PreÄÃ­tajte si o technikÃ¡ch Stepwise, Ridge, Lasso a Elasticnet. Dobrou Å¡tudijnou pomÃ´ckou je [kurz Stanford Statistical Learning](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Zadanie

[Postavte model](assignment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Upozornenie**:  
Tento dokument bol preloÅ¾enÃ½ pomocou AI prekladateÄ¾skej sluÅ¾by [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snaÅ¾Ã­me o presnosÅ¥, uvedomte si, Å¾e automatickÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. OriginÃ¡lny dokument v jeho pÃ´vodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nenesieme zodpovednosÅ¥ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->